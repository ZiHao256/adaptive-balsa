Looking up params by name: Balsa_JOBRandSplit
BalsaAgent params:
{
  adamw: None
  agent_checkpoint: None
  avoid_eq_filters: False
  beam: 20
  bs: 1024
  bushy: True
  check_hint: True
  cls: <class 'experiments.Balsa_JOBRandSplit'>
  cost_model: "mincardcost"
  cross_entropy: False
  db: "imdbload"
  dedup_training_data: True
  drop_cache: True
  dropout: 0.0
  early_stop_on_skip_fraction: None
  ema_decay: 0.95
  engine: "postgres"
  engine_dialect_query_dir: None
  epochs: 100
  epsilon_greedy: 0
  epsilon_greedy_random_plan: False
  epsilon_greedy_random_transform: False
  epsilon_greedy_within_beam_search: False
  explore_soft_v: False
  explore_visit_counts: False
  explore_visit_counts_latency_sort: False
  explore_visit_counts_sort: True
  final_decay_rate: None
  finetune_out_mlp_only: False
  generic_ops_only_for_min_card_cost: False
  gradient_clip_val: 0
  increment_iter_despite_timeouts: True
  inherit_optimizer_state: False
  init_experience: "data/initial_policy_data.pkl"
  initial_timeout_ms: None
  l2_lambda: 0
  label_transform_running_stats: False
  label_transforms: ['log1p', 'standardize']
  linear_decay_to_zero: False
  loss_type: None
  lr: 0.001
  lr_decay_iters: None
  lr_decay_rate: None
  lr_piecewise: [(0, 0.001), (60, 0.0005), (110, 0.00025), (160, 0.000125), (210, 0.0001)]
  on_policy: True
  param_noise: 0.0
  param_tau: 1.0
  per_transition_sgd_steps: -1
  perturb_query_features: None
  physical_execution_hindsight: False
  plan_physical: True
  planner_config: None
  pos_embs: True
  prev_replay_buffers_glob: None
  prev_replay_buffers_glob_val: None
  prev_replay_keep_last_fraction: 1
  query_dir: "queries/simple"
  query_glob: ['1*.sql', '2*.sql']
  real_use_plan_restrictions: True
  reduce_lr_within_val_iter: False
  relax_timeout_factor: None
  relax_timeout_on_n_timeout_iters: None
  replay_buffer_reset_at_iter: None
  run_baseline: False
  search_method: "beam_bk"
  search_space_join_ops: ['Hash Join', 'Merge Join', 'Nested Loop']
  search_space_scan_ops: ['Index Scan', 'Index Only Scan', 'Seq Scan']
  search_until_n_complete_plans: 1
  sim: True
  sim_checkpoint: None
  sim_data_collection_intermediate_goals: True
  sim_query_featurizer: True
  sim_use_plan_restrictions: True
  skip_sim_init_iter_1p: False
  skip_training_on_expert: True
  skip_training_on_timeouts: False
  special_timeout_label: True
  test_after_n_iters: 0
  test_every_n_iters: 1
  test_query_glob: ['2*.sql']
  test_using_retrained_model: False
  timeout_slack: 2
  track_model_moving_averages: False
  tree_conv: True
  tree_conv_version: None
  update_label_stats_every_iter: True
  use_adaptive_lr: None
  use_adaptive_lr_decay_to_zero: None
  use_cache: True
  use_ema_source: False
  use_last_n_iters: -1
  use_local_execution: True
  use_new_data_only: False
  use_timeout: True
  v2: True
  val_iters: 10
  validate_early_stop_patience: 3
  validate_every_n_epochs: 5
  validate_fraction: 0.1
}
Connected to ray!  Resources: {'pg': 1.0, 'CPU': 16.0, 'memory': 7923150030.0, 'object_store_memory': 3961575014.0, 'node:172.17.0.2': 1.0}
4 train queries: ['1a', '1b', '1c', '1d']
4 test queries: ['2a', '2b', '2c', '2d']
wandb: Currently logged in as: zihao626 (zihao626_). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.1
wandb: Run data is saved locally in /balsa/balsa/wandb/run-20240611_074349-enqqtg8t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-hill-51
wandb: ‚≠êÔ∏è View project at https://wandb.ai/zihao626_/balsa
wandb: üöÄ View run at https://wandb.ai/zihao626_/balsa/runs/enqqtg8t
latency_expert/workload (seconds): 1.12 (4 queries)
latency_expert_test/workload (seconds): 13.69 (4 queries)
Number of joins [4, 4], avg 4.0
I0611 07:43:56.822027 140711206703488 sim.py:521] {
  bs: 1024
  cls: <class 'sim.Sim'>
  epochs: 100
  eval_latency_output_path: "eval-latency.csv"
  eval_output_path: "eval-cost.csv"
  generic_ops_only_for_min_card_cost: False
  gradient_clip_val: 0
  infer_beam_size: 20
  infer_search_method: "beam_bk"
  infer_search_until_n_complete_plans: 1
  label_transforms: ['log1p', 'standardize']
  loss_type: None
  perturb_query_features: None
  plan_featurizer_cls: <class 'balsa.util.plans_lib.PhysicalTreeNodeFeaturizer'>
  plan_physical: True
  query_featurizer_cls: <class 'sim.SimQueryFeaturizer'>
  search: {
    cls: <class 'balsa.search.DynamicProgramming'>
    collect_data_include_suboptimal: True
    cost_model: {
      cls: <class 'balsa.costing.MinCardCost'>
      cost_physical_ops: True
    }
    plan_physical_ops: True
    search_space: "bushy"
  }
  sim_data_collection_intermediate_goals: True
  skip_data_collection_geq_num_rels: 12
  tree_conv_version: None
  validate_fraction: 0.1
  workload: {
    cls: <class 'balsa.envs.envs.JoinOrderBenchmark'>
    loop_through_queries: False
    query_dir: "queries/simple"
    query_glob: ['1*.sql', '2*.sql']
    search_space_join_ops: ['Hash Join', 'Merge Join', 'Nested Loop']
    search_space_scan_ops: ['Index Scan', 'Index Only Scan', 'Seq Scan']
    test_query_glob: ['2*.sql']
  }
}
Search space (old=query nodes; new=agent action space):
old: ['Index Scan' 'Seq Scan'] ['Hash Join' 'Nested Loop'] ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Scan'
 'Nested Loop' 'PartialAggregate' 'Seq Scan' 'SimpleAggregate']
new: ['Index Only Scan' 'Index Scan' 'Seq Scan'] ['Hash Join' 'Merge Join' 'Nested Loop'] ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Scan'
 'Nested Loop' 'PartialAggregate' 'Seq Scan' 'SimpleAggregate']
I0611 07:43:57.038983 140711206703488 sim.py:553] 4 train queries: ['1a', '1b', '1c', '1d']
I0611 07:43:57.039244 140711206703488 sim.py:556] 4 test queries: ['2a', '2b', '2c', '2d']
plans_lib.FilterScansOrJoins()
plans_lib.GatherUnaryFiltersInfo()
postgres.EstimateFilterRows()
14 unique filters
{('company_name AS cn', "((cn.country_code)::text = '[de]'::text)"): 9792,
 ('company_name AS cn', "((cn.country_code)::text = '[nl]'::text)"): 2185,
 ('company_name AS cn', "((cn.country_code)::text = '[sm]'::text)"): 19,
 ('company_name AS cn', "((cn.country_code)::text = '[us]'::text)"): 84732,
 ('company_type AS ct', "((ct.kind)::text = 'production companies'::text)"): 1,
 ('info_type AS it', "((it.info)::text = 'bottom 10 rank'::text)"): 1,
 ('info_type AS it', "((it.info)::text = 'top 250 rank'::text)"): 1,
 ('keyword AS k', "(k.keyword = 'character-name-in-title'::text)"): 1,
 ('movie_companies AS mc', "((mc.note !~~ '%(as Metro-Goldwyn-Mayer Pictures)%'::text) AND ((mc.note ~~ '%(co-production)%'::text) OR (mc.note ~~ '%(presents)%'::text)))"): 27149,
 ('movie_companies AS mc', "((mc.note !~~ '%(as Metro-Goldwyn-Mayer Pictures)%'::text) AND (mc.note ~~ '%(co-production)%'::text))"): 14028,
 ('movie_companies AS mc', "(mc.note !~~ '%(as Metro-Goldwyn-Mayer Pictures)%'::text)"): 1328998,
 ('title AS t', '((t.production_year >= 2005) AND (t.production_year <= 2010))'): 725345,
 ('title AS t', '(t.production_year > 2000)'): 1377608,
 ('title AS t', '(t.production_year > 2010)'): 390764}
I0611 07:43:57.140509 140711206703488 sim.py:653] Loaded simulation data (len 1370) from: data/sim-data-e0e0819b.pkl
I0611 07:43:57.141070 140711206703488 sim.py:655] Training data (first 50, total 1370):
I0611 07:43:57.141900 140711206703488 sim.py:656] SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx it)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx mc)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((mi_idx t)) */', goal='mi_idx,t', cost=5288382)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((it mi_idx)) */', goal='it,mi_idx', cost=1392249)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
SubplanGoalCost(subplan='/*+ Leading((mc mi_idx)) */', goal='mc,mi_idx', cost=1482220)
I0611 07:43:57.166414 140711206703488 sim.py:682] Loaded featurized data (len 1370) from: data/sim-featurized-54cf2385.pkl
I0611 07:43:57.166757 140711206703488 sim.py:954] _MakeDatasetAndLoader()
costs stats mean 14.657443921473481 std 1.0028742659249696
I0611 07:43:57.169302 140711206703488 sim.py:807] num_train=1233 num_validation=137
I0611 07:43:57.179496 140711206703488 sim.py:959] Example batch (query,plan,indexes,cost):
[tensor([[0.0000, 0.2500, 0.0000,  ..., 1.0000, 0.0000, 0.5449],
        [0.0000, 0.2500, 0.0000,  ..., 1.0000, 0.0000, 0.5449],
        [0.0000, 0.0000, 0.0088,  ..., 1.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2869],
        [0.0000, 0.2500, 0.0088,  ..., 1.0000, 0.0000, 0.5449],
        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.1546]]), tensor([[[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.]],

        [[0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 1.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 1., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 1.,  ..., 0., 0., 0.]],

        [[0., 0., 1.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 1., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[1],
         [2],
         [7],
         ...,
         [0],
         [0],
         [0]],

        [[1],
         [2],
         [3],
         ...,
         [0],
         [0],
         [0]],

        [[1],
         [2],
         [5],
         ...,
         [0],
         [0],
         [0]],

        ...,

        [[1],
         [2],
         [3],
         ...,
         [0],
         [0],
         [0]],

        [[1],
         [2],
         [3],
         ...,
         [0],
         [0],
         [0]],

        [[1],
         [2],
         [5],
         ...,
         [0],
         [0],
         [0]]]), tensor([ 0.8244,  0.7034,  1.0137,  ...,  0.0482,  0.5698, -0.2372])]
I0611 07:43:57.179980 140711206703488 sim.py:968] unused_bs, plan_feat_dims, unused_max_tree_nodes (1024, 27, 10)
I0611 07:43:57.180260 140711206703488 sim.py:753] SIM query_feat_dims=8 plan_feat_dims=27
I0611 07:43:57.180493 140711206703488 sim.py:755] SIM query_feat=<class 'sim.SimQueryFeaturizer'> plan_feat=<class 'balsa.util.plans_lib.PhysicalTreeNodeFeaturizer'>
Number of model parameters: 605473 (~= 2.3MB)
SimModel(
  (tree_conv): TreeConvolution(
    (query_mlp): Sequential(
      (0): Linear(in_features=8, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (2): LeakyReLU(negative_slope=0.01)
      (3): Linear(in_features=128, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=64, out_features=32, bias=True)
    )
    (conv): Sequential(
      (0): TreeConv1d(
        (weights): Conv1d(59, 512, kernel_size=(3,), stride=(3,))
      )
      (1): TreeStandardize()
      (2): TreeAct(
        (activation): LeakyReLU(negative_slope=0.01)
      )
      (3): TreeConv1d(
        (weights): Conv1d(512, 256, kernel_size=(3,), stride=(3,))
      )
      (4): TreeStandardize()
      (5): TreeAct(
        (activation): LeakyReLU(negative_slope=0.01)
      )
      (6): TreeConv1d(
        (weights): Conv1d(256, 128, kernel_size=(3,), stride=(3,))
      )
      (7): TreeStandardize()
      (8): TreeAct(
        (activation): LeakyReLU(negative_slope=0.01)
      )
      (9): TreeMaxPool()
    )
    (out_mlp): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): LeakyReLU(negative_slope=0.01)
      (3): Linear(in_features=64, out_features=32, bias=True)
      (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=32, out_features=1, bias=True)
    )
  )
)
GPU available: False, used: False
I0611 07:43:57.209143 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 07:43:57.209717 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)

   | Name                        | Type            | Params
-----------------------------------------------------------------
0  | tree_conv                   | TreeConvolution | 605 K 
1  | tree_conv.query_mlp         | Sequential      | 11 K  
2  | tree_conv.query_mlp.0       | Linear          | 1 K   
3  | tree_conv.query_mlp.1       | LayerNorm       | 256   
4  | tree_conv.query_mlp.2       | LeakyReLU       | 0     
5  | tree_conv.query_mlp.3       | Linear          | 8 K   
6  | tree_conv.query_mlp.4       | LayerNorm       | 128   
7  | tree_conv.query_mlp.5       | LeakyReLU       | 0     
8  | tree_conv.query_mlp.6       | Linear          | 2 K   
9  | tree_conv.conv              | Sequential      | 583 K 
10 | tree_conv.conv.0            | TreeConv1d      | 91 K  
11 | tree_conv.conv.0.weights    | Conv1d          | 91 K  
12 | tree_conv.conv.1            | TreeStandardize | 0     
13 | tree_conv.conv.2            | TreeAct         | 0     
14 | tree_conv.conv.2.activation | LeakyReLU       | 0     
15 | tree_conv.conv.3            | TreeConv1d      | 393 K 
16 | tree_conv.conv.3.weights    | Conv1d          | 393 K 
17 | tree_conv.conv.4            | TreeStandardize | 0     
18 | tree_conv.conv.5            | TreeAct         | 0     
19 | tree_conv.conv.5.activation | LeakyReLU       | 0     
20 | tree_conv.conv.6            | TreeConv1d      | 98 K  
21 | tree_conv.conv.6.weights    | Conv1d          | 98 K  
22 | tree_conv.conv.7            | TreeStandardize | 0     
23 | tree_conv.conv.8            | TreeAct         | 0     
24 | tree_conv.conv.8.activation | LeakyReLU       | 0     
25 | tree_conv.conv.9            | TreeMaxPool     | 0     
26 | tree_conv.out_mlp           | Sequential      | 10 K  
27 | tree_conv.out_mlp.0         | Linear          | 8 K   
28 | tree_conv.out_mlp.1         | LayerNorm       | 128   
29 | tree_conv.out_mlp.2         | LeakyReLU       | 0     
30 | tree_conv.out_mlp.3         | Linear          | 2 K   
31 | tree_conv.out_mlp.4         | LayerNorm       | 64    
32 | tree_conv.out_mlp.5         | LeakyReLU       | 0     
33 | tree_conv.out_mlp.6         | Linear          | 33    
I0611 07:43:57.272184 140711206703488 lightning.py:1449] 
   | Name                        | Type            | Params
-----------------------------------------------------------------
0  | tree_conv                   | TreeConvolution | 605 K 
1  | tree_conv.query_mlp         | Sequential      | 11 K  
2  | tree_conv.query_mlp.0       | Linear          | 1 K   
3  | tree_conv.query_mlp.1       | LayerNorm       | 256   
4  | tree_conv.query_mlp.2       | LeakyReLU       | 0     
5  | tree_conv.query_mlp.3       | Linear          | 8 K   
6  | tree_conv.query_mlp.4       | LayerNorm       | 128   
7  | tree_conv.query_mlp.5       | LeakyReLU       | 0     
8  | tree_conv.query_mlp.6       | Linear          | 2 K   
9  | tree_conv.conv              | Sequential      | 583 K 
10 | tree_conv.conv.0            | TreeConv1d      | 91 K  
11 | tree_conv.conv.0.weights    | Conv1d          | 91 K  
12 | tree_conv.conv.1            | TreeStandardize | 0     
13 | tree_conv.conv.2            | TreeAct         | 0     
14 | tree_conv.conv.2.activation | LeakyReLU       | 0     
15 | tree_conv.conv.3            | TreeConv1d      | 393 K 
16 | tree_conv.conv.3.weights    | Conv1d          | 393 K 
17 | tree_conv.conv.4            | TreeStandardize | 0     
18 | tree_conv.conv.5            | TreeAct         | 0     
19 | tree_conv.conv.5.activation | LeakyReLU       | 0     
20 | tree_conv.conv.6            | TreeConv1d      | 98 K  
21 | tree_conv.conv.6.weights    | Conv1d          | 98 K  
22 | tree_conv.conv.7            | TreeStandardize | 0     
23 | tree_conv.conv.8            | TreeAct         | 0     
24 | tree_conv.conv.8.activation | LeakyReLU       | 0     
25 | tree_conv.conv.9            | TreeMaxPool     | 0     
26 | tree_conv.out_mlp           | Sequential      | 10 K  
27 | tree_conv.out_mlp.0         | Linear          | 8 K   
28 | tree_conv.out_mlp.1         | LayerNorm       | 128   
29 | tree_conv.out_mlp.2         | LeakyReLU       | 0     
30 | tree_conv.out_mlp.3         | Linear          | 2 K   
31 | tree_conv.out_mlp.4         | LayerNorm       | 64    
32 | tree_conv.out_mlp.5         | LeakyReLU       | 0     
33 | tree_conv.out_mlp.6         | Linear          | 33    
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]                                           /home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.81it/s]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.81it/s, loss=1.019, v_num=tg8t, train_loss=1.02]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  2.99it/s, loss=1.019, v_num=tg8t, train_loss=1.02]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  2.98it/s, loss=1.042, v_num=tg8t, train_loss=1.07]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.07it/s, loss=1.042, v_num=tg8t, train_loss=1.07, val_loss=1.01]
                              [AEpoch 0:   0%|          | 0/3 [00:00<?, ?it/s, loss=1.042, v_num=tg8t, train_loss=1.07, val_loss=1.01]        Epoch 1:   0%|          | 0/3 [00:00<?, ?it/s, loss=1.042, v_num=tg8t, train_loss=1.07, val_loss=1.01]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.88it/s, loss=1.042, v_num=tg8t, train_loss=1.07, val_loss=1.01]Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.88it/s, loss=1.012, v_num=tg8t, train_loss=0.952, val_loss=1.01]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.14it/s, loss=1.012, v_num=tg8t, train_loss=0.952, val_loss=1.01]Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.14it/s, loss=1.055, v_num=tg8t, train_loss=1.18, val_loss=1.01] 
Validating: 0it [00:00, ?it/s][AEpoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.23it/s, loss=1.055, v_num=tg8t, train_loss=1.18, val_loss=0.845]
                              [AEpoch 1:   0%|          | 0/3 [00:00<?, ?it/s, loss=1.055, v_num=tg8t, train_loss=1.18, val_loss=0.845]        Epoch 2:   0%|          | 0/3 [00:00<?, ?it/s, loss=1.055, v_num=tg8t, train_loss=1.18, val_loss=0.845]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.15it/s, loss=1.055, v_num=tg8t, train_loss=1.18, val_loss=0.845]Epoch 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.15it/s, loss=0.999, v_num=tg8t, train_loss=0.777, val_loss=0.845]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.49it/s, loss=0.999, v_num=tg8t, train_loss=0.777, val_loss=0.845]Epoch 2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.48it/s, loss=1.052, v_num=tg8t, train_loss=1.32, val_loss=0.845] 
Validating: 0it [00:00, ?it/s][AEpoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.68it/s, loss=1.052, v_num=tg8t, train_loss=1.32, val_loss=0.757]
                              [AEpoch 2:   0%|          | 0/3 [00:00<?, ?it/s, loss=1.052, v_num=tg8t, train_loss=1.32, val_loss=0.757]        Epoch 3:   0%|          | 0/3 [00:00<?, ?it/s, loss=1.052, v_num=tg8t, train_loss=1.32, val_loss=0.757]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.02it/s, loss=1.052, v_num=tg8t, train_loss=1.32, val_loss=0.757]Epoch 3:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.01it/s, loss=1.012, v_num=tg8t, train_loss=0.769, val_loss=0.757]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.31it/s, loss=1.012, v_num=tg8t, train_loss=0.769, val_loss=0.757]Epoch 3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.30it/s, loss=0.993, v_num=tg8t, train_loss=0.864, val_loss=0.757]
Validating: 0it [00:00, ?it/s][AEpoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.47it/s, loss=0.993, v_num=tg8t, train_loss=0.864, val_loss=0.619]
                              [AEpoch 3:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.993, v_num=tg8t, train_loss=0.864, val_loss=0.619]        Epoch 4:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.993, v_num=tg8t, train_loss=0.864, val_loss=0.619]Epoch 4:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.15it/s, loss=0.993, v_num=tg8t, train_loss=0.864, val_loss=0.619]Epoch 4:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.15it/s, loss=0.953, v_num=tg8t, train_loss=0.629, val_loss=0.619]Epoch 4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.51it/s, loss=0.953, v_num=tg8t, train_loss=0.629, val_loss=0.619]Epoch 4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.51it/s, loss=0.936, v_num=tg8t, train_loss=0.787, val_loss=0.619]
Validating: 0it [00:00, ?it/s][AEpoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.69it/s, loss=0.936, v_num=tg8t, train_loss=0.787, val_loss=0.584]
                              [AEpoch 4:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.936, v_num=tg8t, train_loss=0.787, val_loss=0.584]        Epoch 5:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.936, v_num=tg8t, train_loss=0.787, val_loss=0.584]Epoch 5:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.24it/s, loss=0.936, v_num=tg8t, train_loss=0.787, val_loss=0.584]Epoch 5:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.23it/s, loss=0.908, v_num=tg8t, train_loss=0.63, val_loss=0.584] Epoch 5:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.62it/s, loss=0.908, v_num=tg8t, train_loss=0.63, val_loss=0.584]Epoch 5:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.62it/s, loss=0.888, v_num=tg8t, train_loss=0.67, val_loss=0.584]
Validating: 0it [00:00, ?it/s][AEpoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.82it/s, loss=0.888, v_num=tg8t, train_loss=0.67, val_loss=0.564]
                              [AEpoch 5:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.888, v_num=tg8t, train_loss=0.67, val_loss=0.564]        Epoch 6:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.888, v_num=tg8t, train_loss=0.67, val_loss=0.564]Epoch 6:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.22it/s, loss=0.888, v_num=tg8t, train_loss=0.67, val_loss=0.564]Epoch 6:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.21it/s, loss=0.871, v_num=tg8t, train_loss=0.665, val_loss=0.564]Epoch 6:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.57it/s, loss=0.871, v_num=tg8t, train_loss=0.665, val_loss=0.564]Epoch 6:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.56it/s, loss=0.840, v_num=tg8t, train_loss=0.43, val_loss=0.564] 
Validating: 0it [00:00, ?it/s][AEpoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.75it/s, loss=0.840, v_num=tg8t, train_loss=0.43, val_loss=0.55] 
                              [AEpoch 6:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.840, v_num=tg8t, train_loss=0.43, val_loss=0.55]        Epoch 7:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.840, v_num=tg8t, train_loss=0.43, val_loss=0.55]Epoch 7:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.09it/s, loss=0.840, v_num=tg8t, train_loss=0.43, val_loss=0.55]Epoch 7:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.09it/s, loss=0.828, v_num=tg8t, train_loss=0.665, val_loss=0.55]Epoch 7:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.45it/s, loss=0.828, v_num=tg8t, train_loss=0.665, val_loss=0.55]Epoch 7:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.44it/s, loss=0.799, v_num=tg8t, train_loss=0.365, val_loss=0.55]
Validating: 0it [00:00, ?it/s][AEpoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.59it/s, loss=0.799, v_num=tg8t, train_loss=0.365, val_loss=0.527]
                              [AEpoch 7:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.799, v_num=tg8t, train_loss=0.365, val_loss=0.527]        Epoch 8:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.799, v_num=tg8t, train_loss=0.365, val_loss=0.527]Epoch 8:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.04it/s, loss=0.799, v_num=tg8t, train_loss=0.365, val_loss=0.527]Epoch 8:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.03it/s, loss=0.787, v_num=tg8t, train_loss=0.596, val_loss=0.527]Epoch 8:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.36it/s, loss=0.787, v_num=tg8t, train_loss=0.596, val_loss=0.527]Epoch 8:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.36it/s, loss=0.776, v_num=tg8t, train_loss=0.581, val_loss=0.527]
Validating: 0it [00:00, ?it/s][AEpoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.53it/s, loss=0.776, v_num=tg8t, train_loss=0.581, val_loss=0.506]
                              [AEpoch 8:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.776, v_num=tg8t, train_loss=0.581, val_loss=0.506]        Epoch 9:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.776, v_num=tg8t, train_loss=0.581, val_loss=0.506]Epoch 9:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.21it/s, loss=0.776, v_num=tg8t, train_loss=0.581, val_loss=0.506]Epoch 9:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.21it/s, loss=0.767, v_num=tg8t, train_loss=0.613, val_loss=0.506]Epoch 9:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.49it/s, loss=0.767, v_num=tg8t, train_loss=0.613, val_loss=0.506]Epoch 9:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.49it/s, loss=0.747, v_num=tg8t, train_loss=0.36, val_loss=0.506] 
Validating: 0it [00:00, ?it/s][AEpoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.72it/s, loss=0.747, v_num=tg8t, train_loss=0.36, val_loss=0.488]
                              [AEpoch 9:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.747, v_num=tg8t, train_loss=0.36, val_loss=0.488]        Epoch 10:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.747, v_num=tg8t, train_loss=0.36, val_loss=0.488]Epoch 10:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.97it/s, loss=0.747, v_num=tg8t, train_loss=0.36, val_loss=0.488]Epoch 10:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.97it/s, loss=0.725, v_num=tg8t, train_loss=0.583, val_loss=0.488]Epoch 10:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.28it/s, loss=0.725, v_num=tg8t, train_loss=0.583, val_loss=0.488]Epoch 10:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.28it/s, loss=0.691, v_num=tg8t, train_loss=0.382, val_loss=0.488]
Validating: 0it [00:00, ?it/s][AEpoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.45it/s, loss=0.691, v_num=tg8t, train_loss=0.382, val_loss=0.47] 
                              [AEpoch 10:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.691, v_num=tg8t, train_loss=0.382, val_loss=0.47]        Epoch 11:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.691, v_num=tg8t, train_loss=0.382, val_loss=0.47]Epoch 11:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.04it/s, loss=0.691, v_num=tg8t, train_loss=0.382, val_loss=0.47]Epoch 11:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.04it/s, loss=0.671, v_num=tg8t, train_loss=0.549, val_loss=0.47]Epoch 11:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.32it/s, loss=0.671, v_num=tg8t, train_loss=0.549, val_loss=0.47]Epoch 11:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.31it/s, loss=0.634, v_num=tg8t, train_loss=0.447, val_loss=0.47]
Validating: 0it [00:00, ?it/s][AEpoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.49it/s, loss=0.634, v_num=tg8t, train_loss=0.447, val_loss=0.447]
                              [AEpoch 11:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.634, v_num=tg8t, train_loss=0.447, val_loss=0.447]        Epoch 12:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.634, v_num=tg8t, train_loss=0.447, val_loss=0.447]Epoch 12:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.16it/s, loss=0.634, v_num=tg8t, train_loss=0.447, val_loss=0.447]Epoch 12:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.15it/s, loss=0.622, v_num=tg8t, train_loss=0.543, val_loss=0.447]Epoch 12:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.35it/s, loss=0.622, v_num=tg8t, train_loss=0.543, val_loss=0.447]Epoch 12:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.34it/s, loss=0.571, v_num=tg8t, train_loss=0.294, val_loss=0.447]
Validating: 0it [00:00, ?it/s][AEpoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.51it/s, loss=0.571, v_num=tg8t, train_loss=0.294, val_loss=0.431]
                              [AEpoch 12:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.571, v_num=tg8t, train_loss=0.294, val_loss=0.431]        Epoch 13:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.571, v_num=tg8t, train_loss=0.294, val_loss=0.431]Epoch 13:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.07it/s, loss=0.571, v_num=tg8t, train_loss=0.294, val_loss=0.431]Epoch 13:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.06it/s, loss=0.555, v_num=tg8t, train_loss=0.453, val_loss=0.431]Epoch 13:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.38it/s, loss=0.555, v_num=tg8t, train_loss=0.453, val_loss=0.431]Epoch 13:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.37it/s, loss=0.539, v_num=tg8t, train_loss=0.539, val_loss=0.431]
Validating: 0it [00:00, ?it/s][AEpoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.55it/s, loss=0.539, v_num=tg8t, train_loss=0.539, val_loss=0.379]
                              [AEpoch 13:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.539, v_num=tg8t, train_loss=0.539, val_loss=0.379]        Epoch 14:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.539, v_num=tg8t, train_loss=0.539, val_loss=0.379]Epoch 14:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.16it/s, loss=0.539, v_num=tg8t, train_loss=0.539, val_loss=0.379]Epoch 14:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.16it/s, loss=0.530, v_num=tg8t, train_loss=0.458, val_loss=0.379]Epoch 14:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.52it/s, loss=0.530, v_num=tg8t, train_loss=0.458, val_loss=0.379]Epoch 14:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.52it/s, loss=0.503, v_num=tg8t, train_loss=0.244, val_loss=0.379]
Validating: 0it [00:00, ?it/s][AEpoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.68it/s, loss=0.503, v_num=tg8t, train_loss=0.244, val_loss=0.347]
                              [AEpoch 14:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.503, v_num=tg8t, train_loss=0.244, val_loss=0.347]        Epoch 15:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.503, v_num=tg8t, train_loss=0.244, val_loss=0.347]Epoch 15:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.01it/s, loss=0.503, v_num=tg8t, train_loss=0.244, val_loss=0.347]Epoch 15:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.01it/s, loss=0.491, v_num=tg8t, train_loss=0.385, val_loss=0.347]Epoch 15:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.34it/s, loss=0.491, v_num=tg8t, train_loss=0.385, val_loss=0.347]Epoch 15:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.33it/s, loss=0.478, v_num=tg8t, train_loss=0.418, val_loss=0.347]
Validating: 0it [00:00, ?it/s][AEpoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.50it/s, loss=0.478, v_num=tg8t, train_loss=0.418, val_loss=0.321]
                              [AEpoch 15:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.478, v_num=tg8t, train_loss=0.418, val_loss=0.321]        Epoch 16:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.478, v_num=tg8t, train_loss=0.418, val_loss=0.321]Epoch 16:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.16it/s, loss=0.478, v_num=tg8t, train_loss=0.418, val_loss=0.321]Epoch 16:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.16it/s, loss=0.465, v_num=tg8t, train_loss=0.4, val_loss=0.321]  Epoch 16:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.53it/s, loss=0.465, v_num=tg8t, train_loss=0.4, val_loss=0.321]Epoch 16:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.52it/s, loss=0.453, v_num=tg8t, train_loss=0.196, val_loss=0.321]
Validating: 0it [00:00, ?it/s][AEpoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.72it/s, loss=0.453, v_num=tg8t, train_loss=0.196, val_loss=0.294]
                              [AEpoch 16:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.453, v_num=tg8t, train_loss=0.196, val_loss=0.294]        Epoch 17:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.453, v_num=tg8t, train_loss=0.196, val_loss=0.294]Epoch 17:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.09it/s, loss=0.453, v_num=tg8t, train_loss=0.196, val_loss=0.294]Epoch 17:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.09it/s, loss=0.438, v_num=tg8t, train_loss=0.352, val_loss=0.294]Epoch 17:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.38it/s, loss=0.438, v_num=tg8t, train_loss=0.352, val_loss=0.294]Epoch 17:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.38it/s, loss=0.433, v_num=tg8t, train_loss=0.273, val_loss=0.294]
Validating: 0it [00:00, ?it/s][AEpoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.55it/s, loss=0.433, v_num=tg8t, train_loss=0.273, val_loss=0.277]
                              [AEpoch 17:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.433, v_num=tg8t, train_loss=0.273, val_loss=0.277]        Epoch 18:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.433, v_num=tg8t, train_loss=0.273, val_loss=0.277]Epoch 18:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.16it/s, loss=0.433, v_num=tg8t, train_loss=0.273, val_loss=0.277]Epoch 18:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.15it/s, loss=0.419, v_num=tg8t, train_loss=0.312, val_loss=0.277]Epoch 18:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.49it/s, loss=0.419, v_num=tg8t, train_loss=0.312, val_loss=0.277]Epoch 18:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.49it/s, loss=0.408, v_num=tg8t, train_loss=0.359, val_loss=0.277]
Validating: 0it [00:00, ?it/s][AEpoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.62it/s, loss=0.408, v_num=tg8t, train_loss=0.359, val_loss=0.252]
                              [AEpoch 18:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.408, v_num=tg8t, train_loss=0.359, val_loss=0.252]        Epoch 19:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.408, v_num=tg8t, train_loss=0.359, val_loss=0.252]Epoch 19:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.94it/s, loss=0.408, v_num=tg8t, train_loss=0.359, val_loss=0.252]Epoch 19:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.94it/s, loss=0.391, v_num=tg8t, train_loss=0.282, val_loss=0.252]Epoch 19:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.27it/s, loss=0.391, v_num=tg8t, train_loss=0.356, val_loss=0.252]
Validating: 0it [00:00, ?it/s][AEpoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.72it/s, loss=0.391, v_num=tg8t, train_loss=0.356, val_loss=0.252]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.43it/s, loss=0.391, v_num=tg8t, train_loss=0.356, val_loss=0.245]
                              [AEpoch 19:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.391, v_num=tg8t, train_loss=0.356, val_loss=0.245]        Epoch 20:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.391, v_num=tg8t, train_loss=0.356, val_loss=0.245]Epoch 20:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.13it/s, loss=0.377, v_num=tg8t, train_loss=0.293, val_loss=0.245]Epoch 20:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.51it/s, loss=0.377, v_num=tg8t, train_loss=0.293, val_loss=0.245]Epoch 20:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.50it/s, loss=0.370, v_num=tg8t, train_loss=0.249, val_loss=0.245]
Validating: 0it [00:00, ?it/s][AEpoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.70it/s, loss=0.370, v_num=tg8t, train_loss=0.249, val_loss=0.214]
                              [AEpoch 20:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.370, v_num=tg8t, train_loss=0.249, val_loss=0.214]        Epoch 21:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.370, v_num=tg8t, train_loss=0.249, val_loss=0.214]Epoch 21:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.19it/s, loss=0.356, v_num=tg8t, train_loss=0.271, val_loss=0.214]Epoch 21:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.52it/s, loss=0.356, v_num=tg8t, train_loss=0.271, val_loss=0.214]Epoch 21:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.51it/s, loss=0.342, v_num=tg8t, train_loss=0.163, val_loss=0.214]
Validating: 0it [00:00, ?it/s][AEpoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.66it/s, loss=0.342, v_num=tg8t, train_loss=0.163, val_loss=0.208]
                              [AEpoch 21:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.342, v_num=tg8t, train_loss=0.163, val_loss=0.208]        Epoch 22:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.342, v_num=tg8t, train_loss=0.163, val_loss=0.208]Epoch 22:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.14it/s, loss=0.326, v_num=tg8t, train_loss=0.217, val_loss=0.208]Epoch 22:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.54it/s, loss=0.326, v_num=tg8t, train_loss=0.217, val_loss=0.208]Epoch 22:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.54it/s, loss=0.329, v_num=tg8t, train_loss=0.353, val_loss=0.208]
Validating: 0it [00:00, ?it/s][AEpoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.59it/s, loss=0.329, v_num=tg8t, train_loss=0.353, val_loss=0.191]
                              [AEpoch 22:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.329, v_num=tg8t, train_loss=0.353, val_loss=0.191]        Epoch 23:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.329, v_num=tg8t, train_loss=0.353, val_loss=0.191]Epoch 23:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.12it/s, loss=0.318, v_num=tg8t, train_loss=0.235, val_loss=0.191]Epoch 23:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.47it/s, loss=0.318, v_num=tg8t, train_loss=0.235, val_loss=0.191]Epoch 23:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.47it/s, loss=0.299, v_num=tg8t, train_loss=0.164, val_loss=0.191]
Validating: 0it [00:00, ?it/s][AEpoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.64it/s, loss=0.299, v_num=tg8t, train_loss=0.164, val_loss=0.181]
                              [AEpoch 23:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.299, v_num=tg8t, train_loss=0.164, val_loss=0.181]        Epoch 24:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.299, v_num=tg8t, train_loss=0.164, val_loss=0.181]Epoch 24:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.03it/s, loss=0.287, v_num=tg8t, train_loss=0.212, val_loss=0.181]Epoch 24:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.28it/s, loss=0.287, v_num=tg8t, train_loss=0.212, val_loss=0.181]Epoch 24:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.28it/s, loss=0.284, v_num=tg8t, train_loss=0.195, val_loss=0.181]
Validating: 0it [00:00, ?it/s][AEpoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.40it/s, loss=0.284, v_num=tg8t, train_loss=0.195, val_loss=0.179]
                              [AEpoch 24:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.284, v_num=tg8t, train_loss=0.195, val_loss=0.179]        Epoch 25:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.284, v_num=tg8t, train_loss=0.195, val_loss=0.179]Epoch 25:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.95it/s, loss=0.275, v_num=tg8t, train_loss=0.2, val_loss=0.179]Epoch 25:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.24it/s, loss=0.275, v_num=tg8t, train_loss=0.2, val_loss=0.179]Epoch 25:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.23it/s, loss=0.265, v_num=tg8t, train_loss=0.21, val_loss=0.179]
Validating: 0it [00:00, ?it/s][AEpoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.37it/s, loss=0.265, v_num=tg8t, train_loss=0.21, val_loss=0.164]
                              [AEpoch 25:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.265, v_num=tg8t, train_loss=0.21, val_loss=0.164]        Epoch 26:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.265, v_num=tg8t, train_loss=0.21, val_loss=0.164]Epoch 26:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.19it/s, loss=0.255, v_num=tg8t, train_loss=0.2, val_loss=0.164]Epoch 26:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.51it/s, loss=0.255, v_num=tg8t, train_loss=0.2, val_loss=0.164]Epoch 26:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.51it/s, loss=0.251, v_num=tg8t, train_loss=0.126, val_loss=0.164]
Validating: 0it [00:00, ?it/s][AEpoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.66it/s, loss=0.251, v_num=tg8t, train_loss=0.126, val_loss=0.154]
                              [AEpoch 26:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.251, v_num=tg8t, train_loss=0.126, val_loss=0.154]        Epoch 27:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.251, v_num=tg8t, train_loss=0.126, val_loss=0.154]Epoch 27:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.06it/s, loss=0.241, v_num=tg8t, train_loss=0.142, val_loss=0.154]Epoch 27:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.41it/s, loss=0.241, v_num=tg8t, train_loss=0.142, val_loss=0.154]Epoch 27:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.41it/s, loss=0.244, v_num=tg8t, train_loss=0.331, val_loss=0.154]
Validating: 0it [00:00, ?it/s][AEpoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.58it/s, loss=0.244, v_num=tg8t, train_loss=0.331, val_loss=0.148]
                              [AEpoch 27:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.244, v_num=tg8t, train_loss=0.331, val_loss=0.148]        Epoch 28:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.244, v_num=tg8t, train_loss=0.331, val_loss=0.148]Epoch 28:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.09it/s, loss=0.237, v_num=tg8t, train_loss=0.171, val_loss=0.148]Epoch 28:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.39it/s, loss=0.237, v_num=tg8t, train_loss=0.171, val_loss=0.148]Epoch 28:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.38it/s, loss=0.226, v_num=tg8t, train_loss=0.144, val_loss=0.148]
Validating: 0it [00:00, ?it/s][AEpoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.57it/s, loss=0.226, v_num=tg8t, train_loss=0.144, val_loss=0.138]
                              [AEpoch 28:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.226, v_num=tg8t, train_loss=0.144, val_loss=0.138]        Epoch 29:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.226, v_num=tg8t, train_loss=0.144, val_loss=0.138]Epoch 29:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.08it/s, loss=0.219, v_num=tg8t, train_loss=0.151, val_loss=0.138]Epoch 29:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.46it/s, loss=0.219, v_num=tg8t, train_loss=0.151, val_loss=0.138]Epoch 29:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.45it/s, loss=0.210, v_num=tg8t, train_loss=0.173, val_loss=0.138]
Validating: 0it [00:00, ?it/s][AEpoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.66it/s, loss=0.210, v_num=tg8t, train_loss=0.173, val_loss=0.138]
                              [AEpoch 29:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.210, v_num=tg8t, train_loss=0.173, val_loss=0.138]        Epoch 30:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.210, v_num=tg8t, train_loss=0.173, val_loss=0.138]Epoch 30:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.15it/s, loss=0.202, v_num=tg8t, train_loss=0.135, val_loss=0.138]Epoch 30:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.55it/s, loss=0.202, v_num=tg8t, train_loss=0.135, val_loss=0.138]Epoch 30:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.55it/s, loss=0.201, v_num=tg8t, train_loss=0.223, val_loss=0.138]
Validating: 0it [00:00, ?it/s][AEpoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.76it/s, loss=0.201, v_num=tg8t, train_loss=0.223, val_loss=0.135]
                              [AEpoch 30:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.201, v_num=tg8t, train_loss=0.223, val_loss=0.135]        Epoch 31:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.201, v_num=tg8t, train_loss=0.223, val_loss=0.135]Epoch 31:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.95it/s, loss=0.195, v_num=tg8t, train_loss=0.143, val_loss=0.135]Epoch 31:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.23it/s, loss=0.195, v_num=tg8t, train_loss=0.143, val_loss=0.135]Epoch 31:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.23it/s, loss=0.194, v_num=tg8t, train_loss=0.146, val_loss=0.135]
Validating: 0it [00:00, ?it/s][AEpoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.34it/s, loss=0.194, v_num=tg8t, train_loss=0.146, val_loss=0.133]
                              [AEpoch 31:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.194, v_num=tg8t, train_loss=0.146, val_loss=0.133]        Epoch 32:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.194, v_num=tg8t, train_loss=0.146, val_loss=0.133]Epoch 32:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.13it/s, loss=0.189, v_num=tg8t, train_loss=0.129, val_loss=0.133]Epoch 32:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.49it/s, loss=0.189, v_num=tg8t, train_loss=0.129, val_loss=0.133]Epoch 32:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.48it/s, loss=0.181, v_num=tg8t, train_loss=0.185, val_loss=0.133]
Validating: 0it [00:00, ?it/s][AEpoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.90it/s, loss=0.181, v_num=tg8t, train_loss=0.185, val_loss=0.134]
                              [AEpoch 32:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.181, v_num=tg8t, train_loss=0.185, val_loss=0.134]        Epoch 33:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.181, v_num=tg8t, train_loss=0.185, val_loss=0.134]Epoch 33:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.05it/s, loss=0.176, v_num=tg8t, train_loss=0.141, val_loss=0.134]Epoch 33:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.37it/s, loss=0.176, v_num=tg8t, train_loss=0.141, val_loss=0.134]Epoch 33:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.37it/s, loss=0.174, v_num=tg8t, train_loss=0.123, val_loss=0.134]
Validating: 0it [00:00, ?it/s][AEpoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.71it/s, loss=0.174, v_num=tg8t, train_loss=0.123, val_loss=0.136]
                              [AEpoch 33:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.174, v_num=tg8t, train_loss=0.123, val_loss=0.136]        Epoch 34:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.174, v_num=tg8t, train_loss=0.123, val_loss=0.136]Epoch 34:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.97it/s, loss=0.170, v_num=tg8t, train_loss=0.138, val_loss=0.136]Epoch 34:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.21it/s, loss=0.170, v_num=tg8t, train_loss=0.138, val_loss=0.136]Epoch 34:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.21it/s, loss=0.167, v_num=tg8t, train_loss=0.134, val_loss=0.136]
Validating: 0it [00:00, ?it/s][AEpoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.52it/s, loss=0.167, v_num=tg8t, train_loss=0.134, val_loss=0.136]
                              [AEpoch 34:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.167, v_num=tg8t, train_loss=0.134, val_loss=0.136]        Epoch 35:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.167, v_num=tg8t, train_loss=0.134, val_loss=0.136]Epoch 35:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.94it/s, loss=0.164, v_num=tg8t, train_loss=0.143, val_loss=0.136]Epoch 35:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.18it/s, loss=0.164, v_num=tg8t, train_loss=0.143, val_loss=0.136]Epoch 35:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.18it/s, loss=0.159, v_num=tg8t, train_loss=0.0972, val_loss=0.136]
Validating: 0it [00:00, ?it/s][AEpoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.49it/s, loss=0.159, v_num=tg8t, train_loss=0.0972, val_loss=0.134]
                              [AEpoch 35:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.159, v_num=tg8t, train_loss=0.0972, val_loss=0.134]        Epoch 36:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.159, v_num=tg8t, train_loss=0.0972, val_loss=0.134]Epoch 36:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:01,  1.98it/s, loss=0.156, v_num=tg8t, train_loss=0.142, val_loss=0.134]Epoch 36:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.30it/s, loss=0.156, v_num=tg8t, train_loss=0.142, val_loss=0.134]Epoch 36:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  3.29it/s, loss=0.154, v_num=tg8t, train_loss=0.0805, val_loss=0.134]
Validating: 0it [00:00, ?it/s][AEpoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.56it/s, loss=0.154, v_num=tg8t, train_loss=0.0805, val_loss=0.136]
                              [ASaving latest checkpoint..
I0611 07:44:21.697522 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 00037: early stopping triggered.
I0611 07:44:21.698323 140711206703488 early_stopping.py:237] Epoch 00037: early stopping triggered.
Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.42it/s, loss=0.154, v_num=tg8t, train_loss=0.0805, val_loss=0.136]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=31.ckpt (current_epoch=36)
plans_lib.FilterScansOrJoins()
plans_lib.GatherUnaryFiltersInfo()
postgres.EstimateFilterRows()
9 unique filters
{('company_type AS ct', "((ct.kind)::text = 'production companies'::text)"): 1,
 ('info_type AS it', "((it.info)::text = 'bottom 10 rank'::text)"): 1,
 ('info_type AS it', "((it.info)::text = 'top 250 rank'::text)"): 1,
 ('movie_companies AS mc', "((mc.note !~~ '%(as Metro-Goldwyn-Mayer Pictures)%'::text) AND ((mc.note ~~ '%(co-production)%'::text) OR (mc.note ~~ '%(presents)%'::text)))"): 27149,
 ('movie_companies AS mc', "((mc.note !~~ '%(as Metro-Goldwyn-Mayer Pictures)%'::text) AND (mc.note ~~ '%(co-production)%'::text))"): 14028,
 ('movie_companies AS mc', "(mc.note !~~ '%(as Metro-Goldwyn-Mayer Pictures)%'::text)"): 1328998,
 ('title AS t', '((t.production_year >= 2005) AND (t.production_year <= 2010))'): 725345,
 ('title AS t', '(t.production_year > 2000)'): 1377608,
 ('title AS t', '(t.production_year > 2010)'): 390764}
8 rels: ['company_name' 'company_type' 'info_type' 'keyword' 'movie_companies'
 'movie_info_idx' 'movie_keyword' 'title']
8 rel_ids: ['company_name AS cn' 'company_type AS ct' 'info_type AS it'
 'keyword AS k' 'movie_companies AS mc' 'movie_info_idx AS mi_idx'
 'movie_keyword AS mk' 'title AS t']
3 scans: ['Index Only Scan' 'Index Scan' 'Seq Scan']
3 joins: ['Hash Join' 'Merge Join' 'Nested Loop']
11 all ops: ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Only Scan'
 'Index Scan' 'Merge Join' 'Nested Loop' 'PartialAggregate' 'Seq Scan'
 'SimpleAggregate']
Calling make_and_featurize_trees()...
took 0.0s
num_total_subtrees=16 num_featurized_subtrees=16 num_new_datapoints=16
head
  query=2.269 feat=23.0 cost=943.509
  query=2.269 feat=16.0 cost=943.509
  query=2.269 feat=10.0 cost=943.509
tail
  query=2.313 feat=5.0 cost=55.812
  query=2.313 feat=10.0 cost=55.812
  query=2.313 feat=16.0 cost=55.812
costs stats mean 4.773597846400859 std 1.1998888582630085
num_train=14 num_validation=2
MakeModel afresh
InitializeModel curr_value_iter=0
Initialized from SIM weights.
iter 0 lr 0.001
number of model parameters: 605473 (~= 2.3MB)
BalsaModel(
  (model): TreeConvolution(
    (query_mlp): Sequential(
      (0): Linear(in_features=8, out_features=128, bias=True)
      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (2): LeakyReLU(negative_slope=0.01)
      (3): Linear(in_features=128, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=64, out_features=32, bias=True)
    )
    (conv): Sequential(
      (0): TreeConv1d(
        (weights): Conv1d(59, 512, kernel_size=(3,), stride=(3,))
      )
      (1): TreeStandardize()
      (2): TreeAct(
        (activation): LeakyReLU(negative_slope=0.01)
      )
      (3): TreeConv1d(
        (weights): Conv1d(512, 256, kernel_size=(3,), stride=(3,))
      )
      (4): TreeStandardize()
      (5): TreeAct(
        (activation): LeakyReLU(negative_slope=0.01)
      )
      (6): TreeConv1d(
        (weights): Conv1d(256, 128, kernel_size=(3,), stride=(3,))
      )
      (7): TreeStandardize()
      (8): TreeAct(
        (activation): LeakyReLU(negative_slope=0.01)
      )
      (9): TreeMaxPool()
    )
    (out_mlp): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): LeakyReLU(negative_slope=0.01)
      (3): Linear(in_features=64, out_features=32, bias=True)
      (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (5): LeakyReLU(negative_slope=0.01)
      (6): Linear(in_features=32, out_features=1, bias=True)
    )
  )
)
GPU available: False, used: False
I0611 07:44:21.818875 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 07:44:21.819370 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
No best checkpoint found (run validaiton yet?); model left unchanged.
---------------------------------------
Planning took 49.3ms
q1a, predicted time: 196.1
q1a,(predicted 196.1),/*+ MergeJoin(mc mi_idx it ct t)
 NestLoop(mc mi_idx it ct)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(mc)
 SeqScan(mi_idx)
 IndexScan(it)
 IndexScan(ct)
 SeqScan(t)
 Leading((((mc (mi_idx it)) ct) t)) */
---------------------------------------
Planning took 361.3ms
q1b, predicted time: 182.1
q1b,(predicted 182.1),/*+ HashJoin(ct mc mi_idx it t)
 HashJoin(ct mc mi_idx it)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(ct)
 SeqScan(mc)
 SeqScan(mi_idx)
 IndexScan(it)
 SeqScan(t)
 Leading(((ct (mc (mi_idx it))) t)) */
---------------------------------------
Planning took 497.5ms
q1c, predicted time: 82.8
q1c,(predicted 82.8),/*+ MergeJoin(mc ct t it mi_idx)
 MergeJoin(it mi_idx)
 MergeJoin(mc ct t)
 NestLoop(mc ct)
 IndexScan(mc)
 IndexScan(ct)
 SeqScan(t)
 SeqScan(it)
 IndexScan(mi_idx)
 Leading((((mc ct) t) (it mi_idx))) */
---------------------------------------
Planning took 56.0ms
q1d, predicted time: 198.2
q1d,(predicted 198.2),/*+ MergeJoin(mc mi_idx it ct t)
 NestLoop(mc mi_idx it ct)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(mc)
 SeqScan(mi_idx)
 IndexScan(it)
 IndexScan(ct)
 SeqScan(t)
 Leading((((mc (mi_idx it)) ct) t)) */
Waiting on Ray tasks...value_iter=0
Running 1a: hinted plan
Merge Join cost=196.1095428466797
  Nested Loop cost=193.39768981933594
    Hash Join cost=201.33428955078125
      Seq Scan [movie_companies AS mc] cost=0.64
      Hash Join cost=215.78565979003906
        Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41
    Index Scan [company_type AS ct] cost=1.05
  Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,5049.6,/*+ MergeJoin(mc mi_idx it ct t)
 NestLoop(mc mi_idx it ct)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(mc)
 SeqScan(mi_idx)
 IndexScan(it)
 IndexScan(ct)
 SeqScan(t)
 Leading((((mc (mi_idx it)) ct) t)) */
1a Execution time: 5049.6 (predicted 196.1) curr_timeout_ms=None
Expert plan: latency, predicted, hint
  943.5 (predicted 286.2)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  3554909.2  196.1  /*+ Leading((((mc (mi_idx it)) ct) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1b: hinted plan
Hash Join cost=182.11126708984375
  Hash Join cost=184.40060424804688
    Seq Scan [company_type AS ct] cost=1.05
    Hash Join cost=189.60125732421875
      Seq Scan [movie_companies AS mc] cost=0.61
      Hash Join cost=202.66827392578125
        Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41
  Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,519.3,/*+ HashJoin(ct mc mi_idx it t)
 HashJoin(ct mc mi_idx it)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(ct)
 SeqScan(mc)
 SeqScan(mi_idx)
 IndexScan(it)
 SeqScan(t)
 Leading(((ct (mc (mi_idx it))) t)) */
1b Execution time: 519.3 (predicted 182.1) curr_timeout_ms=None
Expert plan: latency, predicted, hint
  62.3 (predicted 301.4)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3342634.8  182.1  /*+ Leading(((ct (mc (mi_idx it))) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Merge Join cost=82.79776000976562
  Merge Join cost=76.83545684814453
    Nested Loop cost=72.76242065429688
      Index Scan [movie_companies AS mc] cost=0.62
      Index Scan [company_type AS ct] cost=1.05
    Seq Scan [title AS t] cost=0.58
  Merge Join cost=72.90064239501953
    Seq Scan [info_type AS it] cost=2.41
    Index Scan [movie_info_idx AS mi_idx] cost=13685.15

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,2999.8,/*+ MergeJoin(mc ct t it mi_idx)
 MergeJoin(it mi_idx)
 MergeJoin(mc ct t)
 NestLoop(mc ct)
 IndexScan(mc)
 IndexScan(ct)
 SeqScan(t)
 SeqScan(it)
 IndexScan(mi_idx)
 Leading((((mc ct) t) (it mi_idx))) */
1c Execution time: 2999.8 (predicted 82.8) curr_timeout_ms=None
Expert plan: latency, predicted, hint
  56.8 (predicted 77.0)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1739190.4  82.8  /*+ Leading((((mc ct) t) (it mi_idx))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1d: hinted plan
Merge Join cost=198.22665405273438
  Nested Loop cost=195.46554565429688
    Hash Join cost=203.3881072998047
      Seq Scan [movie_companies AS mc] cost=0.61
      Hash Join cost=220.34225463867188
        Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41
    Index Scan [company_type AS ct] cost=1.05
  Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,2041.2,/*+ MergeJoin(mc mi_idx it ct t)
 NestLoop(mc mi_idx it ct)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(mc)
 SeqScan(mi_idx)
 IndexScan(it)
 IndexScan(ct)
 SeqScan(t)
 Leading((((mc (mi_idx it)) ct) t)) */
1d Execution time: 2041.2 (predicted 198.2) curr_timeout_ms=None
Expert plan: latency, predicted, hint
  55.8 (predicted 306.5)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3586794.0  198.2  /*+ Leading((((mc (mi_idx it)) ct) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
---------------------------------------
Planning took 150.3ms
[Test set] q2a, predicted time: 187.9
q2a,(predicted 187.9),/*+ HashJoin(mc mk k cn t)
 HashJoin(mc mk k cn)
 HashJoin(mc mk k)
 HashJoin(mk k)
 SeqScan(mc)
 SeqScan(mk)
 IndexScan(k)
 SeqScan(cn)
 SeqScan(t)
 Leading((((mc (mk k)) cn) t)) */
---------------------------------------
Planning took 140.5ms
[Test set] q2b, predicted time: 188.1
q2b,(predicted 188.1),/*+ HashJoin(mc mk k cn t)
 HashJoin(mc mk k cn)
 HashJoin(mc mk k)
 HashJoin(mk k)
 SeqScan(mc)
 SeqScan(mk)
 IndexScan(k)
 SeqScan(cn)
 SeqScan(t)
 Leading((((mc (mk k)) cn) t)) */
---------------------------------------
Planning took 154.4ms
[Test set] q2c, predicted time: 188.1
q2c,(predicted 188.1),/*+ HashJoin(mc mk k cn t)
 HashJoin(mc mk k cn)
 HashJoin(mc mk k)
 HashJoin(mk k)
 SeqScan(mc)
 SeqScan(mk)
 IndexScan(k)
 SeqScan(cn)
 SeqScan(t)
 Leading((((mc (mk k)) cn) t)) */
---------------------------------------
Planning took 189.6ms
[Test set] q2d, predicted time: 187.7
q2d,(predicted 187.7),/*+ HashJoin(mc mk k cn t)
 MergeJoin(mc mk k cn)
 HashJoin(mc mk k)
 NestLoop(mk k)
 SeqScan(mc)
 SeqScan(mk)
 IndexScan(k)
 SeqScan(cn)
 SeqScan(t)
 Leading((((mc (mk k)) cn) t)) */
[Test set] Waiting on Ray tasks...value_iter=0
[Test set] Running 2a: hinted plan
Hash Join cost=187.91482543945312
  Hash Join cost=186.90206909179688
    Hash Join cost=191.27734375
      Seq Scan [movie_companies AS mc] cost=0.54
      Hash Join cost=189.59962463378906
        Seq Scan [movie_keyword AS mk] cost=1229.38
        Index Scan [keyword AS k] cost=2626.12
    Seq Scan [company_name AS cn] cost=0.45
  Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,630.6,/*+ HashJoin(mc mk k cn t)
 HashJoin(mc mk k cn)
 HashJoin(mc mk k)
 HashJoin(mk k)
 SeqScan(mc)
 SeqScan(mk)
 IndexScan(k)
 SeqScan(cn)
 SeqScan(t)
 Leading((((mc (mk k)) cn) t)) */
2a Execution time: 630.6 (predicted 187.9) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 214.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3430954.5  187.9  /*+ Leading((((mc (mk k)) cn) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Hash Join cost=188.0839080810547
  Hash Join cost=187.0697784423828
    Hash Join cost=191.44326782226562
      Seq Scan [movie_companies AS mc] cost=0.54
      Hash Join cost=189.83123779296875
        Seq Scan [movie_keyword AS mk] cost=1229.38
        Index Scan [keyword AS k] cost=2626.12
    Seq Scan [company_name AS cn] cost=0.45
  Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,522.1,/*+ HashJoin(mc mk k cn t)
 HashJoin(mc mk k cn)
 HashJoin(mc mk k)
 HashJoin(mk k)
 SeqScan(mc)
 SeqScan(mk)
 IndexScan(k)
 SeqScan(cn)
 SeqScan(t)
 Leading((((mc (mk k)) cn) t)) */
2b Execution time: 522.1 (predicted 188.1) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 214.3)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3433520.8  188.1  /*+ Leading((((mc (mk k)) cn) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Hash Join cost=188.13204956054688
  Hash Join cost=187.11749267578125
    Hash Join cost=191.4904327392578
      Seq Scan [movie_companies AS mc] cost=0.54
wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: \ 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: | 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: / 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)wandb: - 0.060 MB of 0.060 MB uploaded (0.002 MB deduped)    Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=2.014, v_num=tg8t, train_loss=1.19]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 61.00it/s, loss=1.834, v_num=tg8t, train_loss=1.11]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 4: : 2it [00:00, 34.08it/s, loss=1.834, v_num=tg8t, train_loss=1.11, val_loss=0.364]     
                              [AEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.834, v_num=tg8t, train_loss=1.11, val_loss=0.364]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.834, v_num=tg8t, train_loss=1.11, val_loss=0.364]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 44.94it/s, loss=1.708, v_num=tg8t, train_loss=1.08, val_loss=0.364]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.708, v_num=tg8t, train_loss=1.08, val_loss=0.364]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.708, v_num=tg8t, train_loss=1.08, val_loss=0.364]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.83it/s, loss=1.618, v_num=tg8t, train_loss=1.08, val_loss=0.364]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.618, v_num=tg8t, train_loss=1.08, val_loss=0.364]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.618, v_num=tg8t, train_loss=1.08, val_loss=0.364]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.89it/s, loss=1.554, v_num=tg8t, train_loss=1.1, val_loss=0.364]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.554, v_num=tg8t, train_loss=1.1, val_loss=0.364]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.554, v_num=tg8t, train_loss=1.1, val_loss=0.364]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.04it/s, loss=1.505, v_num=tg8t, train_loss=1.12, val_loss=0.364]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.505, v_num=tg8t, train_loss=1.12, val_loss=0.364]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.505, v_num=tg8t, train_loss=1.12, val_loss=0.364]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.49it/s, loss=1.468, v_num=tg8t, train_loss=1.13, val_loss=0.364]
Validating: 0it [00:00, ?it/s][AEpoch 9: : 2it [00:00, 51.56it/s, loss=1.468, v_num=tg8t, train_loss=1.13, val_loss=0.657]                     
                              [AEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.468, v_num=tg8t, train_loss=1.13, val_loss=0.657]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.468, v_num=tg8t, train_loss=1.13, val_loss=0.657]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 35.79it/s, loss=1.438, v_num=tg8t, train_loss=1.14, val_loss=0.657]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.438, v_num=tg8t, train_loss=1.14, val_loss=0.657]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.438, v_num=tg8t, train_loss=1.14, val_loss=0.657]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.56it/s, loss=1.413, v_num=tg8t, train_loss=1.14, val_loss=0.657]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.413, v_num=tg8t, train_loss=1.14, val_loss=0.657]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.413, v_num=tg8t, train_loss=1.14, val_loss=0.657]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.90it/s, loss=1.391, v_num=tg8t, train_loss=1.13, val_loss=0.657]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.391, v_num=tg8t, train_loss=1.13, val_loss=0.657]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.391, v_num=tg8t, train_loss=1.13, val_loss=0.657]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.02it/s, loss=1.371, v_num=tg8t, train_loss=1.11, val_loss=0.657]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.371, v_num=tg8t, train_loss=1.11, val_loss=0.657]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.371, v_num=tg8t, train_loss=1.11, val_loss=0.657]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.54it/s, loss=1.353, v_num=tg8t, train_loss=1.1, val_loss=0.657]
Validating: 0it [00:00, ?it/s][AEpoch 14: : 2it [00:00, 43.00it/s, loss=1.353, v_num=tg8t, train_loss=1.1, val_loss=0.524]                     
                              [AEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.353, v_num=tg8t, train_loss=1.1, val_loss=0.524]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.353, v_num=tg8t, train_loss=1.1, val_loss=0.524]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 44.77it/s, loss=1.337, v_num=tg8t, train_loss=1.09, val_loss=0.524]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.337, v_num=tg8t, train_loss=1.09, val_loss=0.524]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.337, v_num=tg8t, train_loss=1.09, val_loss=0.524]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.90it/s, loss=1.321, v_num=tg8t, train_loss=1.07, val_loss=0.524]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.321, v_num=tg8t, train_loss=1.07, val_loss=0.524]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.321, v_num=tg8t, train_loss=1.07, val_loss=0.524]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.67it/s, loss=1.307, v_num=tg8t, train_loss=1.06, val_loss=0.524]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.307, v_num=tg8t, train_loss=1.06, val_loss=0.524]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.307, v_num=tg8t, train_loss=1.06, val_loss=0.524]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.87it/s, loss=1.294, v_num=tg8t, train_loss=1.06, val_loss=0.524]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.294, v_num=tg8t, train_loss=1.06, val_loss=0.524]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.294, v_num=tg8t, train_loss=1.06, val_loss=0.524]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.62it/s, loss=1.282, v_num=tg8t, train_loss=1.05, val_loss=0.524]
Validating: 0it [00:00, ?it/s][AEpoch 19: : 2it [00:00, 54.55it/s, loss=1.282, v_num=tg8t, train_loss=1.05, val_loss=0.365]                     
                              [ASaving latest checkpoint..
I0611 07:53:07.388777 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 00020: early stopping triggered.
I0611 07:53:07.389410 140711206703488 early_stopping.py:237] Epoch 00020: early stopping triggered.
Epoch 19: : 2it [00:00, 33.90it/s, loss=1.282, v_num=tg8t, train_loss=1.05, val_loss=0.365]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=4.ckpt (current_epoch=19)
---------------------------------------
Planning took 237.3ms
q1a, predicted time: 992.8
q1a,(predicted 992.8),/*+ HashJoin(ct mc t mi_idx it)
 HashJoin(t mi_idx it)
 HashJoin(mi_idx it)
 NestLoop(ct mc)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(t)
 IndexScan(mi_idx)
 SeqScan(it)
 Leading(((ct mc) (t (mi_idx it)))) */
---------------------------------------
Planning took 326.5ms
q1b, predicted time: 994.5
q1b,(predicted 994.5),/*+ MergeJoin(ct mc it mi_idx t)
 HashJoin(it mi_idx t)
 HashJoin(mi_idx t)
 NestLoop(ct mc)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(it)
 IndexScan(mi_idx)
 SeqScan(t)
 Leading(((ct mc) (it (mi_idx t)))) */
---------------------------------------
Planning took 212.7ms
q1c, predicted time: 1000.4
q1c,(predicted 1000.4),/*+ HashJoin(ct mc it t mi_idx)
 NestLoop(it t mi_idx)
 NestLoop(t mi_idx)
 HashJoin(ct mc)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(it)
 SeqScan(t)
 IndexScan(mi_idx)
 Leading(((ct mc) (it (t mi_idx)))) */
---------------------------------------
Planning took 251.3ms
q1d, predicted time: 991.1
q1d,(predicted 991.1),/*+ HashJoin(t ct mc mi_idx it)
 NestLoop(mi_idx it)
 MergeJoin(t ct mc)
 NestLoop(ct mc)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 SeqScan(it)
 Leading(((t (ct mc)) (mi_idx it))) */
Waiting on Ray tasks...value_iter=3
Running 1a: hinted plan
Hash Join cost=992.8331909179688
  Nested Loop cost=981.3230590820312
    Index Scan [company_type AS ct] cost=1.05
    Index Scan [movie_companies AS mc] cost=0.64
  Hash Join cost=984.2127075195312
    Seq Scan [title AS t] cost=0.58
    Hash Join cost=981.1830444335938
      Index Scan [movie_info_idx AS mi_idx] cost=13685.15
      Seq Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,346.7,/*+ HashJoin(ct mc t mi_idx it)
 HashJoin(t mi_idx it)
 HashJoin(mi_idx it)
 NestLoop(ct mc)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(t)
 IndexScan(mi_idx)
 SeqScan(it)
 Leading(((ct mc) (t (mi_idx it)))) */
1a Execution time: 346.7 (predicted 992.8) curr_timeout_ms=6003.492
Expert plan: latency, predicted, hint
  943.5 (predicted 997.8)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  4810934.5  992.8  /*+ Leading(((ct mc) (t (mi_idx it)))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1b: hinted plan
Merge Join cost=994.5178833007812
  Nested Loop cost=982.8517456054688
    Index Scan [company_type AS ct] cost=1.05
    Index Scan [movie_companies AS mc] cost=0.61
  Hash Join cost=985.2731323242188
    Seq Scan [info_type AS it] cost=2.41
    Hash Join cost=982.5609130859375
      Index Scan [movie_info_idx AS mi_idx] cost=13685.15
      Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,553.1,/*+ MergeJoin(ct mc it mi_idx t)
 HashJoin(it mi_idx t)
 HashJoin(mi_idx t)
 NestLoop(ct mc)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(it)
 IndexScan(mi_idx)
 SeqScan(t)
 Leading(((ct mc) (it (mi_idx t)))) */
1b Execution time: 553.1 (predicted 994.5) curr_timeout_ms=6003.492
Expert plan: latency, predicted, hint
  62.3 (predicted 1000.0)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  4776360.0  994.5  /*+ Leading(((ct mc) (it (mi_idx t)))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Hash Join cost=1000.4077758789062
  Hash Join cost=988.5040893554688
    Index Scan [company_type AS ct] cost=1.05
    Index Scan [movie_companies AS mc] cost=0.62
  Nested Loop cost=990.5199584960938
    Seq Scan [info_type AS it] cost=2.41
    Nested Loop cost=988.0479736328125
      Seq Scan [title AS t] cost=0.58
      Index Scan [movie_info_idx AS mi_idx] cost=13685.15

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,396.3,/*+ HashJoin(ct mc it t mi_idx)
 NestLoop(it t mi_idx)
 NestLoop(t mi_idx)
 HashJoin(ct mc)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(it)
 SeqScan(t)
 IndexScan(mi_idx)
 Leading(((ct mc) (it (t mi_idx)))) */
1c Execution time: 396.3 (predicted 1000.4) curr_timeout_ms=6003.492
Expert plan: latency, predicted, hint
  56.8 (predicted 1006.5)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1772209.5  1000.4  /*+ Leading(((ct mc) (it (t mi_idx)))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1d: hinted plan
Hash Join cost=991.0628662109375
  Merge Join cost=981.53857421875
    Seq Scan [title AS t] cost=0.58
    Nested Loop cost=978.6656494140625
      Index Scan [company_type AS ct] cost=1.05
      Index Scan [movie_companies AS mc] cost=0.61
  Nested Loop cost=978.8772583007812
    Index Scan [movie_info_idx AS mi_idx] cost=13685.15
    Seq Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,757.4,/*+ HashJoin(t ct mc mi_idx it)
 NestLoop(mi_idx it)
 MergeJoin(t ct mc)
 NestLoop(ct mc)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 SeqScan(it)
 Leading(((t (ct mc)) (mi_idx it))) */
1d Execution time: 757.4 (predicted 991.1) curr_timeout_ms=6003.492
Expert plan: latency, predicted, hint
  55.8 (predicted 994.8)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  4954829.5  991.1  /*+ Leading(((t (ct mc)) (mi_idx it))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
---------------------------------------
Planning took 136.2ms
[Test set] q2a, predicted time: 1067.8
q2a,(predicted 1067.8),/*+ HashJoin(t k cn mk mc)
 HashJoin(k cn mk mc)
 HashJoin(cn mk mc)
 HashJoin(mk mc)
 SeqScan(t)
 SeqScan(k)
 SeqScan(cn)
 IndexScan(mk)
 SeqScan(mc)
 Leading((t (k (cn (mk mc))))) */
---------------------------------------
Planning took 214.6ms
[Test set] q2b, predicted time: 1069.2
q2b,(predicted 1069.2),/*+ HashJoin(t k cn mk mc)
 HashJoin(k cn mk mc)
 HashJoin(cn mk mc)
 HashJoin(mk mc)
 SeqScan(t)
 SeqScan(k)
 SeqScan(cn)
 IndexScan(mk)
 SeqScan(mc)
 Leading((t (k (cn (mk mc))))) */
---------------------------------------
Planning took 88.1ms
[Test set] q2c, predicted time: 1071.3
q2c,(predicted 1071.3),/*+ MergeJoin(t k mk cn mc)
 MergeJoin(k mk cn mc)
 HashJoin(mk cn mc)
 HashJoin(cn mc)
 IndexScan(t)
 IndexScan(k)
 SeqScan(mk)
 SeqScan(cn)
 SeqScan(mc)
 Leading((t (k (mk (cn mc))))) */
---------------------------------------
Planning took 250.4ms
[Test set] q2d, predicted time: 1059.0
q2d,(predicted 1059.0),/*+ MergeJoin(t k cn mk mc)
 MergeJoin(k cn mk mc)
 HashJoin(cn mk mc)
 HashJoin(mk mc)
 IndexScan(t)
 IndexScan(k)
 SeqScan(cn)
 IndexScan(mk)
 SeqScan(mc)
 Leading((t (k (cn (mk mc))))) */
[Test set] Waiting on Ray tasks...value_iter=3
[Test set] Running 2a: hinted plan
Hash Join cost=1067.7857666015625
  Seq Scan [title AS t] cost=0.49
  Hash Join cost=1047.133056640625
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=1051.8165283203125
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=1068.494873046875
        Index Scan [movie_keyword AS mk] cost=1229.38
        Seq Scan [movie_companies AS mc] cost=0.54

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,38061.8,/*+ HashJoin(t k cn mk mc)
 HashJoin(k cn mk mc)
 HashJoin(cn mk mc)
 HashJoin(mk mc)
 SeqScan(t)
 SeqScan(k)
 SeqScan(cn)
 IndexScan(mk)
 SeqScan(mc)
 Leading((t (k (cn (mk mc))))) */
2a Execution time: 38061.8 (predicted 1067.8) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 1186.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3585258.5  1067.8  /*+ Leading((t (k (cn (mk mc))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Hash Join cost=1069.167236328125
  Seq Scan [title AS t] cost=0.49
  Hash Join cost=1048.457763671875
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=1053.4146728515625
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=1070.9154052734375
        Index Scan [movie_keyword AS mk] cost=1229.38
        Seq Scan [movie_companies AS mc] cost=0.54

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,38362.0,/*+ HashJoin(t k cn mk mc)
 HashJoin(k cn mk mc)
 HashJoin(cn mk mc)
 HashJoin(mk mc)
 SeqScan(t)
 SeqScan(k)
 SeqScan(cn)
 IndexScan(mk)
 SeqScan(mc)
 Leading((t (k (cn (mk mc))))) */
2b Execution time: 38362.0 (predicted 1069.2) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 1188.7)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3587919.8  1069.2  /*+ Leading((t (k (cn (mk mc))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Merge Join cost=1071.3289794921875
  Index Scan [title AS t] cost=0.49
  Merge Join cost=1051.501220703125
    Index Scan [keyword AS k] cost=2626.12
    Hash Join cost=1055.650390625
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=1071.228759765625
        Seq Scan [company_name AS cn] cost=0.45
        Seq Scan [movie_companies AS mc] cost=0.54

filters
{'company_name AS cn': "((cn.country_code)::text = '[sm]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2c,328.7,/*+ MergeJoin(t k mk cn mc)
 MergeJoin(k mk cn mc)
 HashJoin(mk cn mc)
 HashJoin(cn mc)
 IndexScan(t)
 IndexScan(k)
 SeqScan(mk)
 SeqScan(cn)
 SeqScan(mc)
 Leading((t (k (mk (cn mc))))) */
2c Execution time: 328.7 (predicted 1071.3) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  269.8 (predicted 1189.1)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3599081.5  1071.3  /*+ Leading((t (k (mk (cn mc))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2d: hinted plan
Merge Join cost=1058.97900390625
  Index Scan [title AS t] cost=0.49
  Merge Join cost=1040.660400390625
    Index Scan [keyword AS k] cost=2626.12
    Hash Join cost=1043.115966796875
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=1056.89111328125
        Index Scan [movie_keyword AS mk] cost=1229.38
        Seq Scan [movie_companies AS mc] cost=0.54

filters
{'company_name AS cn': "((cn.country_code)::text = '[us]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2d,44034.5,/*+ MergeJoin(t k cn mk mc)
 MergeJoin(k cn mk mc)
 HashJoin(cn mk mc)
 HashJoin(mk mc)
 IndexScan(t)
 IndexScan(k)
 SeqScan(cn)
 IndexScan(mk)
 SeqScan(mc)
 Leading((t (k (cn (mk mc))))) */
2d Execution time: 44034.5 (predicted 1059.0) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  4175.6 (predicted 1237.3)  /*+ Leading(((((k mk) t) mc) cn)) */
SIM-predicted costs, predicted latency, plan: 
  3607937.5  1059.0  /*+ Leading((t (k (cn (mk mc))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Dropping buffer cache.
8 rels: ['company_name' 'company_type' 'info_type' 'keyword' 'movie_companies'
 'movie_info_idx' 'movie_keyword' 'title']
8 rel_ids: ['company_name AS cn' 'company_type AS ct' 'info_type AS it'
 'keyword AS k' 'movie_companies AS mc' 'movie_info_idx AS mi_idx'
 'movie_keyword AS mk' 'title AS t']
3 scans: ['Index Only Scan' 'Index Scan' 'Seq Scan']
3 joins: ['Hash Join' 'Merge Join' 'Nested Loop']
11 all ops: ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Only Scan'
 'Index Scan' 'Merge Join' 'Nested Loop' 'PartialAggregate' 'Seq Scan'
 'SimpleAggregate']
Calling make_and_featurize_trees()...
took 0.0s
num_total_subtrees=64 num_featurized_subtrees=16 num_new_datapoints=16
head
  query=2.269 feat=21.0 cost=346.73
  query=2.269 feat=5.0 cost=346.73
  query=2.269 feat=10.0 cost=346.73
tail
  query=2.313 feat=5.0 cost=757.425
  query=2.313 feat=5.0 cost=757.425
  query=2.313 feat=10.0 cost=757.425
costs stats mean 6.19617541299033 std 0.30315275424052257
num_train=14 num_validation=2
InitializeModel curr_value_iter=4
Assigning real model := 0.0*SIM + 1.0*previous real model
iter 4 lr 0.001
GPU available: False, used: False
I0611 07:55:12.171376 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 07:55:12.172049 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]                                           /home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 28.53it/s, loss=1.043, v_num=tg8t, train_loss=1.04]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.043, v_num=tg8t, train_loss=1.04]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.043, v_num=tg8t, train_loss=1.04]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.46it/s, loss=1.045, v_num=tg8t, train_loss=1.05]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.045, v_num=tg8t, train_loss=1.05]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.045, v_num=tg8t, train_loss=1.05]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.98it/s, loss=1.040, v_num=tg8t, train_loss=1.03]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.040, v_num=tg8t, train_loss=1.03]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.040, v_num=tg8t, train_loss=1.03]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.27it/s, loss=1.036, v_num=tg8t, train_loss=1.03]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.036, v_num=tg8t, train_loss=1.03]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.036, v_num=tg8t, train_loss=1.03]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.42it/s, loss=1.034, v_num=tg8t, train_loss=1.02]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 4: : 2it [00:00, 33.93it/s, loss=1.034, v_num=tg8t, train_loss=1.02, val_loss=0.793]     
                              [AEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.034, v_num=tg8t, train_loss=1.02, val_loss=0.793]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.034, v_num=tg8t, train_loss=1.02, val_loss=0.793]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 49.70it/s, loss=1.029, v_num=tg8t, train_loss=1, val_loss=0.793]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.029, v_num=tg8t, train_loss=1, val_loss=0.793]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.029, v_num=tg8t, train_loss=1, val_loss=0.793]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.98it/s, loss=1.021, v_num=tg8t, train_loss=0.977, val_loss=0.793]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.021, v_num=tg8t, train_loss=0.977, val_loss=0.793]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.021, v_num=tg8t, train_loss=0.977, val_loss=0.793]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.51it/s, loss=1.010, v_num=tg8t, train_loss=0.93, val_loss=0.793]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.010, v_num=tg8t, train_loss=0.93, val_loss=0.793]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.010, v_num=tg8t, train_loss=0.93, val_loss=0.793]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.74it/s, loss=0.993, v_num=tg8t, train_loss=0.859, val_loss=0.793]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.993, v_num=tg8t, train_loss=0.859, val_loss=0.793]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.993, v_num=tg8t, train_loss=0.859, val_loss=0.793]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.45it/s, loss=0.970, v_num=tg8t, train_loss=0.759, val_loss=0.793]
Validating: 0it [00:00, ?it/s][AEpoch 9: : 2it [00:00, 29.73it/s, loss=0.970, v_num=tg8t, train_loss=0.759, val_loss=0.787]                     
                              [AEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.970, v_num=tg8t, train_loss=0.759, val_loss=0.787]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.970, v_num=tg8t, train_loss=0.759, val_loss=0.787]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 41.15it/s, loss=0.936, v_num=tg8t, train_loss=0.602, val_loss=0.787]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.936, v_num=tg8t, train_loss=0.602, val_loss=0.787]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.936, v_num=tg8t, train_loss=0.602, val_loss=0.787]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.32it/s, loss=0.893, v_num=tg8t, train_loss=0.412, val_loss=0.787]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.893, v_num=tg8t, train_loss=0.412, val_loss=0.787]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.893, v_num=tg8t, train_loss=0.412, val_loss=0.787]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.91it/s, loss=0.843, v_num=tg8t, train_loss=0.25, val_loss=0.787]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.843, v_num=tg8t, train_loss=0.25, val_loss=0.787]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.843, v_num=tg8t, train_loss=0.25, val_loss=0.787]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 54.18it/s, loss=0.795, v_num=tg8t, train_loss=0.17, val_loss=0.787]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.795, v_num=tg8t, train_loss=0.17, val_loss=0.787]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.795, v_num=tg8t, train_loss=0.17, val_loss=0.787]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.94it/s, loss=0.754, v_num=tg8t, train_loss=0.172, val_loss=0.787]
Validating: 0it [00:00, ?it/s][AEpoch 14: : 2it [00:00, 30.52it/s, loss=0.754, v_num=tg8t, train_loss=0.172, val_loss=0.136]                     
                              [AEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.754, v_num=tg8t, train_loss=0.172, val_loss=0.136]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.754, v_num=tg8t, train_loss=0.172, val_loss=0.136]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 47.08it/s, loss=0.719, v_num=tg8t, train_loss=0.196, val_loss=0.136]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.719, v_num=tg8t, train_loss=0.196, val_loss=0.136]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.719, v_num=tg8t, train_loss=0.196, val_loss=0.136]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.16it/s, loss=0.688, v_num=tg8t, train_loss=0.205, val_loss=0.136]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.688, v_num=tg8t, train_loss=0.205, val_loss=0.136]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.688, v_num=tg8t, train_loss=0.205, val_loss=0.136]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.54it/s, loss=0.661, v_num=tg8t, train_loss=0.197, val_loss=0.136]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.661, v_num=tg8t, train_loss=0.197, val_loss=0.136]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.661, v_num=tg8t, train_loss=0.197, val_loss=0.136]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.88it/s, loss=0.636, v_num=tg8t, train_loss=0.181, val_loss=0.136]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.636, v_num=tg8t, train_loss=0.181, val_loss=0.136]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.636, v_num=tg8t, train_loss=0.181, val_loss=0.136]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.38it/s, loss=0.612, v_num=tg8t, train_loss=0.165, val_loss=0.136]
Validating: 0it [00:00, ?it/s][AEpoch 19: : 2it [00:00, 46.26it/s, loss=0.612, v_num=tg8t, train_loss=0.165, val_loss=0.203]                     
                              [AEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.612, v_num=tg8t, train_loss=0.165, val_loss=0.203]Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.612, v_num=tg8t, train_loss=0.165, val_loss=0.203]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 40.40it/s, loss=0.568, v_num=tg8t, train_loss=0.156, val_loss=0.203]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.568, v_num=tg8t, train_loss=0.156, val_loss=0.203]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.568, v_num=tg8t, train_loss=0.156, val_loss=0.203]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.62it/s, loss=0.523, v_num=tg8t, train_loss=0.154, val_loss=0.203]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.523, v_num=tg8t, train_loss=0.154, val_loss=0.203]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.523, v_num=tg8t, train_loss=0.154, val_loss=0.203]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.29it/s, loss=0.480, v_num=tg8t, train_loss=0.158, val_loss=0.203]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.480, v_num=tg8t, train_loss=0.158, val_loss=0.203]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.480, v_num=tg8t, train_loss=0.158, val_loss=0.203]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.37it/s, loss=0.437, v_num=tg8t, train_loss=0.165, val_loss=0.203]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.437, v_num=tg8t, train_loss=0.165, val_loss=0.203]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.437, v_num=tg8t, train_loss=0.165, val_loss=0.203]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.19it/s, loss=0.394, v_num=tg8t, train_loss=0.169, val_loss=0.203]
Validating: 0it [00:00, ?it/s][AEpoch 24: : 2it [00:00, 51.49it/s, loss=0.394, v_num=tg8t, train_loss=0.169, val_loss=0.312]                     
                              [AEpoch 24:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.394, v_num=tg8t, train_loss=0.169, val_loss=0.312]Epoch 25:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.394, v_num=tg8t, train_loss=0.169, val_loss=0.312]Epoch 25:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 50.90it/s, loss=0.352, v_num=tg8t, train_loss=0.169, val_loss=0.312]Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.352, v_num=tg8t, train_loss=0.169, val_loss=0.312]        Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.352, v_num=tg8t, train_loss=0.169, val_loss=0.312]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.74it/s, loss=0.312, v_num=tg8t, train_loss=0.166, val_loss=0.312]Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.312, v_num=tg8t, train_loss=0.166, val_loss=0.312]        Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.312, v_num=tg8t, train_loss=0.166, val_loss=0.312]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.51it/s, loss=0.273, v_num=tg8t, train_loss=0.162, val_loss=0.312]Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.273, v_num=tg8t, train_loss=0.162, val_loss=0.312]        Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.273, v_num=tg8t, train_loss=0.162, val_loss=0.312]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.07it/s, loss=0.238, v_num=tg8t, train_loss=0.157, val_loss=0.312]Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.238, v_num=tg8t, train_loss=0.157, val_loss=0.312]        Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.238, v_num=tg8t, train_loss=0.157, val_loss=0.312]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.45it/s, loss=0.208, v_num=tg8t, train_loss=0.154, val_loss=0.312]
Validating: 0it [00:00, ?it/s][AEpoch 29: : 2it [00:00, 48.92it/s, loss=0.208, v_num=tg8t, train_loss=0.154, val_loss=0.236]                     
                              [ASaving latest checkpoint..
I0611 07:55:13.391749 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 00030: early stopping triggered.
I0611 07:55:13.392412 140711206703488 early_stopping.py:237] Epoch 00030: early stopping triggered.
Epoch 29: : 2it [00:00, 32.02it/s, loss=0.208, v_num=tg8t, train_loss=0.154, val_loss=0.236]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=14.ckpt (current_epoch=29)
---------------------------------------
Planning took 211.1ms
q1a, predicted time: 351.7
q1a,(predicted 351.7),/*+ HashJoin(mi_idx it ct mc t)
 HashJoin(ct mc t)
 HashJoin(mc t)
 HashJoin(mi_idx it)
 SeqScan(mi_idx)
 SeqScan(it)
 SeqScan(ct)
 SeqScan(mc)
 SeqScan(t)
 Leading(((mi_idx it) (ct (mc t)))) */
---------------------------------------
Planning took 28.6ms
q1b, predicted time: 640.3
q1b,(predicted 640.3),/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
---------------------------------------
Planning took 256.6ms
q1c, predicted time: 351.7
q1c,(predicted 351.7),/*+ HashJoin(ct t mc it mi_idx)
 HashJoin(it mi_idx)
 HashJoin(ct t mc)
 HashJoin(t mc)
 SeqScan(ct)
 SeqScan(t)
 SeqScan(mc)
 IndexScan(it)
 IndexScan(mi_idx)
 Leading(((ct (t mc)) (it mi_idx))) */
---------------------------------------
Planning took 30.8ms
q1d, predicted time: 641.7
q1d,(predicted 641.7),/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
Waiting on Ray tasks...value_iter=4
Running 1a: hinted plan
Hash Join cost=351.7386474609375
  Hash Join cost=338.1872863769531
    Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
    Seq Scan [info_type AS it] cost=2.41
  Hash Join cost=345.32257080078125
    Seq Scan [company_type AS ct] cost=1.05
    Hash Join cost=338.1322937011719
      Seq Scan [movie_companies AS mc] cost=0.64
      Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,478.4,/*+ HashJoin(mi_idx it ct mc t)
 HashJoin(ct mc t)
 HashJoin(mc t)
 HashJoin(mi_idx it)
 SeqScan(mi_idx)
 SeqScan(it)
 SeqScan(ct)
 SeqScan(mc)
 SeqScan(t)
 Leading(((mi_idx it) (ct (mc t)))) */
1a Execution time: 478.4 (predicted 351.7) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  943.5 (predicted 349.6)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  4665026.0  351.7  /*+ Leading(((mi_idx it) (ct (mc t)))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1b: hinted plan
Hash Join cost=640.3029174804688
  Seq Scan [title AS t] cost=0.58
  Hash Join cost=641.2347412109375
    Index Scan [company_type AS ct] cost=1.05
    Merge Join cost=643.60791015625
      Index Scan [movie_companies AS mc] cost=0.61
      Hash Join cost=646.3197021484375
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,654.6,/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
1b Execution time: 654.6 (predicted 640.3) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  62.3 (predicted 649.8)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3616136.0  640.3  /*+ Leading((t (ct (mc (mi_idx it))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Hash Join cost=351.72149658203125
  Hash Join cost=345.3645324707031
    Seq Scan [company_type AS ct] cost=1.05
    Hash Join cost=337.78155517578125
      Seq Scan [title AS t] cost=0.58
      Seq Scan [movie_companies AS mc] cost=0.62
  Hash Join cost=337.7392272949219
    Index Scan [info_type AS it] cost=2.41
    Index Scan [movie_info_idx AS mi_idx] cost=13685.15

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,1074.3,/*+ HashJoin(ct t mc it mi_idx)
 HashJoin(it mi_idx)
 HashJoin(ct t mc)
 HashJoin(t mc)
 SeqScan(ct)
 SeqScan(t)
 SeqScan(mc)
 IndexScan(it)
 IndexScan(mi_idx)
 Leading(((ct (t mc)) (it mi_idx))) */
1c Execution time: 1074.3 (predicted 351.7) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  56.8 (predicted 349.0)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1800509.2  351.7  /*+ Leading(((ct (t mc)) (it mi_idx))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1d: hinted plan
Hash Join cost=641.74267578125
  Seq Scan [title AS t] cost=0.58
  Hash Join cost=642.7604370117188
    Index Scan [company_type AS ct] cost=1.05
    Merge Join cost=644.5562133789062
      Index Scan [movie_companies AS mc] cost=0.61
      Hash Join cost=646.7155151367188
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,714.2,/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
1d Execution time: 714.2 (predicted 641.7) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  55.8 (predicted 650.0)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3786051.0  641.7  /*+ Leading((t (ct (mc (mi_idx it))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Saved iter=4 checkpoint to: /balsa/balsa/wandb/run-20240611_074349-enqqtg8t/files/checkpoint.pt
Saved Experience to: data/replay-Balsa_JOBRandSplit-20execs-24nodes-2s-4iters-enqqtg8t.pkl
---------------------------------------
Planning took 28.5ms
[Test set] q2a, predicted time: 646.5
q2a,(predicted 646.5),/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
---------------------------------------
Planning took 29.4ms
[Test set] q2b, predicted time: 646.5
q2b,(predicted 646.5),/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
---------------------------------------
Planning took 30.7ms
[Test set] q2c, predicted time: 646.5
q2c,(predicted 646.5),/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
---------------------------------------
Planning took 28.8ms
[Test set] q2d, predicted time: 646.5
q2d,(predicted 646.5),/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
[Test set] Waiting on Ray tasks...value_iter=4
[Test set] Running 2a: hinted plan
Hash Join cost=646.5358276367188
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=646.5639038085938
    Seq Scan [company_name AS cn] cost=0.45
    Hash Join cost=646.7000732421875
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=647.222900390625
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,51541.7,/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
2a Execution time: 51541.7 (predicted 646.5) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 649.4)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3741135.8  646.5  /*+ Leading((k (cn (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Hash Join cost=646.5361328125
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=646.564208984375
    Seq Scan [company_name AS cn] cost=0.45
    Hash Join cost=646.7003784179688
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=647.2225952148438
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,56913.2,/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
2b Execution time: 56913.2 (predicted 646.5) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 649.4)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3743934.0  646.5  /*+ Leading((k (cn (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Hash Join cost=646.5361328125
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=646.5639038085938
    Seq Scan [company_name AS cn] cost=0.45
    Hash Join cost=646.7003784179688
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=647.2222290039062
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[sm]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2c,45783.1,/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
2c Execution time: 45783.1 (predicted 646.5) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  269.8 (predicted 649.4)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3744730.2  646.5  /*+ Leading((k (cn (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2d: hinted plan
Hash Join cost=646.5135498046875
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=646.5413818359375
    Seq Scan [company_name AS cn] cost=0.45
    Hash Join cost=646.678466796875
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=647.2083740234375
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[us]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2d,53442.2,/*+ HashJoin(k cn mk mc t)
 HashJoin(cn mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mk)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (cn (mk (mc t))))) */
2d Execution time: 53442.2 (predicted 646.5) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  4175.6 (predicted 649.5)  /*+ Leading(((((k mk) t) mc) cn)) */
SIM-predicted costs, predicted latency, plan: 
  3717570.0  646.5  /*+ Leading((k (cn (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Dropping buffer cache.
8 rels: ['company_name' 'company_type' 'info_type' 'keyword' 'movie_companies'
 'movie_info_idx' 'movie_keyword' 'title']
8 rel_ids: ['company_name AS cn' 'company_type AS ct' 'info_type AS it'
 'keyword AS k' 'movie_companies AS mc' 'movie_info_idx AS mi_idx'
 'movie_keyword AS mk' 'title AS t']
3 scans: ['Index Only Scan' 'Index Scan' 'Seq Scan']
3 joins: ['Hash Join' 'Merge Join' 'Nested Loop']
11 all ops: ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Only Scan'
 'Index Scan' 'Merge Join' 'Nested Loop' 'PartialAggregate' 'Seq Scan'
 'SimpleAggregate']
Calling make_and_featurize_trees()...
took 0.0s
num_total_subtrees=80 num_featurized_subtrees=16 num_new_datapoints=14
head
  query=2.269 feat=21.0 cost=478.417
  query=2.269 feat=5.0 cost=478.417
  query=2.269 feat=10.0 cost=478.417
tail
  query=2.313 feat=5.0 cost=542.188
  query=2.313 feat=10.0 cost=714.153
  query=2.313 feat=16.0 cost=714.153
costs stats mean 6.52102634699966 std 0.30237213880586894
num_train=14 num_validation=2
InitializeModel curr_value_iter=5
Assigning real model := 0.0*SIM + 1.0*previous real model
iter 5 lr 0.001
GPU available: False, used: False
I0611 07:58:45.618202 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 07:58:45.619230 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]                                           /home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 25.54it/s, loss=1.752, v_num=tg8t, train_loss=1.75]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.752, v_num=tg8t, train_loss=1.75]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.752, v_num=tg8t, train_loss=1.75]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.87it/s, loss=1.553, v_num=tg8t, train_loss=1.35]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.553, v_num=tg8t, train_loss=1.35]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.553, v_num=tg8t, train_loss=1.35]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.21it/s, loss=1.303, v_num=tg8t, train_loss=0.802]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.303, v_num=tg8t, train_loss=0.802]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.303, v_num=tg8t, train_loss=0.802]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.59it/s, loss=1.175, v_num=tg8t, train_loss=0.794]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.175, v_num=tg8t, train_loss=0.794]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.175, v_num=tg8t, train_loss=0.794]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.43it/s, loss=1.089, v_num=tg8t, train_loss=0.746]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 4: : 2it [00:00, 23.74it/s, loss=1.089, v_num=tg8t, train_loss=0.746, val_loss=3.43]      
                              [AEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.089, v_num=tg8t, train_loss=0.746, val_loss=3.43]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=1.089, v_num=tg8t, train_loss=0.746, val_loss=3.43]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 42.85it/s, loss=1.028, v_num=tg8t, train_loss=0.723, val_loss=3.43]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.028, v_num=tg8t, train_loss=0.723, val_loss=3.43]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.028, v_num=tg8t, train_loss=0.723, val_loss=3.43]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.09it/s, loss=0.985, v_num=tg8t, train_loss=0.725, val_loss=3.43]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.985, v_num=tg8t, train_loss=0.725, val_loss=3.43]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.985, v_num=tg8t, train_loss=0.725, val_loss=3.43]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.16it/s, loss=0.954, v_num=tg8t, train_loss=0.74, val_loss=3.43]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.954, v_num=tg8t, train_loss=0.74, val_loss=3.43]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.954, v_num=tg8t, train_loss=0.74, val_loss=3.43]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.97it/s, loss=0.932, v_num=tg8t, train_loss=0.753, val_loss=3.43]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.932, v_num=tg8t, train_loss=0.753, val_loss=3.43]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.932, v_num=tg8t, train_loss=0.753, val_loss=3.43]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.86it/s, loss=0.915, v_num=tg8t, train_loss=0.758, val_loss=3.43]
Validating: 0it [00:00, ?it/s][AEpoch 9: : 2it [00:00, 28.93it/s, loss=0.915, v_num=tg8t, train_loss=0.758, val_loss=2.64]                     
                              [AEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.915, v_num=tg8t, train_loss=0.758, val_loss=2.64]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.915, v_num=tg8t, train_loss=0.758, val_loss=2.64]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 34.52it/s, loss=0.900, v_num=tg8t, train_loss=0.754, val_loss=2.64]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.900, v_num=tg8t, train_loss=0.754, val_loss=2.64]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.900, v_num=tg8t, train_loss=0.754, val_loss=2.64]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 59.99it/s, loss=0.887, v_num=tg8t, train_loss=0.744, val_loss=2.64]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.887, v_num=tg8t, train_loss=0.744, val_loss=2.64]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.887, v_num=tg8t, train_loss=0.744, val_loss=2.64]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.43it/s, loss=0.875, v_num=tg8t, train_loss=0.73, val_loss=2.64]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.875, v_num=tg8t, train_loss=0.73, val_loss=2.64]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.875, v_num=tg8t, train_loss=0.73, val_loss=2.64]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.44it/s, loss=0.864, v_num=tg8t, train_loss=0.716, val_loss=2.64]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.864, v_num=tg8t, train_loss=0.716, val_loss=2.64]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.864, v_num=tg8t, train_loss=0.716, val_loss=2.64]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.24it/s, loss=0.853, v_num=tg8t, train_loss=0.706, val_loss=2.64]
Validating: 0it [00:00, ?it/s][AEpoch 14: : 2it [00:00, 47.98it/s, loss=0.853, v_num=tg8t, train_loss=0.706, val_loss=3.32]                     
                              [AEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.853, v_num=tg8t, train_loss=0.706, val_loss=3.32]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.853, v_num=tg8t, train_loss=0.706, val_loss=3.32]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 38.40it/s, loss=0.844, v_num=tg8t, train_loss=0.699, val_loss=3.32]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.844, v_num=tg8t, train_loss=0.699, val_loss=3.32]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.844, v_num=tg8t, train_loss=0.699, val_loss=3.32]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.50it/s, loss=0.835, v_num=tg8t, train_loss=0.695, val_loss=3.32]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.835, v_num=tg8t, train_loss=0.695, val_loss=3.32]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.835, v_num=tg8t, train_loss=0.695, val_loss=3.32]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.72it/s, loss=0.827, v_num=tg8t, train_loss=0.691, val_loss=3.32]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.827, v_num=tg8t, train_loss=0.691, val_loss=3.32]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.827, v_num=tg8t, train_loss=0.691, val_loss=3.32]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.93it/s, loss=0.819, v_num=tg8t, train_loss=0.688, val_loss=3.32]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.819, v_num=tg8t, train_loss=0.688, val_loss=3.32]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.819, v_num=tg8t, train_loss=0.688, val_loss=3.32]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.10it/s, loss=0.813, v_num=tg8t, train_loss=0.683, val_loss=3.32]
Validating: 0it [00:00, ?it/s][AEpoch 19: : 2it [00:00, 50.76it/s, loss=0.813, v_num=tg8t, train_loss=0.683, val_loss=3.92]                     
                              [AEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.813, v_num=tg8t, train_loss=0.683, val_loss=3.92]Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.813, v_num=tg8t, train_loss=0.683, val_loss=3.92]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 34.44it/s, loss=0.759, v_num=tg8t, train_loss=0.675, val_loss=3.92]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.759, v_num=tg8t, train_loss=0.675, val_loss=3.92]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.759, v_num=tg8t, train_loss=0.675, val_loss=3.92]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.36it/s, loss=0.724, v_num=tg8t, train_loss=0.664, val_loss=3.92]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.724, v_num=tg8t, train_loss=0.664, val_loss=3.92]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.724, v_num=tg8t, train_loss=0.664, val_loss=3.92]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 60.36it/s, loss=0.717, v_num=tg8t, train_loss=0.652, val_loss=3.92]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.717, v_num=tg8t, train_loss=0.652, val_loss=3.92]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.717, v_num=tg8t, train_loss=0.652, val_loss=3.92]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.81it/s, loss=0.709, v_num=tg8t, train_loss=0.639, val_loss=3.92]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.709, v_num=tg8t, train_loss=0.639, val_loss=3.92]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.709, v_num=tg8t, train_loss=0.639, val_loss=3.92]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.89it/s, loss=0.703, v_num=tg8t, train_loss=0.626, val_loss=3.92]
Validating: 0it [00:00, ?it/s][AEpoch 24: : 2it [00:00, 50.69it/s, loss=0.703, v_num=tg8t, train_loss=0.626, val_loss=4.05]                     
                              [ASaving latest checkpoint..
I0611 07:58:46.716491 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 00025: early stopping triggered.
I0611 07:58:46.717104 140711206703488 early_stopping.py:237] Epoch 00025: early stopping triggered.
Epoch 24: : 2it [00:00, 31.46it/s, loss=0.703, v_num=tg8t, train_loss=0.626, val_loss=4.05]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=9.ckpt (current_epoch=24)
---------------------------------------
Planning took 236.6ms
q1a, predicted time: 690.4
q1a,(predicted 690.4),/*+ HashJoin(it t mi_idx ct mc)
 HashJoin(ct mc)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 SeqScan(it)
 IndexScan(t)
 SeqScan(mi_idx)
 SeqScan(ct)
 SeqScan(mc)
 Leading(((it (t mi_idx)) (ct mc))) */
---------------------------------------
Planning took 225.4ms
q1b, predicted time: 692.3
q1b,(predicted 692.3),/*+ HashJoin(it mi_idx t mc ct)
 HashJoin(mc ct)
 HashJoin(it mi_idx t)
 HashJoin(mi_idx t)
 SeqScan(it)
 SeqScan(mi_idx)
 IndexScan(t)
 SeqScan(mc)
 SeqScan(ct)
 Leading(((it (mi_idx t)) (mc ct))) */
---------------------------------------
Planning took 238.7ms
q1c, predicted time: 690.7
q1c,(predicted 690.7),/*+ HashJoin(it t mi_idx ct mc)
 HashJoin(ct mc)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 SeqScan(it)
 IndexScan(t)
 SeqScan(mi_idx)
 SeqScan(ct)
 SeqScan(mc)
 Leading(((it (t mi_idx)) (ct mc))) */
---------------------------------------
Planning took 225.4ms
q1d, predicted time: 692.2
q1d,(predicted 692.2),/*+ HashJoin(it mi_idx t mc ct)
 HashJoin(mc ct)
 HashJoin(it mi_idx t)
 HashJoin(mi_idx t)
 SeqScan(it)
 SeqScan(mi_idx)
 IndexScan(t)
 SeqScan(mc)
 SeqScan(ct)
 Leading(((it (mi_idx t)) (mc ct))) */
Waiting on Ray tasks...value_iter=5
Timeout occurred; checking the hint against local PG.
Running 1a: hinted plan
Hash Join cost=690.4244995117188
  Hash Join cost=677.4774169921875
    Seq Scan [info_type AS it] cost=2.41
    Hash Join cost=665.2389526367188
      Index Scan [title AS t] cost=0.58
      Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
  Hash Join cost=665.119873046875
    Seq Scan [company_type AS ct] cost=1.05
    Seq Scan [movie_companies AS mc] cost=0.64

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,-1.0,/*+ HashJoin(it t mi_idx ct mc)
 HashJoin(ct mc)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 SeqScan(it)
 IndexScan(t)
 SeqScan(mi_idx)
 SeqScan(ct)
 SeqScan(mc)
 Leading(((it (t mi_idx)) (ct mc))) */
1a Execution time: -1.0 (predicted 690.4) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  943.5 (predicted 688.0)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  4804346.0  690.4  /*+ Leading(((it (t mi_idx)) (ct mc))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1b: hinted plan
Hash Join cost=692.302734375
  Hash Join cost=678.7562255859375
    Seq Scan [info_type AS it] cost=2.41
    Hash Join cost=667.102294921875
      Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
      Index Scan [title AS t] cost=0.58
  Hash Join cost=666.9216918945312
    Seq Scan [movie_companies AS mc] cost=0.61
    Seq Scan [company_type AS ct] cost=1.05

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,851.6,/*+ HashJoin(it mi_idx t mc ct)
 HashJoin(mc ct)
 HashJoin(it mi_idx t)
 HashJoin(mi_idx t)
 SeqScan(it)
 SeqScan(mi_idx)
 IndexScan(t)
 SeqScan(mc)
 SeqScan(ct)
 Leading(((it (mi_idx t)) (mc ct))) */
1b Execution time: 851.6 (predicted 692.3) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  62.3 (predicted 690.7)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  4690726.5  692.3  /*+ Leading(((it (mi_idx t)) (mc ct))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Hash Join cost=690.73583984375
  Hash Join cost=677.6553955078125
    Seq Scan [info_type AS it] cost=2.41
    Hash Join cost=665.3536987304688
      Index Scan [title AS t] cost=0.58
      Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
  Hash Join cost=665.2412109375
    Seq Scan [company_type AS ct] cost=1.05
    Seq Scan [movie_companies AS mc] cost=0.62

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,435.5,/*+ HashJoin(it t mi_idx ct mc)
 HashJoin(ct mc)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 SeqScan(it)
 IndexScan(t)
 SeqScan(mi_idx)
 SeqScan(ct)
 SeqScan(mc)
 Leading(((it (t mi_idx)) (ct mc))) */
1c Execution time: 435.5 (predicted 690.7) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  56.8 (predicted 688.3)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1789686.4  690.7  /*+ Leading(((it (t mi_idx)) (ct mc))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout occurred; checking the hint against local PG.
Running 1d: hinted plan
Hash Join cost=692.1817016601562
  Hash Join cost=678.6968994140625
    Seq Scan [info_type AS it] cost=2.41
    Hash Join cost=667.0924072265625
      Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
      Index Scan [title AS t] cost=0.58
  Hash Join cost=666.885986328125
    Seq Scan [movie_companies AS mc] cost=0.61
    Seq Scan [company_type AS ct] cost=1.05

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,-1.0,/*+ HashJoin(it mi_idx t mc ct)
 HashJoin(mc ct)
 HashJoin(it mi_idx t)
 HashJoin(mi_idx t)
 SeqScan(it)
 SeqScan(mi_idx)
 IndexScan(t)
 SeqScan(mc)
 SeqScan(ct)
 Leading(((it (mi_idx t)) (mc ct))) */
1d Execution time: -1.0 (predicted 692.2) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  55.8 (predicted 690.6)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  4815998.0  692.2  /*+ Leading(((it (mi_idx t)) (mc ct))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout detected! Assigning a special label 4096000 (server_ip=172.17.0.2)
Timeout detected! Assigning a special label 4096000 (server_ip=172.17.0.2)
---------------------------------------
Planning took 41.3ms
[Test set] q2a, predicted time: 758.3
q2a,(predicted 758.3),/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
---------------------------------------
Planning took 31.6ms
[Test set] q2b, predicted time: 758.3
q2b,(predicted 758.3),/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
---------------------------------------
Planning took 32.0ms
[Test set] q2c, predicted time: 758.3
q2c,(predicted 758.3),/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
---------------------------------------
Planning took 32.2ms
[Test set] q2d, predicted time: 758.3
q2d,(predicted 758.3),/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
[Test set] Waiting on Ray tasks...value_iter=5
[Test set] Running 2a: hinted plan
Hash Join cost=758.3287963867188
  Seq Scan [company_name AS cn] cost=0.45
  Hash Join cost=758.3345336914062
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=759.213134765625
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=766.6298828125
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,68763.6,/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
2a Execution time: 68763.6 (predicted 758.3) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 781.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3587745.2  758.3  /*+ Leading((cn (k (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Hash Join cost=758.2958374023438
  Seq Scan [company_name AS cn] cost=0.45
  Hash Join cost=758.300537109375
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=759.177978515625
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=766.58740234375
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,71851.8,/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
2b Execution time: 71851.8 (predicted 758.3) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 781.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3590264.2  758.3  /*+ Leading((cn (k (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Hash Join cost=758.2864379882812
  Seq Scan [company_name AS cn] cost=0.45
  Hash Join cost=758.290771484375
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=759.1677856445312
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=766.57568359375
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[sm]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2c,71623.0,/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
2c Execution time: 71623.0 (predicted 758.3) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  269.8 (predicted 781.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3590980.0  758.3  /*+ Leading((cn (k (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2d: hinted plan
Hash Join cost=758.2643432617188
  Seq Scan [company_name AS cn] cost=0.45
  Hash Join cost=758.2762451171875
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=759.1670532226562
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=766.5899658203125
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[us]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2d,75004.8,/*+ HashJoin(cn k mk mc t)
 HashJoin(k mk mc t)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(cn)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 Leading((cn (k (mk (mc t))))) */
2d Execution time: 75004.8 (predicted 758.3) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  4175.6 (predicted 782.1)  /*+ Leading(((((k mk) t) mc) cn)) */
SIM-predicted costs, predicted latency, plan: 
  3566965.0  758.3  /*+ Leading((cn (k (mk (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Dropping buffer cache.
8 rels: ['company_name' 'company_type' 'info_type' 'keyword' 'movie_companies'
 'movie_info_idx' 'movie_keyword' 'title']
8 rel_ids: ['company_name AS cn' 'company_type AS ct' 'info_type AS it'
 'keyword AS k' 'movie_companies AS mc' 'movie_info_idx AS mi_idx'
 'movie_keyword AS mk' 'title AS t']
3 scans: ['Index Only Scan' 'Index Scan' 'Seq Scan']
3 joins: ['Hash Join' 'Merge Join' 'Nested Loop']
11 all ops: ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Only Scan'
 'Index Scan' 'Merge Join' 'Nested Loop' 'PartialAggregate' 'Seq Scan'
 'SimpleAggregate']
Calling make_and_featurize_trees()...
took 0.0s
num_total_subtrees=96 num_featurized_subtrees=16 num_new_datapoints=16
head
  query=2.269 feat=21.0 cost=4096000
  query=2.269 feat=10.0 cost=4096000
  query=2.269 feat=5.0 cost=4096000
tail
  query=2.313 feat=5.0 cost=4096000
  query=2.313 feat=5.0 cost=4096000
  query=2.313 feat=10.0 cost=4096000
costs stats mean 10.819514766187778 std 4.412361496750472
num_train=14 num_validation=2
InitializeModel curr_value_iter=6
Assigning real model := 0.0*SIM + 1.0*previous real model
iter 6 lr 0.001
GPU available: False, used: False
I0611 08:03:39.998028 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 08:03:39.998830 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]                                           /home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 22.06it/s, loss=1.017, v_num=tg8t, train_loss=1.02]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.017, v_num=tg8t, train_loss=1.02]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.017, v_num=tg8t, train_loss=1.02]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.59it/s, loss=0.999, v_num=tg8t, train_loss=0.981]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.999, v_num=tg8t, train_loss=0.981]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.999, v_num=tg8t, train_loss=0.981]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.47it/s, loss=0.993, v_num=tg8t, train_loss=0.982]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.993, v_num=tg8t, train_loss=0.982]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.993, v_num=tg8t, train_loss=0.982]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.02it/s, loss=0.990, v_num=tg8t, train_loss=0.98]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.990, v_num=tg8t, train_loss=0.98]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.990, v_num=tg8t, train_loss=0.98]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.30it/s, loss=0.986, v_num=tg8t, train_loss=0.97]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 4: : 2it [00:00, 20.97it/s, loss=0.986, v_num=tg8t, train_loss=0.97, val_loss=1.53]      
                              [AEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.986, v_num=tg8t, train_loss=0.97, val_loss=1.53]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.986, v_num=tg8t, train_loss=0.97, val_loss=1.53]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 34.99it/s, loss=0.982, v_num=tg8t, train_loss=0.96, val_loss=1.53]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.982, v_num=tg8t, train_loss=0.96, val_loss=1.53]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.982, v_num=tg8t, train_loss=0.96, val_loss=1.53]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.09it/s, loss=0.977, v_num=tg8t, train_loss=0.951, val_loss=1.53]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.977, v_num=tg8t, train_loss=0.951, val_loss=1.53]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.977, v_num=tg8t, train_loss=0.951, val_loss=1.53]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.36it/s, loss=0.972, v_num=tg8t, train_loss=0.932, val_loss=1.53]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.972, v_num=tg8t, train_loss=0.932, val_loss=1.53]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.972, v_num=tg8t, train_loss=0.932, val_loss=1.53]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.52it/s, loss=0.964, v_num=tg8t, train_loss=0.901, val_loss=1.53]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.964, v_num=tg8t, train_loss=0.901, val_loss=1.53]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.964, v_num=tg8t, train_loss=0.901, val_loss=1.53]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.32it/s, loss=0.956, v_num=tg8t, train_loss=0.883, val_loss=1.53]
Validating: 0it [00:00, ?it/s][AEpoch 9: : 2it [00:00, 13.45it/s, loss=0.956, v_num=tg8t, train_loss=0.883, val_loss=1.14]                     
                              [AEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.956, v_num=tg8t, train_loss=0.883, val_loss=1.14]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.956, v_num=tg8t, train_loss=0.883, val_loss=1.14]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 23.98it/s, loss=0.945, v_num=tg8t, train_loss=0.841, val_loss=1.14]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.945, v_num=tg8t, train_loss=0.841, val_loss=1.14]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.945, v_num=tg8t, train_loss=0.841, val_loss=1.14]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.53it/s, loss=0.934, v_num=tg8t, train_loss=0.805, val_loss=1.14]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.934, v_num=tg8t, train_loss=0.805, val_loss=1.14]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.934, v_num=tg8t, train_loss=0.805, val_loss=1.14]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.85it/s, loss=0.918, v_num=tg8t, train_loss=0.732, val_loss=1.14]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.918, v_num=tg8t, train_loss=0.732, val_loss=1.14]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.918, v_num=tg8t, train_loss=0.732, val_loss=1.14]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.91it/s, loss=0.894, v_num=tg8t, train_loss=0.579, val_loss=1.14]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.894, v_num=tg8t, train_loss=0.579, val_loss=1.14]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.894, v_num=tg8t, train_loss=0.579, val_loss=1.14]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.69it/s, loss=0.867, v_num=tg8t, train_loss=0.495, val_loss=1.14]
Validating: 0it [00:00, ?it/s][AEpoch 14: : 2it [00:00, 19.40it/s, loss=0.867, v_num=tg8t, train_loss=0.495, val_loss=0.726]                    
                              [AEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.867, v_num=tg8t, train_loss=0.495, val_loss=0.726]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.867, v_num=tg8t, train_loss=0.495, val_loss=0.726]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 40.41it/s, loss=0.835, v_num=tg8t, train_loss=0.356, val_loss=0.726]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.835, v_num=tg8t, train_loss=0.356, val_loss=0.726]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.835, v_num=tg8t, train_loss=0.356, val_loss=0.726]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.06it/s, loss=0.802, v_num=tg8t, train_loss=0.268, val_loss=0.726]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.802, v_num=tg8t, train_loss=0.268, val_loss=0.726]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.802, v_num=tg8t, train_loss=0.268, val_loss=0.726]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.32it/s, loss=0.768, v_num=tg8t, train_loss=0.19, val_loss=0.726]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.768, v_num=tg8t, train_loss=0.19, val_loss=0.726]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.768, v_num=tg8t, train_loss=0.19, val_loss=0.726]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.80it/s, loss=0.734, v_num=tg8t, train_loss=0.124, val_loss=0.726]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.734, v_num=tg8t, train_loss=0.124, val_loss=0.726]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.734, v_num=tg8t, train_loss=0.124, val_loss=0.726]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.37it/s, loss=0.701, v_num=tg8t, train_loss=0.0723, val_loss=0.726]
Validating: 0it [00:00, ?it/s][AEpoch 19: : 2it [00:00, 26.44it/s, loss=0.701, v_num=tg8t, train_loss=0.0723, val_loss=0.107]                     
                              [AEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.701, v_num=tg8t, train_loss=0.0723, val_loss=0.107]Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.701, v_num=tg8t, train_loss=0.0723, val_loss=0.107]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 17.06it/s, loss=0.653, v_num=tg8t, train_loss=0.057, val_loss=0.107]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.653, v_num=tg8t, train_loss=0.057, val_loss=0.107]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.653, v_num=tg8t, train_loss=0.057, val_loss=0.107]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.33it/s, loss=0.605, v_num=tg8t, train_loss=0.0144, val_loss=0.107]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.605, v_num=tg8t, train_loss=0.0144, val_loss=0.107]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.605, v_num=tg8t, train_loss=0.0144, val_loss=0.107]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.45it/s, loss=0.556, v_num=tg8t, train_loss=0.0094, val_loss=0.107]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.556, v_num=tg8t, train_loss=0.0094, val_loss=0.107]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.556, v_num=tg8t, train_loss=0.0094, val_loss=0.107]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.72it/s, loss=0.508, v_num=tg8t, train_loss=0.016, val_loss=0.107]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.508, v_num=tg8t, train_loss=0.016, val_loss=0.107]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.508, v_num=tg8t, train_loss=0.016, val_loss=0.107]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.34it/s, loss=0.471, v_num=tg8t, train_loss=0.241, val_loss=0.107]
Validating: 0it [00:00, ?it/s][AEpoch 24: : 2it [00:00, 21.94it/s, loss=0.471, v_num=tg8t, train_loss=0.241, val_loss=0.0375]                    
                              [AEpoch 24:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.471, v_num=tg8t, train_loss=0.241, val_loss=0.0375]Epoch 25:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.471, v_num=tg8t, train_loss=0.241, val_loss=0.0375]Epoch 25:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 38.39it/s, loss=0.426, v_num=tg8t, train_loss=0.0417, val_loss=0.0375]Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.426, v_num=tg8t, train_loss=0.0417, val_loss=0.0375]        Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.426, v_num=tg8t, train_loss=0.0417, val_loss=0.0375]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.79it/s, loss=0.381, v_num=tg8t, train_loss=0.0508, val_loss=0.0375]Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.381, v_num=tg8t, train_loss=0.0508, val_loss=0.0375]        Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.381, v_num=tg8t, train_loss=0.0508, val_loss=0.0375]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.01it/s, loss=0.336, v_num=tg8t, train_loss=0.0518, val_loss=0.0375]Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.336, v_num=tg8t, train_loss=0.0518, val_loss=0.0375]        Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.336, v_num=tg8t, train_loss=0.0518, val_loss=0.0375]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.89it/s, loss=0.333, v_num=tg8t, train_loss=0.833, val_loss=0.0375]Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.333, v_num=tg8t, train_loss=0.833, val_loss=0.0375]        Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.333, v_num=tg8t, train_loss=0.833, val_loss=0.0375]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.08it/s, loss=0.290, v_num=tg8t, train_loss=0.0293, val_loss=0.0375]
Validating: 0it [00:00, ?it/s][AEpoch 29: : 2it [00:00, 25.50it/s, loss=0.290, v_num=tg8t, train_loss=0.0293, val_loss=0.00807]                    
                              [AEpoch 29:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.290, v_num=tg8t, train_loss=0.0293, val_loss=0.00807]Epoch 30:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.290, v_num=tg8t, train_loss=0.0293, val_loss=0.00807]Epoch 30:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 39.42it/s, loss=0.249, v_num=tg8t, train_loss=0.0171, val_loss=0.00807]Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.249, v_num=tg8t, train_loss=0.0171, val_loss=0.00807]        Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.249, v_num=tg8t, train_loss=0.0171, val_loss=0.00807]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.54it/s, loss=0.209, v_num=tg8t, train_loss=0.00847, val_loss=0.00807]Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.209, v_num=tg8t, train_loss=0.00847, val_loss=0.00807]        Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.209, v_num=tg8t, train_loss=0.00847, val_loss=0.00807]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.17it/s, loss=0.173, v_num=tg8t, train_loss=0.00381, val_loss=0.00807]Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.173, v_num=tg8t, train_loss=0.00381, val_loss=0.00807]        Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.173, v_num=tg8t, train_loss=0.00381, val_loss=0.00807]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.74it/s, loss=0.144, v_num=tg8t, train_loss=0.00276, val_loss=0.00807]Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.144, v_num=tg8t, train_loss=0.00276, val_loss=0.00807]        Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.144, v_num=tg8t, train_loss=0.00276, val_loss=0.00807]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.68it/s, loss=0.120, v_num=tg8t, train_loss=0.0044, val_loss=0.00807]
Validating: 0it [00:00, ?it/s][AEpoch 34: : 2it [00:00, 40.62it/s, loss=0.120, v_num=tg8t, train_loss=0.0044, val_loss=0.0357]                      
                              [AEpoch 34:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.120, v_num=tg8t, train_loss=0.0044, val_loss=0.0357]Epoch 35:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.120, v_num=tg8t, train_loss=0.0044, val_loss=0.0357]Epoch 35:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 42.98it/s, loss=0.102, v_num=tg8t, train_loss=0.00764, val_loss=0.0357]Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.102, v_num=tg8t, train_loss=0.00764, val_loss=0.0357]        Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.102, v_num=tg8t, train_loss=0.00764, val_loss=0.0357]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.39it/s, loss=0.090, v_num=tg8t, train_loss=0.0162, val_loss=0.0357]Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.090, v_num=tg8t, train_loss=0.0162, val_loss=0.0357]        Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.090, v_num=tg8t, train_loss=0.0162, val_loss=0.0357]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.33it/s, loss=0.136, v_num=tg8t, train_loss=1.12, val_loss=0.0357]Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.136, v_num=tg8t, train_loss=1.12, val_loss=0.0357]        Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.136, v_num=tg8t, train_loss=1.12, val_loss=0.0357]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.87it/s, loss=0.186, v_num=tg8t, train_loss=1.11, val_loss=0.0357]Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.186, v_num=tg8t, train_loss=1.11, val_loss=0.0357]        Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.186, v_num=tg8t, train_loss=1.11, val_loss=0.0357]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.15it/s, loss=0.237, v_num=tg8t, train_loss=1.1, val_loss=0.0357]
Validating: 0it [00:00, ?it/s][AEpoch 39: : 2it [00:00, 42.63it/s, loss=0.237, v_num=tg8t, train_loss=1.1, val_loss=0.0634]                     
                              [AEpoch 39:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.237, v_num=tg8t, train_loss=1.1, val_loss=0.0634]Epoch 40:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.237, v_num=tg8t, train_loss=1.1, val_loss=0.0634]Epoch 40:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 32.02it/s, loss=0.288, v_num=tg8t, train_loss=1.07, val_loss=0.0634]Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.288, v_num=tg8t, train_loss=1.07, val_loss=0.0634]        Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.288, v_num=tg8t, train_loss=1.07, val_loss=0.0634]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 30.70it/s, loss=0.340, v_num=tg8t, train_loss=1.05, val_loss=0.0634]Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.340, v_num=tg8t, train_loss=1.05, val_loss=0.0634]        Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.340, v_num=tg8t, train_loss=1.05, val_loss=0.0634]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.13it/s, loss=0.390, v_num=tg8t, train_loss=1.02, val_loss=0.0634]Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.390, v_num=tg8t, train_loss=1.02, val_loss=0.0634]        Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.390, v_num=tg8t, train_loss=1.02, val_loss=0.0634]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 33.74it/s, loss=0.439, v_num=tg8t, train_loss=0.989, val_loss=0.0634]Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.439, v_num=tg8t, train_loss=0.989, val_loss=0.0634]        Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.439, v_num=tg8t, train_loss=0.989, val_loss=0.0634]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.90it/s, loss=0.475, v_num=tg8t, train_loss=0.958, val_loss=0.0634]
Validating: 0it [00:00, ?it/s][AEpoch 44: : 2it [00:00, 38.10it/s, loss=0.475, v_num=tg8t, train_loss=0.958, val_loss=0.0356]                     
                              [ASaving latest checkpoint..
I0611 08:03:42.399976 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 00045: early stopping triggered.
I0611 08:03:42.400913 140711206703488 early_stopping.py:237] Epoch 00045: early stopping triggered.
Epoch 44: : 2it [00:00, 24.30it/s, loss=0.475, v_num=tg8t, train_loss=0.958, val_loss=0.0356]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=29.ckpt (current_epoch=44)
---------------------------------------
Planning took 35.5ms
q1a, predicted time: 3813835.5
q1a,(predicted 3813835.5),/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
---------------------------------------
Planning took 327.9ms
q1b, predicted time: 310.1
q1b,(predicted 310.1),/*+ HashJoin(mc ct it t mi_idx)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 HashJoin(mc ct)
 SeqScan(mc)
 IndexScan(ct)
 SeqScan(it)
 SeqScan(t)
 IndexScan(mi_idx)
 Leading(((mc ct) (it (t mi_idx)))) */
---------------------------------------
Planning took 378.2ms
q1c, predicted time: 311.7
q1c,(predicted 311.7),/*+ HashJoin(mc ct it t mi_idx)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 HashJoin(mc ct)
 IndexScan(mc)
 SeqScan(ct)
 SeqScan(it)
 SeqScan(t)
 IndexScan(mi_idx)
 Leading(((mc ct) (it (t mi_idx)))) */
---------------------------------------
Planning took 43.2ms
selected cnt,latency=(1, 3804376.0); sorted: [(1, 3804376.0)]
q1d, predicted time: 3804376.0
q1d,(predicted 3804376.0),/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
Waiting on Ray tasks...value_iter=6
Running 1a: hinted plan
Hash Join cost=3813835.5
  Seq Scan [title AS t] cost=0.58
  Hash Join cost=3817125.0
    Index Scan [company_type AS ct] cost=1.05
    Merge Join cost=3826488.25
      Index Scan [movie_companies AS mc] cost=0.64
      Hash Join cost=3855871.0
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,894.4,/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
1a Execution time: 894.4 (predicted 3813835.5) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  943.5 (predicted 3898518.5)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  3782687.5  3813835.5  /*+ Leading((t (ct (mc (mi_idx it))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1b: hinted plan
Hash Join cost=310.1109313964844
  Hash Join cost=287.5903015136719
    Seq Scan [movie_companies AS mc] cost=0.61
    Index Scan [company_type AS ct] cost=1.05
  Hash Join cost=294.9135437011719
    Seq Scan [info_type AS it] cost=2.41
    Hash Join cost=286.9979553222656
      Seq Scan [title AS t] cost=0.58
      Index Scan [movie_info_idx AS mi_idx] cost=13685.15

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,563.5,/*+ HashJoin(mc ct it t mi_idx)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 HashJoin(mc ct)
 SeqScan(mc)
 IndexScan(ct)
 SeqScan(it)
 SeqScan(t)
 IndexScan(mi_idx)
 Leading(((mc ct) (it (t mi_idx)))) */
1b Execution time: 563.5 (predicted 310.1) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  62.3 (predicted 309.8)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  4433114.5  310.1  /*+ Leading(((mc ct) (it (t mi_idx)))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Hash Join cost=311.65802001953125
  Hash Join cost=287.9484558105469
    Index Scan [movie_companies AS mc] cost=0.62
    Seq Scan [company_type AS ct] cost=1.05
  Hash Join cost=295.9996032714844
    Seq Scan [info_type AS it] cost=2.41
    Hash Join cost=287.8921203613281
      Seq Scan [title AS t] cost=0.58
      Index Scan [movie_info_idx AS mi_idx] cost=13685.15

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,343.2,/*+ HashJoin(mc ct it t mi_idx)
 HashJoin(it t mi_idx)
 HashJoin(t mi_idx)
 HashJoin(mc ct)
 IndexScan(mc)
 SeqScan(ct)
 SeqScan(it)
 SeqScan(t)
 IndexScan(mi_idx)
 Leading(((mc ct) (it (t mi_idx)))) */
1c Execution time: 343.2 (predicted 311.7) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  56.8 (predicted 309.8)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1752241.0  311.7  /*+ Leading(((mc ct) (it (t mi_idx)))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1d: hinted plan
Hash Join cost=3804376.0
  Seq Scan [title AS t] cost=0.58
  Hash Join cost=3809364.5
    Index Scan [company_type AS ct] cost=1.05
    Merge Join cost=3826054.0
      Index Scan [movie_companies AS mc] cost=0.61
      Hash Join cost=3863678.25
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,714.2,/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 MergeJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 SeqScan(t)
 IndexScan(ct)
 IndexScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
1d Execution time: 714.2 (predicted 3804376.0) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  55.8 (predicted 3909583.8)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3786051.0  3804376.0  /*+ Leading((t (ct (mc (mi_idx it))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
---------------------------------------
Planning took 35.1ms
[Test set] q2a, predicted time: 3845632.5
q2a,(predicted 3845632.5),/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
---------------------------------------
Planning took 31.6ms
[Test set] q2b, predicted time: 3845610.5
q2b,(predicted 3845610.5),/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
---------------------------------------
Planning took 35.4ms
[Test set] q2c, predicted time: 3845595.8
q2c,(predicted 3845595.8),/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
---------------------------------------
Planning took 31.2ms
[Test set] q2d, predicted time: 3845515.0
q2d,(predicted 3845515.0),/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
[Test set] Waiting on Ray tasks...value_iter=6
[Test set] Running 2a: hinted plan
Hash Join cost=3845632.5
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=3846421.0
    Seq Scan [movie_keyword AS mk] cost=1229.38
    Hash Join cost=3848861.25
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=3866763.5
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,10108.5,/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
2a Execution time: 10108.5 (predicted 3845632.5) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 3896333.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3731241.0  3845632.5  /*+ Leading((k (mk (cn (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Hash Join cost=3845610.5
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=3846395.25
    Seq Scan [movie_keyword AS mk] cost=1229.38
    Hash Join cost=3848831.75
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=3866741.5
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,3659.3,/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
2b Execution time: 3659.3 (predicted 3845610.5) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 3896318.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3734007.0  3845610.5  /*+ Leading((k (mk (cn (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Hash Join cost=3845595.75
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=3846388.0
    Seq Scan [movie_keyword AS mk] cost=1229.38
    Hash Join cost=3848828.0
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=3866734.0
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[sm]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2c,1267.4,/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
2c Execution time: 1267.4 (predicted 3845595.8) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  269.8 (predicted 3896314.2)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3734790.5  3845595.8  /*+ Leading((k (mk (cn (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2d: hinted plan
Hash Join cost=3845515.0
  Seq Scan [keyword AS k] cost=2626.12
  Hash Join cost=3846300.0
    Seq Scan [movie_keyword AS mk] cost=1229.38
    Hash Join cost=3848751.0
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=3866682.5
        Index Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49

filters
{'company_name AS cn': "((cn.country_code)::text = '[us]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2d,18678.4,/*+ HashJoin(k mk cn mc t)
 HashJoin(mk cn mc t)
 HashJoin(cn mc t)
 HashJoin(mc t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(cn)
 IndexScan(mc)
 SeqScan(t)
 Leading((k (mk (cn (mc t))))) */
2d Execution time: 18678.4 (predicted 3845515.0) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  4175.6 (predicted 3897559.2)  /*+ Leading(((((k mk) t) mc) cn)) */
SIM-predicted costs, predicted latency, plan: 
  3708091.2  3845515.0  /*+ Leading((k (mk (cn (mc t))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Dropping buffer cache.
8 rels: ['company_name' 'company_type' 'info_type' 'keyword' 'movie_companies'
 'movie_info_idx' 'movie_keyword' 'title']
8 rel_ids: ['company_name AS cn' 'company_type AS ct' 'info_type AS it'
 'keyword AS k' 'movie_companies AS mc' 'movie_info_idx AS mi_idx'
 'movie_keyword AS mk' 'title AS t']
3 scans: ['Index Only Scan' 'Index Scan' 'Seq Scan']
3 joins: ['Hash Join' 'Merge Join' 'Nested Loop']
11 all ops: ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Only Scan'
 'Index Scan' 'Merge Join' 'Nested Loop' 'PartialAggregate' 'Seq Scan'
 'SimpleAggregate']
Calling make_and_featurize_trees()...
took 0.0s
num_total_subtrees=112 num_featurized_subtrees=16 num_new_datapoints=14
head
  query=2.269 feat=23.0 cost=894.355
  query=2.269 feat=16.0 cost=894.355
  query=2.269 feat=10.0 cost=894.355
tail
  query=2.313 feat=5.0 cost=542.188
  query=2.313 feat=10.0 cost=714.153
  query=2.313 feat=16.0 cost=714.153
costs stats mean 6.325254250151714 std 0.3396432274408631
num_train=14 num_validation=2
InitializeModel curr_value_iter=7
Assigning real model := 0.0*SIM + 1.0*previous real model
iter 7 lr 0.001
GPU available: False, used: False
I0611 08:04:19.420021 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 08:04:19.420678 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]                                           /home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 24.30it/s, loss=0.750, v_num=tg8t, train_loss=0.75]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.750, v_num=tg8t, train_loss=0.75]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.750, v_num=tg8t, train_loss=0.75]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.98it/s, loss=0.917, v_num=tg8t, train_loss=1.08]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.917, v_num=tg8t, train_loss=1.08]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.917, v_num=tg8t, train_loss=1.08]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.10it/s, loss=0.819, v_num=tg8t, train_loss=0.622]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.819, v_num=tg8t, train_loss=0.622]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.819, v_num=tg8t, train_loss=0.622]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.28it/s, loss=0.762, v_num=tg8t, train_loss=0.594]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.762, v_num=tg8t, train_loss=0.594]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.762, v_num=tg8t, train_loss=0.594]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.21it/s, loss=0.726, v_num=tg8t, train_loss=0.58]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 4: : 2it [00:00, 27.46it/s, loss=0.726, v_num=tg8t, train_loss=0.58, val_loss=0.399]     
                              [AEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.726, v_num=tg8t, train_loss=0.58, val_loss=0.399]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.726, v_num=tg8t, train_loss=0.58, val_loss=0.399]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 40.61it/s, loss=0.701, v_num=tg8t, train_loss=0.576, val_loss=0.399]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.701, v_num=tg8t, train_loss=0.576, val_loss=0.399]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.701, v_num=tg8t, train_loss=0.576, val_loss=0.399]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.27it/s, loss=0.683, v_num=tg8t, train_loss=0.576, val_loss=0.399]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.683, v_num=tg8t, train_loss=0.576, val_loss=0.399]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.683, v_num=tg8t, train_loss=0.576, val_loss=0.399]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.69it/s, loss=0.670, v_num=tg8t, train_loss=0.577, val_loss=0.399]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.670, v_num=tg8t, train_loss=0.577, val_loss=0.399]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.670, v_num=tg8t, train_loss=0.577, val_loss=0.399]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.55it/s, loss=0.659, v_num=tg8t, train_loss=0.574, val_loss=0.399]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.659, v_num=tg8t, train_loss=0.574, val_loss=0.399]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.659, v_num=tg8t, train_loss=0.574, val_loss=0.399]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 30.96it/s, loss=0.650, v_num=tg8t, train_loss=0.568, val_loss=0.399]
Validating: 0it [00:00, ?it/s][AEpoch 9: : 2it [00:00, 36.82it/s, loss=0.650, v_num=tg8t, train_loss=0.568, val_loss=0.52]                      
                              [AEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.650, v_num=tg8t, train_loss=0.568, val_loss=0.52]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.650, v_num=tg8t, train_loss=0.568, val_loss=0.52]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 25.03it/s, loss=0.642, v_num=tg8t, train_loss=0.557, val_loss=0.52]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.642, v_num=tg8t, train_loss=0.557, val_loss=0.52]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.642, v_num=tg8t, train_loss=0.557, val_loss=0.52]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 59.00it/s, loss=0.641, v_num=tg8t, train_loss=0.632, val_loss=0.52]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.641, v_num=tg8t, train_loss=0.632, val_loss=0.52]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.641, v_num=tg8t, train_loss=0.632, val_loss=0.52]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.29it/s, loss=0.632, v_num=tg8t, train_loss=0.53, val_loss=0.52]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.632, v_num=tg8t, train_loss=0.53, val_loss=0.52]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.632, v_num=tg8t, train_loss=0.53, val_loss=0.52]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.54it/s, loss=0.625, v_num=tg8t, train_loss=0.526, val_loss=0.52]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.625, v_num=tg8t, train_loss=0.526, val_loss=0.52]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.625, v_num=tg8t, train_loss=0.526, val_loss=0.52]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 56.59it/s, loss=0.618, v_num=tg8t, train_loss=0.52, val_loss=0.52]
Validating: 0it [00:00, ?it/s][AEpoch 14: : 2it [00:00, 56.10it/s, loss=0.618, v_num=tg8t, train_loss=0.52, val_loss=0.51]                     
                              [AEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.618, v_num=tg8t, train_loss=0.52, val_loss=0.51]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.618, v_num=tg8t, train_loss=0.52, val_loss=0.51]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 60.35it/s, loss=0.614, v_num=tg8t, train_loss=0.561, val_loss=0.51]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.614, v_num=tg8t, train_loss=0.561, val_loss=0.51]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.614, v_num=tg8t, train_loss=0.561, val_loss=0.51]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.12it/s, loss=0.609, v_num=tg8t, train_loss=0.533, val_loss=0.51]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.609, v_num=tg8t, train_loss=0.533, val_loss=0.51]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.609, v_num=tg8t, train_loss=0.533, val_loss=0.51]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.45it/s, loss=0.603, v_num=tg8t, train_loss=0.497, val_loss=0.51]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.603, v_num=tg8t, train_loss=0.497, val_loss=0.51]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.603, v_num=tg8t, train_loss=0.497, val_loss=0.51]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.22it/s, loss=0.597, v_num=tg8t, train_loss=0.486, val_loss=0.51]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.597, v_num=tg8t, train_loss=0.486, val_loss=0.51]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.597, v_num=tg8t, train_loss=0.486, val_loss=0.51]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.83it/s, loss=0.591, v_num=tg8t, train_loss=0.473, val_loss=0.51]
Validating: 0it [00:00, ?it/s][AEpoch 19: : 2it [00:00, 27.96it/s, loss=0.591, v_num=tg8t, train_loss=0.473, val_loss=0.294]                    
                              [AEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.591, v_num=tg8t, train_loss=0.473, val_loss=0.294]Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.591, v_num=tg8t, train_loss=0.473, val_loss=0.294]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 32.85it/s, loss=0.576, v_num=tg8t, train_loss=0.456, val_loss=0.294]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.576, v_num=tg8t, train_loss=0.456, val_loss=0.294]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.576, v_num=tg8t, train_loss=0.456, val_loss=0.294]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 26.28it/s, loss=0.544, v_num=tg8t, train_loss=0.434, val_loss=0.294]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.544, v_num=tg8t, train_loss=0.434, val_loss=0.294]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.544, v_num=tg8t, train_loss=0.434, val_loss=0.294]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.74it/s, loss=0.533, v_num=tg8t, train_loss=0.404, val_loss=0.294]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.533, v_num=tg8t, train_loss=0.404, val_loss=0.294]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.533, v_num=tg8t, train_loss=0.404, val_loss=0.294]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.44it/s, loss=0.521, v_num=tg8t, train_loss=0.363, val_loss=0.294]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.521, v_num=tg8t, train_loss=0.363, val_loss=0.294]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.521, v_num=tg8t, train_loss=0.363, val_loss=0.294]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.20it/s, loss=0.508, v_num=tg8t, train_loss=0.324, val_loss=0.294]
Validating: 0it [00:00, ?it/s][AEpoch 24: : 2it [00:00, 31.31it/s, loss=0.508, v_num=tg8t, train_loss=0.324, val_loss=0.223]                     
                              [AEpoch 24:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.508, v_num=tg8t, train_loss=0.324, val_loss=0.223]Epoch 25:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.508, v_num=tg8t, train_loss=0.324, val_loss=0.223]Epoch 25:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 42.70it/s, loss=0.496, v_num=tg8t, train_loss=0.327, val_loss=0.223]Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.496, v_num=tg8t, train_loss=0.327, val_loss=0.223]        Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.496, v_num=tg8t, train_loss=0.327, val_loss=0.223]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.13it/s, loss=0.482, v_num=tg8t, train_loss=0.304, val_loss=0.223]Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.482, v_num=tg8t, train_loss=0.304, val_loss=0.223]        Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.482, v_num=tg8t, train_loss=0.304, val_loss=0.223]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.10it/s, loss=0.467, v_num=tg8t, train_loss=0.27, val_loss=0.223]Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.467, v_num=tg8t, train_loss=0.27, val_loss=0.223]        Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.467, v_num=tg8t, train_loss=0.27, val_loss=0.223]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.36it/s, loss=0.450, v_num=tg8t, train_loss=0.232, val_loss=0.223]Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.450, v_num=tg8t, train_loss=0.232, val_loss=0.223]        Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.450, v_num=tg8t, train_loss=0.232, val_loss=0.223]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.25it/s, loss=0.431, v_num=tg8t, train_loss=0.197, val_loss=0.223]
Validating: 0it [00:00, ?it/s][AEpoch 29: : 2it [00:00, 28.29it/s, loss=0.431, v_num=tg8t, train_loss=0.197, val_loss=0.0854]                    
                              [AEpoch 29:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.431, v_num=tg8t, train_loss=0.197, val_loss=0.0854]Epoch 30:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.431, v_num=tg8t, train_loss=0.197, val_loss=0.0854]Epoch 30:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 36.80it/s, loss=0.412, v_num=tg8t, train_loss=0.173, val_loss=0.0854]Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.412, v_num=tg8t, train_loss=0.173, val_loss=0.0854]        Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.412, v_num=tg8t, train_loss=0.173, val_loss=0.0854]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 53.59it/s, loss=0.388, v_num=tg8t, train_loss=0.15, val_loss=0.0854]Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.388, v_num=tg8t, train_loss=0.15, val_loss=0.0854]        Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.388, v_num=tg8t, train_loss=0.15, val_loss=0.0854]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.15it/s, loss=0.368, v_num=tg8t, train_loss=0.127, val_loss=0.0854]Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.368, v_num=tg8t, train_loss=0.127, val_loss=0.0854]        Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.368, v_num=tg8t, train_loss=0.127, val_loss=0.0854]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.54it/s, loss=0.347, v_num=tg8t, train_loss=0.106, val_loss=0.0854]Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.347, v_num=tg8t, train_loss=0.106, val_loss=0.0854]        Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.347, v_num=tg8t, train_loss=0.106, val_loss=0.0854]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.52it/s, loss=0.326, v_num=tg8t, train_loss=0.0931, val_loss=0.0854]
Validating: 0it [00:00, ?it/s][AEpoch 34: : 2it [00:00, 29.64it/s, loss=0.326, v_num=tg8t, train_loss=0.0931, val_loss=0.0157]                     
                              [AEpoch 34:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.326, v_num=tg8t, train_loss=0.0931, val_loss=0.0157]Epoch 35:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.326, v_num=tg8t, train_loss=0.0931, val_loss=0.0157]Epoch 35:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 47.18it/s, loss=0.302, v_num=tg8t, train_loss=0.0858, val_loss=0.0157]Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.302, v_num=tg8t, train_loss=0.0858, val_loss=0.0157]        Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.302, v_num=tg8t, train_loss=0.0858, val_loss=0.0157]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.36it/s, loss=0.279, v_num=tg8t, train_loss=0.0787, val_loss=0.0157]Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.279, v_num=tg8t, train_loss=0.0787, val_loss=0.0157]        Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.279, v_num=tg8t, train_loss=0.0787, val_loss=0.0157]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.32it/s, loss=0.258, v_num=tg8t, train_loss=0.0707, val_loss=0.0157]Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.258, v_num=tg8t, train_loss=0.0707, val_loss=0.0157]        Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.258, v_num=tg8t, train_loss=0.0707, val_loss=0.0157]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.56it/s, loss=0.237, v_num=tg8t, train_loss=0.0628, val_loss=0.0157]Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.237, v_num=tg8t, train_loss=0.0628, val_loss=0.0157]        Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.237, v_num=tg8t, train_loss=0.0628, val_loss=0.0157]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.74it/s, loss=0.216, v_num=tg8t, train_loss=0.0558, val_loss=0.0157]
Validating: 0it [00:00, ?it/s][AEpoch 39: : 2it [00:00, 44.57it/s, loss=0.216, v_num=tg8t, train_loss=0.0558, val_loss=0.0269]                     
                              [AEpoch 39:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.216, v_num=tg8t, train_loss=0.0558, val_loss=0.0269]Epoch 40:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.216, v_num=tg8t, train_loss=0.0558, val_loss=0.0269]Epoch 40:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 34.94it/s, loss=0.196, v_num=tg8t, train_loss=0.0519, val_loss=0.0269]Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.196, v_num=tg8t, train_loss=0.0519, val_loss=0.0269]        Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.196, v_num=tg8t, train_loss=0.0519, val_loss=0.0269]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.48it/s, loss=0.176, v_num=tg8t, train_loss=0.0529, val_loss=0.0269]Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.176, v_num=tg8t, train_loss=0.0529, val_loss=0.0269]        Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.176, v_num=tg8t, train_loss=0.0529, val_loss=0.0269]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.20it/s, loss=0.159, v_num=tg8t, train_loss=0.0541, val_loss=0.0269]Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.159, v_num=tg8t, train_loss=0.0541, val_loss=0.0269]        Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.159, v_num=tg8t, train_loss=0.0541, val_loss=0.0269]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.37it/s, loss=0.143, v_num=tg8t, train_loss=0.0518, val_loss=0.0269]Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.143, v_num=tg8t, train_loss=0.0518, val_loss=0.0269]        Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.143, v_num=tg8t, train_loss=0.0518, val_loss=0.0269]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.11it/s, loss=0.130, v_num=tg8t, train_loss=0.0488, val_loss=0.0269]
Validating: 0it [00:00, ?it/s][AEpoch 44: : 2it [00:00, 55.30it/s, loss=0.130, v_num=tg8t, train_loss=0.0488, val_loss=0.0521]                     
                              [AEpoch 44:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.130, v_num=tg8t, train_loss=0.0488, val_loss=0.0521]Epoch 45:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.130, v_num=tg8t, train_loss=0.0488, val_loss=0.0521]Epoch 45:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 46.20it/s, loss=0.116, v_num=tg8t, train_loss=0.0454, val_loss=0.0521]Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.116, v_num=tg8t, train_loss=0.0454, val_loss=0.0521]        Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.116, v_num=tg8t, train_loss=0.0454, val_loss=0.0521]Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.98it/s, loss=0.102, v_num=tg8t, train_loss=0.0391, val_loss=0.0521]Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.102, v_num=tg8t, train_loss=0.0391, val_loss=0.0521]        Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.102, v_num=tg8t, train_loss=0.0391, val_loss=0.0521]Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.34it/s, loss=0.090, v_num=tg8t, train_loss=0.0323, val_loss=0.0521]Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.090, v_num=tg8t, train_loss=0.0323, val_loss=0.0521]        Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.090, v_num=tg8t, train_loss=0.0323, val_loss=0.0521]Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.87it/s, loss=0.080, v_num=tg8t, train_loss=0.0305, val_loss=0.0521]Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.080, v_num=tg8t, train_loss=0.0305, val_loss=0.0521]        Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.080, v_num=tg8t, train_loss=0.0305, val_loss=0.0521]Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.89it/s, loss=0.072, v_num=tg8t, train_loss=0.0284, val_loss=0.0521]
Validating: 0it [00:00, ?it/s][AEpoch 49: : 2it [00:00, 49.25it/s, loss=0.072, v_num=tg8t, train_loss=0.0284, val_loss=0.0184]                     
                              [ASaving latest checkpoint..
I0611 08:04:21.594970 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 00050: early stopping triggered.
I0611 08:04:21.595624 140711206703488 early_stopping.py:237] Epoch 00050: early stopping triggered.
Epoch 49: : 2it [00:00, 31.42it/s, loss=0.072, v_num=tg8t, train_loss=0.0284, val_loss=0.0184]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=34.ckpt (current_epoch=49)
---------------------------------------
Planning took 239.0ms
q1a, predicted time: 737.1
q1a,(predicted 737.1),/*+ HashJoin(mi_idx it t mc ct)
 NestLoop(mc ct)
 HashJoin(mi_idx it t)
 MergeJoin(mi_idx it)
 IndexScan(mi_idx)
 IndexScan(it)
 SeqScan(t)
 IndexScan(mc)
 SeqScan(ct)
 Leading((((mi_idx it) t) (mc ct))) */
---------------------------------------
Planning took 51.8ms
q1b, predicted time: 562.5
q1b,(predicted 562.5),/*+ MergeJoin(mi_idx it mc ct t)
 NestLoop(mi_idx it mc ct)
 NestLoop(mi_idx it mc)
 HashJoin(mi_idx it)
 IndexScan(mi_idx)
 IndexScan(it)
 IndexScan(mc)
 SeqScan(ct)
 SeqScan(t)
 Leading(((((mi_idx it) mc) ct) t)) */
---------------------------------------
Planning took 164.5ms
q1c, predicted time: 358.9
q1c,(predicted 358.9),/*+ HashJoin(it t mi_idx ct mc)
 HashJoin(t mi_idx ct mc)
 MergeJoin(mi_idx ct mc)
 MergeJoin(ct mc)
 IndexScan(it)
 SeqScan(t)
 SeqScan(mi_idx)
 SeqScan(ct)
 SeqScan(mc)
 Leading((it (t (mi_idx (ct mc))))) */
---------------------------------------
Planning took 305.1ms
q1d, predicted time: 696.9
q1d,(predicted 696.9),/*+ HashJoin(mi_idx it t mc ct)
 MergeJoin(mc ct)
 HashJoin(mi_idx it t)
 MergeJoin(mi_idx it)
 IndexScan(mi_idx)
 IndexScan(it)
 SeqScan(t)
 SeqScan(mc)
 SeqScan(ct)
 Leading((((mi_idx it) t) (mc ct))) */
Waiting on Ray tasks...value_iter=7
Running 1a: hinted plan
Hash Join cost=737.1397094726562
  Hash Join cost=527.3030395507812
    Merge Join cost=479.9460144042969
      Index Scan [movie_info_idx AS mi_idx] cost=13685.15
      Index Scan [info_type AS it] cost=2.41
    Seq Scan [title AS t] cost=0.58
  Nested Loop cost=478.4142150878906
    Index Scan [movie_companies AS mc] cost=0.64
    Seq Scan [company_type AS ct] cost=1.05

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,833.9,/*+ HashJoin(mi_idx it t mc ct)
 NestLoop(mc ct)
 HashJoin(mi_idx it t)
 MergeJoin(mi_idx it)
 IndexScan(mi_idx)
 IndexScan(it)
 SeqScan(t)
 IndexScan(mc)
 SeqScan(ct)
 Leading((((mi_idx it) t) (mc ct))) */
1a Execution time: 833.9 (predicted 737.1) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  943.5 (predicted 564.8)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  4165518.5  737.1  /*+ Leading((((mi_idx it) t) (mc ct))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout occurred; checking the hint against local PG.
Running 1b: hinted plan
Merge Join cost=562.4871826171875
  Nested Loop cost=562.0424194335938
    Nested Loop cost=561.6041259765625
      Hash Join cost=570.303955078125
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41
      Index Scan [movie_companies AS mc] cost=0.61
    Seq Scan [company_type AS ct] cost=1.05
  Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,-1.0,/*+ MergeJoin(mi_idx it mc ct t)
 NestLoop(mi_idx it mc ct)
 NestLoop(mi_idx it mc)
 HashJoin(mi_idx it)
 IndexScan(mi_idx)
 IndexScan(it)
 IndexScan(mc)
 SeqScan(ct)
 SeqScan(t)
 Leading(((((mi_idx it) mc) ct) t)) */
1b Execution time: -1.0 (predicted 562.5) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  62.3 (predicted 571.3)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3810977.8  562.5  /*+ Leading(((((mi_idx it) mc) ct) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Hash Join cost=358.85260009765625
  Index Scan [info_type AS it] cost=2.41
  Hash Join cost=358.8148498535156
    Seq Scan [title AS t] cost=0.58
    Merge Join cost=358.7234191894531
      Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
      Merge Join cost=359.0080871582031
        Seq Scan [company_type AS ct] cost=1.05
        Seq Scan [movie_companies AS mc] cost=0.62

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,1296.1,/*+ HashJoin(it t mi_idx ct mc)
 HashJoin(t mi_idx ct mc)
 MergeJoin(mi_idx ct mc)
 MergeJoin(ct mc)
 IndexScan(it)
 SeqScan(t)
 SeqScan(mi_idx)
 SeqScan(ct)
 SeqScan(mc)
 Leading((it (t (mi_idx (ct mc))))) */
1c Execution time: 1296.1 (predicted 358.9) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  56.8 (predicted 359.5)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1711379.4  358.9  /*+ Leading((it (t (mi_idx (ct mc))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1d: hinted plan
Hash Join cost=696.94384765625
  Hash Join cost=519.0323486328125
    Merge Join cost=489.2555236816406
      Index Scan [movie_info_idx AS mi_idx] cost=13685.15
      Index Scan [info_type AS it] cost=2.41
    Seq Scan [title AS t] cost=0.58
  Merge Join cost=486.9780578613281
    Seq Scan [movie_companies AS mc] cost=0.61
    Seq Scan [company_type AS ct] cost=1.05

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,484.3,/*+ HashJoin(mi_idx it t mc ct)
 MergeJoin(mc ct)
 HashJoin(mi_idx it t)
 MergeJoin(mi_idx it)
 IndexScan(mi_idx)
 IndexScan(it)
 SeqScan(t)
 SeqScan(mc)
 SeqScan(ct)
 Leading((((mi_idx it) t) (mc ct))) */
1d Execution time: 484.3 (predicted 696.9) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  55.8 (predicted 562.0)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  4102009.8  696.9  /*+ Leading((((mi_idx it) t) (mc ct))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout detected! Assigning a special label 4096000 (server_ip=172.17.0.2)
---------------------------------------
Planning took 263.8ms
[Test set] q2a, predicted time: 737.9
q2a,(predicted 737.9),/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
---------------------------------------
Planning took 302.7ms
[Test set] q2b, predicted time: 737.9
q2b,(predicted 737.9),/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
---------------------------------------
Planning took 300.4ms
[Test set] q2c, predicted time: 737.9
q2c,(predicted 737.9),/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
---------------------------------------
Planning took 278.8ms
[Test set] q2d, predicted time: 738.1
q2d,(predicted 738.1),/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
[Test set] Waiting on Ray tasks...value_iter=7
[Test set] Running 2a: hinted plan
Hash Join cost=737.9201049804688
  Hash Join cost=531.0704345703125
    Hash Join cost=484.49951171875
      Seq Scan [movie_companies AS mc] cost=0.54
      Index Scan [company_name AS cn] cost=0.45
    Seq Scan [title AS t] cost=0.49
  Nested Loop cost=482.524169921875
    Seq Scan [keyword AS k] cost=2626.12
    Index Scan [movie_keyword AS mk] cost=1229.38

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,568.7,/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
2a Execution time: 568.7 (predicted 737.9) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 556.1)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3716836.2  737.9  /*+ Leading((((mc cn) t) (k mk))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Hash Join cost=737.912353515625
  Hash Join cost=531.0531616210938
    Hash Join cost=484.52728271484375
      Seq Scan [movie_companies AS mc] cost=0.54
      Index Scan [company_name AS cn] cost=0.45
    Seq Scan [title AS t] cost=0.49
  Nested Loop cost=482.549072265625
    Seq Scan [keyword AS k] cost=2626.12
    Index Scan [movie_keyword AS mk] cost=1229.38

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,477.8,/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
2b Execution time: 477.8 (predicted 737.9) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 556.1)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3719314.8  737.9  /*+ Leading((((mc cn) t) (k mk))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Hash Join cost=737.909912109375
  Hash Join cost=531.0478515625
    Hash Join cost=484.535400390625
      Seq Scan [movie_companies AS mc] cost=0.54
      Index Scan [company_name AS cn] cost=0.45
    Seq Scan [title AS t] cost=0.49
  Nested Loop cost=482.5559997558594
    Seq Scan [keyword AS k] cost=2626.12
    Index Scan [movie_keyword AS mk] cost=1229.38

filters
{'company_name AS cn': "((cn.country_code)::text = '[sm]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2c,413.0,/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
2c Execution time: 413.0 (predicted 737.9) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  269.8 (predicted 556.1)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3720020.5  737.9  /*+ Leading((((mc cn) t) (k mk))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2d: hinted plan
Hash Join cost=738.0589599609375
  Hash Join cost=531.4432373046875
    Hash Join cost=484.1484375
      Seq Scan [movie_companies AS mc] cost=0.54
      Index Scan [company_name AS cn] cost=0.45
    Seq Scan [title AS t] cost=0.49
  Nested Loop cost=482.2312316894531
    Seq Scan [keyword AS k] cost=2626.12
    Index Scan [movie_keyword AS mk] cost=1229.38

filters
{'company_name AS cn': "((cn.country_code)::text = '[us]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2d,631.7,/*+ HashJoin(mc cn t k mk)
 NestLoop(k mk)
 HashJoin(mc cn t)
 HashJoin(mc cn)
 SeqScan(mc)
 IndexScan(cn)
 SeqScan(t)
 SeqScan(k)
 IndexScan(mk)
 Leading((((mc cn) t) (k mk))) */
2d Execution time: 631.7 (predicted 738.1) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  4175.6 (predicted 561.8)  /*+ Leading(((((k mk) t) mc) cn)) */
SIM-predicted costs, predicted latency, plan: 
  3696245.8  738.1  /*+ Leading((((mc cn) t) (k mk))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Dropping buffer cache.
8 rels: ['company_name' 'company_type' 'info_type' 'keyword' 'movie_companies'
 'movie_info_idx' 'movie_keyword' 'title']
8 rel_ids: ['company_name AS cn' 'company_type AS ct' 'info_type AS it'
 'keyword AS k' 'movie_companies AS mc' 'movie_info_idx AS mi_idx'
 'movie_keyword AS mk' 'title AS t']
3 scans: ['Index Only Scan' 'Index Scan' 'Seq Scan']
3 joins: ['Hash Join' 'Merge Join' 'Nested Loop']
11 all ops: ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Only Scan'
 'Index Scan' 'Merge Join' 'Nested Loop' 'PartialAggregate' 'Seq Scan'
 'SimpleAggregate']
Calling make_and_featurize_trees()...
took 0.0s
num_total_subtrees=128 num_featurized_subtrees=16 num_new_datapoints=15
head
  query=2.269 feat=21.0 cost=833.929
  query=2.269 feat=10.0 cost=833.929
  query=2.269 feat=5.0 cost=833.929
tail
  query=2.313 feat=5.0 cost=484.319
  query=2.313 feat=5.0 cost=484.319
  query=2.313 feat=10.0 cost=484.319
costs stats mean 8.265634432893979 std 3.36316402177106
num_train=14 num_validation=2
InitializeModel curr_value_iter=8
Assigning real model := 0.0*SIM + 1.0*previous real model
iter 8 lr 0.001
GPU available: False, used: False
I0611 08:04:29.563565 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 08:04:29.564386 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]                                           /home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 32.40it/s, loss=1.148, v_num=tg8t, train_loss=1.15]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.148, v_num=tg8t, train_loss=1.15]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.148, v_num=tg8t, train_loss=1.15]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.54it/s, loss=1.032, v_num=tg8t, train_loss=0.916]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.032, v_num=tg8t, train_loss=0.916]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.032, v_num=tg8t, train_loss=0.916]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.05it/s, loss=0.917, v_num=tg8t, train_loss=0.686]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.917, v_num=tg8t, train_loss=0.686]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.917, v_num=tg8t, train_loss=0.686]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.14it/s, loss=0.836, v_num=tg8t, train_loss=0.593]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.836, v_num=tg8t, train_loss=0.593]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.836, v_num=tg8t, train_loss=0.593]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.38it/s, loss=0.761, v_num=tg8t, train_loss=0.46]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 4: : 2it [00:00, 30.68it/s, loss=0.761, v_num=tg8t, train_loss=0.46, val_loss=0.7]       
                              [AEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.761, v_num=tg8t, train_loss=0.46, val_loss=0.7]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.761, v_num=tg8t, train_loss=0.46, val_loss=0.7]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 46.12it/s, loss=0.699, v_num=tg8t, train_loss=0.393, val_loss=0.7]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.699, v_num=tg8t, train_loss=0.393, val_loss=0.7]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.699, v_num=tg8t, train_loss=0.393, val_loss=0.7]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.83it/s, loss=0.651, v_num=tg8t, train_loss=0.359, val_loss=0.7]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.651, v_num=tg8t, train_loss=0.359, val_loss=0.7]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.651, v_num=tg8t, train_loss=0.359, val_loss=0.7]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.23it/s, loss=0.610, v_num=tg8t, train_loss=0.326, val_loss=0.7]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.610, v_num=tg8t, train_loss=0.326, val_loss=0.7]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.610, v_num=tg8t, train_loss=0.326, val_loss=0.7]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.67it/s, loss=0.580, v_num=tg8t, train_loss=0.337, val_loss=0.7]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.580, v_num=tg8t, train_loss=0.337, val_loss=0.7]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.580, v_num=tg8t, train_loss=0.337, val_loss=0.7]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.47it/s, loss=0.557, v_num=tg8t, train_loss=0.353, val_loss=0.7]
Validating: 0it [00:00, ?it/s][AEpoch 9: : 2it [00:00, 29.83it/s, loss=0.557, v_num=tg8t, train_loss=0.353, val_loss=0.614]                   
                              [AEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.557, v_num=tg8t, train_loss=0.353, val_loss=0.614]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.557, v_num=tg8t, train_loss=0.353, val_loss=0.614]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 33.28it/s, loss=0.536, v_num=tg8t, train_loss=0.329, val_loss=0.614]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.536, v_num=tg8t, train_loss=0.329, val_loss=0.614]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.536, v_num=tg8t, train_loss=0.329, val_loss=0.614]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 58.48it/s, loss=0.516, v_num=tg8t, train_loss=0.288, val_loss=0.614]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.516, v_num=tg8t, train_loss=0.288, val_loss=0.614]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.516, v_num=tg8t, train_loss=0.288, val_loss=0.614]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.28it/s, loss=0.498, v_num=tg8t, train_loss=0.28, val_loss=0.614]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.498, v_num=tg8t, train_loss=0.28, val_loss=0.614]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.498, v_num=tg8t, train_loss=0.28, val_loss=0.614]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.92it/s, loss=0.482, v_num=tg8t, train_loss=0.275, val_loss=0.614]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.482, v_num=tg8t, train_loss=0.275, val_loss=0.614]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.482, v_num=tg8t, train_loss=0.275, val_loss=0.614]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.30it/s, loss=0.468, v_num=tg8t, train_loss=0.272, val_loss=0.614]
Validating: 0it [00:00, ?it/s][AEpoch 14: : 2it [00:00, 30.17it/s, loss=0.468, v_num=tg8t, train_loss=0.272, val_loss=0.542]                     
                              [AEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.468, v_num=tg8t, train_loss=0.272, val_loss=0.542]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.468, v_num=tg8t, train_loss=0.272, val_loss=0.542]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 53.18it/s, loss=0.454, v_num=tg8t, train_loss=0.249, val_loss=0.542]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.454, v_num=tg8t, train_loss=0.249, val_loss=0.542]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.454, v_num=tg8t, train_loss=0.249, val_loss=0.542]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.72it/s, loss=0.441, v_num=tg8t, train_loss=0.239, val_loss=0.542]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.441, v_num=tg8t, train_loss=0.239, val_loss=0.542]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.441, v_num=tg8t, train_loss=0.239, val_loss=0.542]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.47it/s, loss=0.429, v_num=tg8t, train_loss=0.222, val_loss=0.542]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.429, v_num=tg8t, train_loss=0.222, val_loss=0.542]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.429, v_num=tg8t, train_loss=0.222, val_loss=0.542]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 59.26it/s, loss=0.418, v_num=tg8t, train_loss=0.207, val_loss=0.542]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.418, v_num=tg8t, train_loss=0.207, val_loss=0.542]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.418, v_num=tg8t, train_loss=0.207, val_loss=0.542]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.56it/s, loss=0.406, v_num=tg8t, train_loss=0.197, val_loss=0.542]
Validating: 0it [00:00, ?it/s][AEpoch 19: : 2it [00:00, 28.68it/s, loss=0.406, v_num=tg8t, train_loss=0.197, val_loss=0.439]                     
                              [AEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.406, v_num=tg8t, train_loss=0.197, val_loss=0.439]Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.406, v_num=tg8t, train_loss=0.197, val_loss=0.439]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 37.92it/s, loss=0.358, v_num=tg8t, train_loss=0.176, val_loss=0.439]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.358, v_num=tg8t, train_loss=0.176, val_loss=0.439]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.358, v_num=tg8t, train_loss=0.176, val_loss=0.439]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 51.68it/s, loss=0.321, v_num=tg8t, train_loss=0.17, val_loss=0.439]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.321, v_num=tg8t, train_loss=0.17, val_loss=0.439]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.321, v_num=tg8t, train_loss=0.17, val_loss=0.439]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.62it/s, loss=0.295, v_num=tg8t, train_loss=0.165, val_loss=0.439]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.295, v_num=tg8t, train_loss=0.165, val_loss=0.439]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.295, v_num=tg8t, train_loss=0.165, val_loss=0.439]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.42it/s, loss=0.273, v_num=tg8t, train_loss=0.156, val_loss=0.439]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.273, v_num=tg8t, train_loss=0.156, val_loss=0.439]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.273, v_num=tg8t, train_loss=0.156, val_loss=0.439]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.32it/s, loss=0.256, v_num=tg8t, train_loss=0.13, val_loss=0.439]
Validating: 0it [00:00, ?it/s][AEpoch 24: : 2it [00:00, 29.83it/s, loss=0.256, v_num=tg8t, train_loss=0.13, val_loss=0.373]                     
                              [AEpoch 24:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.256, v_num=tg8t, train_loss=0.13, val_loss=0.373]Epoch 25:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.256, v_num=tg8t, train_loss=0.13, val_loss=0.373]Epoch 25:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 47.20it/s, loss=0.247, v_num=tg8t, train_loss=0.213, val_loss=0.373]Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.247, v_num=tg8t, train_loss=0.213, val_loss=0.373]        Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.247, v_num=tg8t, train_loss=0.213, val_loss=0.373]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.22it/s, loss=0.239, v_num=tg8t, train_loss=0.194, val_loss=0.373]Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.239, v_num=tg8t, train_loss=0.194, val_loss=0.373]        Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.239, v_num=tg8t, train_loss=0.194, val_loss=0.373]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.14it/s, loss=0.235, v_num=tg8t, train_loss=0.256, val_loss=0.373]Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.235, v_num=tg8t, train_loss=0.256, val_loss=0.373]        Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.235, v_num=tg8t, train_loss=0.256, val_loss=0.373]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.94it/s, loss=0.229, v_num=tg8t, train_loss=0.215, val_loss=0.373]Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.229, v_num=tg8t, train_loss=0.215, val_loss=0.373]        Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.229, v_num=tg8t, train_loss=0.215, val_loss=0.373]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.60it/s, loss=0.219, v_num=tg8t, train_loss=0.153, val_loss=0.373]
Validating: 0it [00:00, ?it/s][AEpoch 29: : 2it [00:00, 40.19it/s, loss=0.219, v_num=tg8t, train_loss=0.153, val_loss=1.01]                      
                              [AEpoch 29:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.219, v_num=tg8t, train_loss=0.153, val_loss=1.01]Epoch 30:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.219, v_num=tg8t, train_loss=0.153, val_loss=1.01]Epoch 30:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 37.26it/s, loss=0.217, v_num=tg8t, train_loss=0.289, val_loss=1.01]Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.217, v_num=tg8t, train_loss=0.289, val_loss=1.01]        Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.217, v_num=tg8t, train_loss=0.289, val_loss=1.01]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.89it/s, loss=0.215, v_num=tg8t, train_loss=0.238, val_loss=1.01]Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.215, v_num=tg8t, train_loss=0.238, val_loss=1.01]        Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.215, v_num=tg8t, train_loss=0.238, val_loss=1.01]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.53it/s, loss=0.207, v_num=tg8t, train_loss=0.12, val_loss=1.01]Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.207, v_num=tg8t, train_loss=0.12, val_loss=1.01]        Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.207, v_num=tg8t, train_loss=0.12, val_loss=1.01]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.99it/s, loss=0.199, v_num=tg8t, train_loss=0.121, val_loss=1.01]Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.199, v_num=tg8t, train_loss=0.121, val_loss=1.01]        Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.199, v_num=tg8t, train_loss=0.121, val_loss=1.01]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.51it/s, loss=0.191, v_num=tg8t, train_loss=0.113, val_loss=1.01]
Validating: 0it [00:00, ?it/s][AEpoch 34: : 2it [00:00, 24.09it/s, loss=0.191, v_num=tg8t, train_loss=0.113, val_loss=0.239]                    
                              [AEpoch 34:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.191, v_num=tg8t, train_loss=0.113, val_loss=0.239]Epoch 35:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.191, v_num=tg8t, train_loss=0.113, val_loss=0.239]Epoch 35:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 51.06it/s, loss=0.184, v_num=tg8t, train_loss=0.0984, val_loss=0.239]Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.184, v_num=tg8t, train_loss=0.0984, val_loss=0.239]        Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.184, v_num=tg8t, train_loss=0.0984, val_loss=0.239]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.75it/s, loss=0.176, v_num=tg8t, train_loss=0.0833, val_loss=0.239]Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.176, v_num=tg8t, train_loss=0.0833, val_loss=0.239]        Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.176, v_num=tg8t, train_loss=0.0833, val_loss=0.239]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.67it/s, loss=0.168, v_num=tg8t, train_loss=0.0709, val_loss=0.239]Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.168, v_num=tg8t, train_loss=0.0709, val_loss=0.239]        Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.168, v_num=tg8t, train_loss=0.0709, val_loss=0.239]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.67it/s, loss=0.161, v_num=tg8t, train_loss=0.0617, val_loss=0.239]Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.161, v_num=tg8t, train_loss=0.0617, val_loss=0.239]        Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.161, v_num=tg8t, train_loss=0.0617, val_loss=0.239]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.96it/s, loss=0.154, v_num=tg8t, train_loss=0.0556, val_loss=0.239]
Validating: 0it [00:00, ?it/s][AEpoch 39: : 2it [00:00, 29.33it/s, loss=0.154, v_num=tg8t, train_loss=0.0556, val_loss=0.171]                     
                              [AEpoch 39:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.154, v_num=tg8t, train_loss=0.0556, val_loss=0.171]Epoch 40:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.154, v_num=tg8t, train_loss=0.0556, val_loss=0.171]Epoch 40:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 38.10it/s, loss=0.148, v_num=tg8t, train_loss=0.0527, val_loss=0.171]Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.148, v_num=tg8t, train_loss=0.0527, val_loss=0.171]        Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.148, v_num=tg8t, train_loss=0.0527, val_loss=0.171]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.99it/s, loss=0.142, v_num=tg8t, train_loss=0.0582, val_loss=0.171]Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.142, v_num=tg8t, train_loss=0.0582, val_loss=0.171]        Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.142, v_num=tg8t, train_loss=0.0582, val_loss=0.171]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 57.04it/s, loss=0.137, v_num=tg8t, train_loss=0.0614, val_loss=0.171]Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.137, v_num=tg8t, train_loss=0.0614, val_loss=0.171]        Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.137, v_num=tg8t, train_loss=0.0614, val_loss=0.171]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.59it/s, loss=0.131, v_num=tg8t, train_loss=0.0433, val_loss=0.171]Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.131, v_num=tg8t, train_loss=0.0433, val_loss=0.171]        Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.131, v_num=tg8t, train_loss=0.0433, val_loss=0.171]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.86it/s, loss=0.127, v_num=tg8t, train_loss=0.0372, val_loss=0.171]
Validating: 0it [00:00, ?it/s][AEpoch 44: : 2it [00:00, 31.06it/s, loss=0.127, v_num=tg8t, train_loss=0.0372, val_loss=0.16]                      
                              [AEpoch 44:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.127, v_num=tg8t, train_loss=0.0372, val_loss=0.16]Epoch 45:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.127, v_num=tg8t, train_loss=0.0372, val_loss=0.16]Epoch 45:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 53.23it/s, loss=0.118, v_num=tg8t, train_loss=0.0348, val_loss=0.16]Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.118, v_num=tg8t, train_loss=0.0348, val_loss=0.16]        Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.118, v_num=tg8t, train_loss=0.0348, val_loss=0.16]Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.40it/s, loss=0.110, v_num=tg8t, train_loss=0.034, val_loss=0.16]Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.110, v_num=tg8t, train_loss=0.034, val_loss=0.16]        Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.110, v_num=tg8t, train_loss=0.034, val_loss=0.16]Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.05it/s, loss=0.099, v_num=tg8t, train_loss=0.0332, val_loss=0.16]Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.099, v_num=tg8t, train_loss=0.0332, val_loss=0.16]        Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.099, v_num=tg8t, train_loss=0.0332, val_loss=0.16]Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 52.09it/s, loss=0.090, v_num=tg8t, train_loss=0.0318, val_loss=0.16]Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.090, v_num=tg8t, train_loss=0.0318, val_loss=0.16]        Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.090, v_num=tg8t, train_loss=0.0318, val_loss=0.16]Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.51it/s, loss=0.083, v_num=tg8t, train_loss=0.0295, val_loss=0.16]
Validating: 0it [00:00, ?it/s][AEpoch 49: : 2it [00:00, 28.13it/s, loss=0.083, v_num=tg8t, train_loss=0.0295, val_loss=0.132]                    
                              [AEpoch 49:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.083, v_num=tg8t, train_loss=0.0295, val_loss=0.132]Epoch 50:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.083, v_num=tg8t, train_loss=0.0295, val_loss=0.132]Epoch 50:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 34.30it/s, loss=0.070, v_num=tg8t, train_loss=0.0266, val_loss=0.132]Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.070, v_num=tg8t, train_loss=0.0266, val_loss=0.132]        Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.070, v_num=tg8t, train_loss=0.0266, val_loss=0.132]Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.82it/s, loss=0.059, v_num=tg8t, train_loss=0.0235, val_loss=0.132]Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.059, v_num=tg8t, train_loss=0.0235, val_loss=0.132]        Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.059, v_num=tg8t, train_loss=0.0235, val_loss=0.132]Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.31it/s, loss=0.055, v_num=tg8t, train_loss=0.0206, val_loss=0.132]Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.055, v_num=tg8t, train_loss=0.0206, val_loss=0.132]        Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.055, v_num=tg8t, train_loss=0.0206, val_loss=0.132]Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.52it/s, loss=0.049, v_num=tg8t, train_loss=0.0183, val_loss=0.132]Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.049, v_num=tg8t, train_loss=0.0183, val_loss=0.132]        Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.049, v_num=tg8t, train_loss=0.0183, val_loss=0.132]Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.06it/s, loss=0.045, v_num=tg8t, train_loss=0.0162, val_loss=0.132]
Validating: 0it [00:00, ?it/s][AEpoch 54: : 2it [00:00, 29.22it/s, loss=0.045, v_num=tg8t, train_loss=0.0162, val_loss=0.0602]                    
                              [AEpoch 54:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.045, v_num=tg8t, train_loss=0.0162, val_loss=0.0602]Epoch 55:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.045, v_num=tg8t, train_loss=0.0162, val_loss=0.0602]Epoch 55:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 46.02it/s, loss=0.040, v_num=tg8t, train_loss=0.0143, val_loss=0.0602]Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.040, v_num=tg8t, train_loss=0.0143, val_loss=0.0602]        Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.040, v_num=tg8t, train_loss=0.0143, val_loss=0.0602]Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.01it/s, loss=0.037, v_num=tg8t, train_loss=0.0125, val_loss=0.0602]Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.037, v_num=tg8t, train_loss=0.0125, val_loss=0.0602]        Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.037, v_num=tg8t, train_loss=0.0125, val_loss=0.0602]Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.99it/s, loss=0.034, v_num=tg8t, train_loss=0.0108, val_loss=0.0602]Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.034, v_num=tg8t, train_loss=0.0108, val_loss=0.0602]        Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.034, v_num=tg8t, train_loss=0.0108, val_loss=0.0602]Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.05it/s, loss=0.031, v_num=tg8t, train_loss=0.00936, val_loss=0.0602]Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.031, v_num=tg8t, train_loss=0.00936, val_loss=0.0602]        Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.031, v_num=tg8t, train_loss=0.00936, val_loss=0.0602]Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.57it/s, loss=0.029, v_num=tg8t, train_loss=0.00828, val_loss=0.0602]
Validating: 0it [00:00, ?it/s][AEpoch 59: : 2it [00:00, 27.86it/s, loss=0.029, v_num=tg8t, train_loss=0.00828, val_loss=0.0266]                     
                              [AEpoch 59:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.029, v_num=tg8t, train_loss=0.00828, val_loss=0.0266]Epoch 60:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.029, v_num=tg8t, train_loss=0.00828, val_loss=0.0266]Epoch 60:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 30.48it/s, loss=0.027, v_num=tg8t, train_loss=0.00747, val_loss=0.0266]Epoch 60:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.027, v_num=tg8t, train_loss=0.00747, val_loss=0.0266]        Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.027, v_num=tg8t, train_loss=0.00747, val_loss=0.0266]Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.71it/s, loss=0.024, v_num=tg8t, train_loss=0.00678, val_loss=0.0266]Epoch 61:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.024, v_num=tg8t, train_loss=0.00678, val_loss=0.0266]        Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.024, v_num=tg8t, train_loss=0.00678, val_loss=0.0266]Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.99it/s, loss=0.021, v_num=tg8t, train_loss=0.00609, val_loss=0.0266]Epoch 62:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.021, v_num=tg8t, train_loss=0.00609, val_loss=0.0266]        Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.021, v_num=tg8t, train_loss=0.00609, val_loss=0.0266]Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.40it/s, loss=0.019, v_num=tg8t, train_loss=0.00534, val_loss=0.0266]Epoch 63:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.019, v_num=tg8t, train_loss=0.00534, val_loss=0.0266]        Epoch 64:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.019, v_num=tg8t, train_loss=0.00534, val_loss=0.0266]Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.40it/s, loss=0.018, v_num=tg8t, train_loss=0.00459, val_loss=0.0266]
Validating: 0it [00:00, ?it/s][AEpoch 64: : 2it [00:00, 27.32it/s, loss=0.018, v_num=tg8t, train_loss=0.00459, val_loss=0.0112]                     
                              [AEpoch 64:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.018, v_num=tg8t, train_loss=0.00459, val_loss=0.0112]Epoch 65:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.018, v_num=tg8t, train_loss=0.00459, val_loss=0.0112]Epoch 65:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 44.85it/s, loss=0.016, v_num=tg8t, train_loss=0.00393, val_loss=0.0112]Epoch 65:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.016, v_num=tg8t, train_loss=0.00393, val_loss=0.0112]        Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.016, v_num=tg8t, train_loss=0.00393, val_loss=0.0112]Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.62it/s, loss=0.015, v_num=tg8t, train_loss=0.00341, val_loss=0.0112]Epoch 66:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.015, v_num=tg8t, train_loss=0.00341, val_loss=0.0112]        Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.015, v_num=tg8t, train_loss=0.00341, val_loss=0.0112]Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.25it/s, loss=0.013, v_num=tg8t, train_loss=0.00298, val_loss=0.0112]Epoch 67:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.013, v_num=tg8t, train_loss=0.00298, val_loss=0.0112]        Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.013, v_num=tg8t, train_loss=0.00298, val_loss=0.0112]Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.30it/s, loss=0.012, v_num=tg8t, train_loss=0.00259, val_loss=0.0112]Epoch 68:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.012, v_num=tg8t, train_loss=0.00259, val_loss=0.0112]        Epoch 69:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.012, v_num=tg8t, train_loss=0.00259, val_loss=0.0112]Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.78it/s, loss=0.010, v_num=tg8t, train_loss=0.00217, val_loss=0.0112]
Validating: 0it [00:00, ?it/s][AEpoch 69: : 2it [00:00, 26.69it/s, loss=0.010, v_num=tg8t, train_loss=0.00217, val_loss=0.0046]                     
                              [AEpoch 69:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00217, val_loss=0.0046]Epoch 70:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00217, val_loss=0.0046]Epoch 70:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 39.80it/s, loss=0.009, v_num=tg8t, train_loss=0.00174, val_loss=0.0046]Epoch 70:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00174, val_loss=0.0046]        Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00174, val_loss=0.0046]Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.22it/s, loss=0.008, v_num=tg8t, train_loss=0.00138, val_loss=0.0046]Epoch 71:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.008, v_num=tg8t, train_loss=0.00138, val_loss=0.0046]        Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.008, v_num=tg8t, train_loss=0.00138, val_loss=0.0046]Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 38.43it/s, loss=0.007, v_num=tg8t, train_loss=0.00112, val_loss=0.0046]Epoch 72:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.007, v_num=tg8t, train_loss=0.00112, val_loss=0.0046]        Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.007, v_num=tg8t, train_loss=0.00112, val_loss=0.0046]Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.50it/s, loss=0.006, v_num=tg8t, train_loss=0.000945, val_loss=0.0046]Epoch 73:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.006, v_num=tg8t, train_loss=0.000945, val_loss=0.0046]        Epoch 74:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.006, v_num=tg8t, train_loss=0.000945, val_loss=0.0046]Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.90it/s, loss=0.005, v_num=tg8t, train_loss=0.000802, val_loss=0.0046]
Validating: 0it [00:00, ?it/s][AEpoch 74: : 2it [00:00, 24.31it/s, loss=0.005, v_num=tg8t, train_loss=0.000802, val_loss=0.00277]                    
                              [AEpoch 74:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.005, v_num=tg8t, train_loss=0.000802, val_loss=0.00277]Epoch 75:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.005, v_num=tg8t, train_loss=0.000802, val_loss=0.00277]Epoch 75:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 39.13it/s, loss=0.005, v_num=tg8t, train_loss=0.000654, val_loss=0.00277]Epoch 75:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.005, v_num=tg8t, train_loss=0.000654, val_loss=0.00277]        Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.005, v_num=tg8t, train_loss=0.000654, val_loss=0.00277]Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.90it/s, loss=0.004, v_num=tg8t, train_loss=0.000506, val_loss=0.00277]Epoch 76:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.004, v_num=tg8t, train_loss=0.000506, val_loss=0.00277]        Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.004, v_num=tg8t, train_loss=0.000506, val_loss=0.00277]Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.20it/s, loss=0.004, v_num=tg8t, train_loss=0.000395, val_loss=0.00277]Epoch 77:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.004, v_num=tg8t, train_loss=0.000395, val_loss=0.00277]        Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.004, v_num=tg8t, train_loss=0.000395, val_loss=0.00277]Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.30it/s, loss=0.003, v_num=tg8t, train_loss=0.000338, val_loss=0.00277]Epoch 78:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.003, v_num=tg8t, train_loss=0.000338, val_loss=0.00277]        Epoch 79:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.003, v_num=tg8t, train_loss=0.000338, val_loss=0.00277]Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.14it/s, loss=0.003, v_num=tg8t, train_loss=0.000319, val_loss=0.00277]
Validating: 0it [00:00, ?it/s][AEpoch 79: : 2it [00:00, 27.19it/s, loss=0.003, v_num=tg8t, train_loss=0.000319, val_loss=0.000759]                    
                              [AEpoch 79:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.003, v_num=tg8t, train_loss=0.000319, val_loss=0.000759]Epoch 80:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.003, v_num=tg8t, train_loss=0.000319, val_loss=0.000759]Epoch 80:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 31.78it/s, loss=0.002, v_num=tg8t, train_loss=0.000306, val_loss=0.000759]Epoch 80:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.002, v_num=tg8t, train_loss=0.000306, val_loss=0.000759]        Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.002, v_num=tg8t, train_loss=0.000306, val_loss=0.000759]Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.00it/s, loss=0.002, v_num=tg8t, train_loss=0.000275, val_loss=0.000759]Epoch 81:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.002, v_num=tg8t, train_loss=0.000275, val_loss=0.000759]        Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.002, v_num=tg8t, train_loss=0.000275, val_loss=0.000759]Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.85it/s, loss=0.002, v_num=tg8t, train_loss=0.000235, val_loss=0.000759]Epoch 82:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.002, v_num=tg8t, train_loss=0.000235, val_loss=0.000759]        Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.002, v_num=tg8t, train_loss=0.000235, val_loss=0.000759]Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.74it/s, loss=0.001, v_num=tg8t, train_loss=0.000208, val_loss=0.000759]Epoch 83:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000208, val_loss=0.000759]        Epoch 84:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000208, val_loss=0.000759]Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.49it/s, loss=0.001, v_num=tg8t, train_loss=0.000205, val_loss=0.000759]
Validating: 0it [00:00, ?it/s][AEpoch 84: : 2it [00:00, 29.21it/s, loss=0.001, v_num=tg8t, train_loss=0.000205, val_loss=0.000484]                     
                              [AEpoch 84:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000205, val_loss=0.000484]Epoch 85:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000205, val_loss=0.000484]Epoch 85:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 45.84it/s, loss=0.001, v_num=tg8t, train_loss=0.000214, val_loss=0.000484]Epoch 85:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000214, val_loss=0.000484]        Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000214, val_loss=0.000484]Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.19it/s, loss=0.001, v_num=tg8t, train_loss=0.000216, val_loss=0.000484]Epoch 86:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000216, val_loss=0.000484]        Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000216, val_loss=0.000484]Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.05it/s, loss=0.001, v_num=tg8t, train_loss=0.000203, val_loss=0.000484]Epoch 87:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000203, val_loss=0.000484]        Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000203, val_loss=0.000484]Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.09it/s, loss=0.001, v_num=tg8t, train_loss=0.000184, val_loss=0.000484]Epoch 88:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000184, val_loss=0.000484]        Epoch 89:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000184, val_loss=0.000484]Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.43it/s, loss=0.001, v_num=tg8t, train_loss=0.000174, val_loss=0.000484]
Validating: 0it [00:00, ?it/s][AEpoch 89: : 2it [00:00, 26.90it/s, loss=0.001, v_num=tg8t, train_loss=0.000174, val_loss=0.000116]                     
                              [AEpoch 89:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000174, val_loss=0.000116]Epoch 90:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.001, v_num=tg8t, train_loss=0.000174, val_loss=0.000116]Epoch 90:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 40.08it/s, loss=0.000, v_num=tg8t, train_loss=0.000174, val_loss=0.000116]Epoch 90:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000174, val_loss=0.000116]        Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000174, val_loss=0.000116]Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.20it/s, loss=0.000, v_num=tg8t, train_loss=0.000179, val_loss=0.000116]Epoch 91:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000179, val_loss=0.000116]        Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000179, val_loss=0.000116]Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.12it/s, loss=0.000, v_num=tg8t, train_loss=0.000176, val_loss=0.000116]Epoch 92:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000176, val_loss=0.000116]        Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000176, val_loss=0.000116]Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.26it/s, loss=0.000, v_num=tg8t, train_loss=0.000167, val_loss=0.000116]Epoch 93:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000167, val_loss=0.000116]        Epoch 94:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000167, val_loss=0.000116]Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.62it/s, loss=0.000, v_num=tg8t, train_loss=0.000157, val_loss=0.000116]
Validating: 0it [00:00, ?it/s][AEpoch 94: : 2it [00:00, 48.97it/s, loss=0.000, v_num=tg8t, train_loss=0.000157, val_loss=0.00014]                      
                              [AEpoch 94:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000157, val_loss=0.00014]Epoch 95:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000157, val_loss=0.00014]Epoch 95:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 57.33it/s, loss=0.000, v_num=tg8t, train_loss=0.000154, val_loss=0.00014]Epoch 95:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000154, val_loss=0.00014]        Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000154, val_loss=0.00014]Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.78it/s, loss=0.000, v_num=tg8t, train_loss=0.000156, val_loss=0.00014]Epoch 96:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000156, val_loss=0.00014]        Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000156, val_loss=0.00014]Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.68it/s, loss=0.000, v_num=tg8t, train_loss=0.000157, val_loss=0.00014]Epoch 97:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000157, val_loss=0.00014]        Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000157, val_loss=0.00014]Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.39it/s, loss=0.000, v_num=tg8t, train_loss=0.000153, val_loss=0.00014]Epoch 98:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000153, val_loss=0.00014]        Epoch 99:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.000, v_num=tg8t, train_loss=0.000153, val_loss=0.00014]Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.21it/s, loss=0.000, v_num=tg8t, train_loss=0.000144, val_loss=0.00014]
Validating: 0it [00:00, ?it/s][AEpoch 99: : 2it [00:00, 48.91it/s, loss=0.000, v_num=tg8t, train_loss=0.000144, val_loss=0.000154]                    
                              [ASaving latest checkpoint..
I0611 08:04:34.067683 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 99: : 2it [00:00, 38.24it/s, loss=0.000, v_num=tg8t, train_loss=0.000144, val_loss=0.000154]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=89.ckpt (current_epoch=99)
---------------------------------------
Planning took 60.4ms
q1a, predicted time: 823.7
q1a,(predicted 823.7),/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 HashJoin(mc mi_idx it)
 MergeJoin(mi_idx it)
 SeqScan(t)
 SeqScan(ct)
 SeqScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
---------------------------------------
Planning took 210.0ms
q1b, predicted time: 60109.9
q1b,(predicted 60109.9),/*+ HashJoin(ct t mc mi_idx it)
 MergeJoin(mi_idx it)
 MergeJoin(ct t mc)
 MergeJoin(t mc)
 SeqScan(ct)
 SeqScan(t)
 SeqScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading(((ct (t mc)) (mi_idx it))) */
---------------------------------------
Planning took 59.9ms
q1c, predicted time: 1288.6
q1c,(predicted 1288.6),/*+ MergeJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 IndexScan(t)
 IndexScan(ct)
 SeqScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
---------------------------------------
Planning took 93.1ms
q1d, predicted time: 441.5
q1d,(predicted 441.5),/*+ MergeJoin(it t mi_idx mc ct)
 MergeJoin(t mi_idx mc ct)
 HashJoin(mi_idx mc ct)
 HashJoin(mc ct)
 SeqScan(it)
 IndexScan(t)
 SeqScan(mi_idx)
 SeqScan(mc)
 SeqScan(ct)
 Leading((it (t (mi_idx (mc ct))))) */
Waiting on Ray tasks...value_iter=8
Running 1a: hinted plan
Hash Join cost=823.67919921875
  Seq Scan [title AS t] cost=0.58
  Hash Join cost=829.9320678710938
    Seq Scan [company_type AS ct] cost=1.05
    Hash Join cost=830.4912719726562
      Seq Scan [movie_companies AS mc] cost=0.64
      Merge Join cost=814.8930053710938
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,516.2,/*+ HashJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 HashJoin(mc mi_idx it)
 MergeJoin(mi_idx it)
 SeqScan(t)
 SeqScan(ct)
 SeqScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
1a Execution time: 516.2 (predicted 823.7) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  943.5 (predicted 927.5)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  3640978.5  823.7  /*+ Leading((t (ct (mc (mi_idx it))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout occurred; checking the hint against local PG.
Running 1b: hinted plan
Hash Join cost=60109.94921875
  Merge Join cost=707.7894897460938
    Seq Scan [company_type AS ct] cost=1.05
    Merge Join cost=549.1070556640625
      Seq Scan [title AS t] cost=0.58
      Seq Scan [movie_companies AS mc] cost=0.61
  Merge Join cost=506.1278381347656
    Index Scan [movie_info_idx AS mi_idx] cost=13685.15
    Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,-1.0,/*+ HashJoin(ct t mc mi_idx it)
 MergeJoin(mi_idx it)
 MergeJoin(ct t mc)
 MergeJoin(t mc)
 SeqScan(ct)
 SeqScan(t)
 SeqScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading(((ct (t mc)) (mi_idx it))) */
1b Execution time: -1.0 (predicted 60109.9) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  62.3 (predicted 3856764.8)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3935759.0  60109.9  /*+ Leading(((ct (t mc)) (mi_idx it))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Merge Join cost=1288.5521240234375
  Index Scan [title AS t] cost=0.58
  Hash Join cost=1287.376953125
    Index Scan [company_type AS ct] cost=1.05
    Hash Join cost=1282.6925048828125
      Seq Scan [movie_companies AS mc] cost=0.62
      Hash Join cost=1295.1466064453125
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,823.7,/*+ MergeJoin(t ct mc mi_idx it)
 HashJoin(ct mc mi_idx it)
 HashJoin(mc mi_idx it)
 HashJoin(mi_idx it)
 IndexScan(t)
 IndexScan(ct)
 SeqScan(mc)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((t (ct (mc (mi_idx it))))) */
1c Execution time: 823.7 (predicted 1288.6) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  56.8 (predicted 2033.5)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1732656.1  1288.6  /*+ Leading((t (ct (mc (mi_idx it))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1d: hinted plan
Merge Join cost=441.5127258300781
  Seq Scan [info_type AS it] cost=2.41
  Merge Join cost=441.67523193359375
    Index Scan [title AS t] cost=0.58
    Hash Join cost=437.4435119628906
      Seq Scan [movie_info_idx AS mi_idx] cost=13685.15
      Hash Join cost=443.2152099609375
        Seq Scan [movie_companies AS mc] cost=0.61
        Seq Scan [company_type AS ct] cost=1.05

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,1396.3,/*+ MergeJoin(it t mi_idx mc ct)
 MergeJoin(t mi_idx mc ct)
 HashJoin(mi_idx mc ct)
 HashJoin(mc ct)
 SeqScan(it)
 IndexScan(t)
 SeqScan(mi_idx)
 SeqScan(mc)
 SeqScan(ct)
 Leading((it (t (mi_idx (mc ct))))) */
1d Execution time: 1396.3 (predicted 441.5) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  55.8 (predicted 518.0)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  4873869.0  441.5  /*+ Leading((it (t (mi_idx (mc ct))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout detected! Assigning a special label 4096000 (server_ip=172.17.0.2)
---------------------------------------
Planning took 142.9ms
[Test set] q2a, predicted time: 563.2
q2a,(predicted 563.2),/*+ MergeJoin(t k cn mc mk)
 HashJoin(k cn mc mk)
 HashJoin(cn mc mk)
 HashJoin(mc mk)
 IndexScan(t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mc)
 IndexScan(mk)
 Leading((t (k (cn (mc mk))))) */
---------------------------------------
Planning took 135.6ms
[Test set] q2b, predicted time: 562.2
q2b,(predicted 562.2),/*+ MergeJoin(t k cn mc mk)
 HashJoin(k cn mc mk)
 HashJoin(cn mc mk)
 HashJoin(mc mk)
 IndexScan(t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mc)
 IndexScan(mk)
 Leading((t (k (cn (mc mk))))) */
---------------------------------------
Planning took 127.2ms
[Test set] q2c, predicted time: 561.9
q2c,(predicted 561.9),/*+ MergeJoin(t k cn mc mk)
 HashJoin(k cn mc mk)
 HashJoin(cn mc mk)
 HashJoin(mc mk)
 IndexScan(t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mc)
 IndexScan(mk)
 Leading((t (k (cn (mc mk))))) */
---------------------------------------
Planning took 231.7ms
[Test set] q2d, predicted time: 571.0
q2d,(predicted 571.0),/*+ MergeJoin(t k mk mc cn)
 HashJoin(k mk mc cn)
 HashJoin(mk mc cn)
 HashJoin(mc cn)
 IndexScan(t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(cn)
 Leading((t (k (mk (mc cn))))) */
[Test set] Waiting on Ray tasks...value_iter=8
[Test set] Running 2a: hinted plan
Merge Join cost=563.1668701171875
  Index Scan [title AS t] cost=0.49
  Hash Join cost=561.158203125
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=553.396240234375
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=562.151123046875
        Seq Scan [movie_companies AS mc] cost=0.54
        Index Scan [movie_keyword AS mk] cost=1229.38

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,45131.3,/*+ MergeJoin(t k cn mc mk)
 HashJoin(k cn mc mk)
 HashJoin(cn mc mk)
 HashJoin(mc mk)
 IndexScan(t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mc)
 IndexScan(mk)
 Leading((t (k (cn (mc mk))))) */
2a Execution time: 45131.3 (predicted 563.2) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 660.3)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3650679.0  563.2  /*+ Leading((t (k (cn (mc mk))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Merge Join cost=562.2137451171875
  Index Scan [title AS t] cost=0.49
  Hash Join cost=560.2068481445312
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=552.48046875
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=561.2501220703125
        Seq Scan [movie_companies AS mc] cost=0.54
        Index Scan [movie_keyword AS mk] cost=1229.38

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,43182.0,/*+ MergeJoin(t k cn mc mk)
 HashJoin(k cn mc mk)
 HashJoin(cn mc mk)
 HashJoin(mc mk)
 IndexScan(t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mc)
 IndexScan(mk)
 Leading((t (k (cn (mc mk))))) */
2b Execution time: 43182.0 (predicted 562.2) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 659.3)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3653510.8  562.2  /*+ Leading((t (k (cn (mc mk))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Merge Join cost=561.9441528320312
  Index Scan [title AS t] cost=0.49
  Hash Join cost=559.9376831054688
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=552.2216186523438
      Seq Scan [company_name AS cn] cost=0.45
      Hash Join cost=560.9987182617188
        Seq Scan [movie_companies AS mc] cost=0.54
        Index Scan [movie_keyword AS mk] cost=1229.38

filters
{'company_name AS cn': "((cn.country_code)::text = '[sm]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2c,36005.5,/*+ MergeJoin(t k cn mc mk)
 HashJoin(k cn mc mk)
 HashJoin(cn mc mk)
 HashJoin(mc mk)
 IndexScan(t)
 SeqScan(k)
 SeqScan(cn)
 SeqScan(mc)
 IndexScan(mk)
 Leading((t (k (cn (mc mk))))) */
2c Execution time: 36005.5 (predicted 561.9) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  269.8 (predicted 659.0)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3654319.2  561.9  /*+ Leading((t (k (cn (mc mk))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2d: hinted plan
Merge Join cost=571.0350341796875
  Index Scan [title AS t] cost=0.49
  Hash Join cost=568.699462890625
    Seq Scan [keyword AS k] cost=2626.12
    Hash Join cost=561.0343627929688
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=569.9635009765625
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [company_name AS cn] cost=0.45

filters
{'company_name AS cn': "((cn.country_code)::text = '[us]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2d,13715.8,/*+ MergeJoin(t k mk mc cn)
 HashJoin(k mk mc cn)
 HashJoin(mk mc cn)
 HashJoin(mc cn)
 IndexScan(t)
 SeqScan(k)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(cn)
 Leading((t (k (mk (mc cn))))) */
2d Execution time: 13715.8 (predicted 571.0) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  4175.6 (predicted 651.9)  /*+ Leading(((((k mk) t) mc) cn)) */
SIM-predicted costs, predicted latency, plan: 
  3589083.2  571.0  /*+ Leading((t (k (mk (mc cn))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Dropping buffer cache.
8 rels: ['company_name' 'company_type' 'info_type' 'keyword' 'movie_companies'
 'movie_info_idx' 'movie_keyword' 'title']
8 rel_ids: ['company_name AS cn' 'company_type AS ct' 'info_type AS it'
 'keyword AS k' 'movie_companies AS mc' 'movie_info_idx AS mi_idx'
 'movie_keyword AS mk' 'title AS t']
3 scans: ['Index Only Scan' 'Index Scan' 'Seq Scan']
3 joins: ['Hash Join' 'Merge Join' 'Nested Loop']
11 all ops: ['FinalizeAggregate' 'Gather' 'Hash' 'Hash Join' 'Index Only Scan'
 'Index Scan' 'Merge Join' 'Nested Loop' 'PartialAggregate' 'Seq Scan'
 'SimpleAggregate']
Calling make_and_featurize_trees()...
took 0.0s
num_total_subtrees=144 num_featurized_subtrees=16 num_new_datapoints=16
head
  query=2.269 feat=23.0 cost=516.238
  query=2.269 feat=16.0 cost=516.238
  query=2.269 feat=10.0 cost=516.238
tail
  query=2.313 feat=5.0 cost=1396.323
  query=2.313 feat=10.0 cost=1396.323
  query=2.313 feat=16.0 cost=1396.323
costs stats mean 8.85783920503224 std 3.6931564722154526
num_train=14 num_validation=2
InitializeModel curr_value_iter=9
Assigning real model := 0.0*SIM + 1.0*previous real model
iter 9 lr 0.001
GPU available: False, used: False
I0611 08:06:58.637881 140711206703488 distributed.py:41] GPU available: False, used: False
TPU available: False, using: 0 TPU cores
I0611 08:06:58.638842 140711206703488 distributed.py:41] TPU available: False, using: 0 TPU cores
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Validation sanity check: 0it [00:00, ?it/s]                                           /home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 25.35it/s, loss=1.169, v_num=tg8t, train_loss=1.17]Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.169, v_num=tg8t, train_loss=1.17]        Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.169, v_num=tg8t, train_loss=1.17]Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.14it/s, loss=1.009, v_num=tg8t, train_loss=0.849]Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.009, v_num=tg8t, train_loss=0.849]        Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.009, v_num=tg8t, train_loss=0.849]Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.88it/s, loss=0.900, v_num=tg8t, train_loss=0.682]Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.900, v_num=tg8t, train_loss=0.682]        Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.900, v_num=tg8t, train_loss=0.682]Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.02it/s, loss=0.851, v_num=tg8t, train_loss=0.704]Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.851, v_num=tg8t, train_loss=0.704]        Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.851, v_num=tg8t, train_loss=0.704]Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.14it/s, loss=0.770, v_num=tg8t, train_loss=0.448]
Validating: 0it [00:00, ?it/s][A/home/balsa/anaconda3/envs/balsa/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: 
                    When using EvalResult(early_stop_on=X) or TrainResult(early_stop_on=X) the
                    'monitor' key of EarlyStopping has no effect.
                    Remove EarlyStopping(monitor='val_early_stop_on) to fix')
                
  warnings.warn(*args, **kwargs)
Epoch 4: : 2it [00:00, 30.39it/s, loss=0.770, v_num=tg8t, train_loss=0.448, val_loss=0.22]      
                              [AEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.770, v_num=tg8t, train_loss=0.448, val_loss=0.22]Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.770, v_num=tg8t, train_loss=0.448, val_loss=0.22]Epoch 5:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 43.60it/s, loss=0.692, v_num=tg8t, train_loss=0.302, val_loss=0.22]Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.692, v_num=tg8t, train_loss=0.302, val_loss=0.22]        Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.692, v_num=tg8t, train_loss=0.302, val_loss=0.22]Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.97it/s, loss=0.620, v_num=tg8t, train_loss=0.185, val_loss=0.22]Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.620, v_num=tg8t, train_loss=0.185, val_loss=0.22]        Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.620, v_num=tg8t, train_loss=0.185, val_loss=0.22]Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.92it/s, loss=0.561, v_num=tg8t, train_loss=0.152, val_loss=0.22]Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.561, v_num=tg8t, train_loss=0.152, val_loss=0.22]        Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.561, v_num=tg8t, train_loss=0.152, val_loss=0.22]Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.18it/s, loss=0.513, v_num=tg8t, train_loss=0.121, val_loss=0.22]Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.513, v_num=tg8t, train_loss=0.121, val_loss=0.22]        Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.513, v_num=tg8t, train_loss=0.121, val_loss=0.22]Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.78it/s, loss=0.470, v_num=tg8t, train_loss=0.0881, val_loss=0.22]
Validating: 0it [00:00, ?it/s][AEpoch 9: : 2it [00:00, 44.91it/s, loss=0.470, v_num=tg8t, train_loss=0.0881, val_loss=0.951]                    
                              [AEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.470, v_num=tg8t, train_loss=0.0881, val_loss=0.951]Epoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.470, v_num=tg8t, train_loss=0.0881, val_loss=0.951]Epoch 10:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 38.29it/s, loss=0.456, v_num=tg8t, train_loss=0.318, val_loss=0.951]Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.456, v_num=tg8t, train_loss=0.318, val_loss=0.951]        Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.456, v_num=tg8t, train_loss=0.318, val_loss=0.951]Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.52it/s, loss=0.424, v_num=tg8t, train_loss=0.0688, val_loss=0.951]Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.424, v_num=tg8t, train_loss=0.0688, val_loss=0.951]        Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.424, v_num=tg8t, train_loss=0.0688, val_loss=0.951]Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.08it/s, loss=0.397, v_num=tg8t, train_loss=0.0687, val_loss=0.951]Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.397, v_num=tg8t, train_loss=0.0687, val_loss=0.951]        Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.397, v_num=tg8t, train_loss=0.0687, val_loss=0.951]Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.87it/s, loss=0.373, v_num=tg8t, train_loss=0.0674, val_loss=0.951]Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.373, v_num=tg8t, train_loss=0.0674, val_loss=0.951]        Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.373, v_num=tg8t, train_loss=0.0674, val_loss=0.951]Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.41it/s, loss=0.352, v_num=tg8t, train_loss=0.0626, val_loss=0.951]
Validating: 0it [00:00, ?it/s][AEpoch 14: : 2it [00:00, 29.18it/s, loss=0.352, v_num=tg8t, train_loss=0.0626, val_loss=0.0425]                    
                              [AEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.352, v_num=tg8t, train_loss=0.0626, val_loss=0.0425]Epoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.352, v_num=tg8t, train_loss=0.0626, val_loss=0.0425]Epoch 15:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 44.05it/s, loss=0.334, v_num=tg8t, train_loss=0.0506, val_loss=0.0425]Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.334, v_num=tg8t, train_loss=0.0506, val_loss=0.0425]        Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.334, v_num=tg8t, train_loss=0.0506, val_loss=0.0425]Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.85it/s, loss=0.321, v_num=tg8t, train_loss=0.114, val_loss=0.0425]Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.321, v_num=tg8t, train_loss=0.114, val_loss=0.0425]        Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.321, v_num=tg8t, train_loss=0.114, val_loss=0.0425]Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.12it/s, loss=0.305, v_num=tg8t, train_loss=0.0423, val_loss=0.0425]Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.305, v_num=tg8t, train_loss=0.0423, val_loss=0.0425]        Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.305, v_num=tg8t, train_loss=0.0423, val_loss=0.0425]Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.07it/s, loss=0.291, v_num=tg8t, train_loss=0.0426, val_loss=0.0425]Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.291, v_num=tg8t, train_loss=0.0426, val_loss=0.0425]        Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.291, v_num=tg8t, train_loss=0.0426, val_loss=0.0425]Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 50.64it/s, loss=0.279, v_num=tg8t, train_loss=0.0378, val_loss=0.0425]
Validating: 0it [00:00, ?it/s][AEpoch 19: : 2it [00:00, 31.58it/s, loss=0.279, v_num=tg8t, train_loss=0.0378, val_loss=0.0159]                     
                              [AEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.279, v_num=tg8t, train_loss=0.0378, val_loss=0.0159]Epoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.279, v_num=tg8t, train_loss=0.0378, val_loss=0.0159]Epoch 20:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 37.94it/s, loss=0.222, v_num=tg8t, train_loss=0.0318, val_loss=0.0159]Epoch 20:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.222, v_num=tg8t, train_loss=0.0318, val_loss=0.0159]        Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.222, v_num=tg8t, train_loss=0.0318, val_loss=0.0159]Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.78it/s, loss=0.181, v_num=tg8t, train_loss=0.0257, val_loss=0.0159]Epoch 21:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.181, v_num=tg8t, train_loss=0.0257, val_loss=0.0159]        Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.181, v_num=tg8t, train_loss=0.0257, val_loss=0.0159]Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.34it/s, loss=0.148, v_num=tg8t, train_loss=0.0203, val_loss=0.0159]Epoch 22:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.148, v_num=tg8t, train_loss=0.0203, val_loss=0.0159]        Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.148, v_num=tg8t, train_loss=0.0203, val_loss=0.0159]Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.58it/s, loss=0.113, v_num=tg8t, train_loss=0.0158, val_loss=0.0159]Epoch 23:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.113, v_num=tg8t, train_loss=0.0158, val_loss=0.0159]        Epoch 24:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.113, v_num=tg8t, train_loss=0.0158, val_loss=0.0159]Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 34.65it/s, loss=0.091, v_num=tg8t, train_loss=0.0127, val_loss=0.0159]
Validating: 0it [00:00, ?it/s][AEpoch 24: : 2it [00:00, 26.04it/s, loss=0.091, v_num=tg8t, train_loss=0.0127, val_loss=0.00993]                    
                              [AEpoch 24:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.091, v_num=tg8t, train_loss=0.0127, val_loss=0.00993]Epoch 25:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.091, v_num=tg8t, train_loss=0.0127, val_loss=0.00993]Epoch 25:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 40.63it/s, loss=0.077, v_num=tg8t, train_loss=0.011, val_loss=0.00993]Epoch 25:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.077, v_num=tg8t, train_loss=0.011, val_loss=0.00993]        Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.077, v_num=tg8t, train_loss=0.011, val_loss=0.00993]Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 38.65it/s, loss=0.068, v_num=tg8t, train_loss=0.0103, val_loss=0.00993]Epoch 26:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.068, v_num=tg8t, train_loss=0.0103, val_loss=0.00993]        Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.068, v_num=tg8t, train_loss=0.0103, val_loss=0.00993]Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.99it/s, loss=0.061, v_num=tg8t, train_loss=0.0104, val_loss=0.00993]Epoch 27:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.061, v_num=tg8t, train_loss=0.0104, val_loss=0.00993]        Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.061, v_num=tg8t, train_loss=0.0104, val_loss=0.00993]Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.09it/s, loss=0.055, v_num=tg8t, train_loss=0.0108, val_loss=0.00993]Epoch 28:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.055, v_num=tg8t, train_loss=0.0108, val_loss=0.00993]        Epoch 29:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.055, v_num=tg8t, train_loss=0.0108, val_loss=0.00993]Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.75it/s, loss=0.052, v_num=tg8t, train_loss=0.0114, val_loss=0.00993]
Validating: 0it [00:00, ?it/s][AEpoch 29: : 2it [00:00, 47.88it/s, loss=0.052, v_num=tg8t, train_loss=0.0114, val_loss=0.0204]                      
                              [AEpoch 29:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.052, v_num=tg8t, train_loss=0.0114, val_loss=0.0204]Epoch 30:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.052, v_num=tg8t, train_loss=0.0114, val_loss=0.0204]Epoch 30:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 34.84it/s, loss=0.036, v_num=tg8t, train_loss=0.0118, val_loss=0.0204]Epoch 30:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.036, v_num=tg8t, train_loss=0.0118, val_loss=0.0204]        Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.036, v_num=tg8t, train_loss=0.0118, val_loss=0.0204]Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.83it/s, loss=0.033, v_num=tg8t, train_loss=0.0119, val_loss=0.0204]Epoch 31:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.033, v_num=tg8t, train_loss=0.0119, val_loss=0.0204]        Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.033, v_num=tg8t, train_loss=0.0119, val_loss=0.0204]Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 54.07it/s, loss=0.031, v_num=tg8t, train_loss=0.0118, val_loss=0.0204]Epoch 32:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.031, v_num=tg8t, train_loss=0.0118, val_loss=0.0204]        Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.031, v_num=tg8t, train_loss=0.0118, val_loss=0.0204]Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 59.69it/s, loss=0.028, v_num=tg8t, train_loss=0.0113, val_loss=0.0204]Epoch 33:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.028, v_num=tg8t, train_loss=0.0113, val_loss=0.0204]        Epoch 34:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.028, v_num=tg8t, train_loss=0.0113, val_loss=0.0204]Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.46it/s, loss=0.025, v_num=tg8t, train_loss=0.0109, val_loss=0.0204]
Validating: 0it [00:00, ?it/s][AEpoch 34: : 2it [00:00, 45.44it/s, loss=0.025, v_num=tg8t, train_loss=0.0109, val_loss=0.0183]                     
                              [AEpoch 34:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.025, v_num=tg8t, train_loss=0.0109, val_loss=0.0183]Epoch 35:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.025, v_num=tg8t, train_loss=0.0109, val_loss=0.0183]Epoch 35:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 32.67it/s, loss=0.023, v_num=tg8t, train_loss=0.0103, val_loss=0.0183]Epoch 35:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.023, v_num=tg8t, train_loss=0.0103, val_loss=0.0183]        Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.023, v_num=tg8t, train_loss=0.0103, val_loss=0.0183]Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.21it/s, loss=0.018, v_num=tg8t, train_loss=0.00984, val_loss=0.0183]Epoch 36:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.018, v_num=tg8t, train_loss=0.00984, val_loss=0.0183]        Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.018, v_num=tg8t, train_loss=0.00984, val_loss=0.0183]Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.41it/s, loss=0.016, v_num=tg8t, train_loss=0.00949, val_loss=0.0183]Epoch 37:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.016, v_num=tg8t, train_loss=0.00949, val_loss=0.0183]        Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.016, v_num=tg8t, train_loss=0.00949, val_loss=0.0183]Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.17it/s, loss=0.015, v_num=tg8t, train_loss=0.00927, val_loss=0.0183]Epoch 38:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.015, v_num=tg8t, train_loss=0.00927, val_loss=0.0183]        Epoch 39:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.015, v_num=tg8t, train_loss=0.00927, val_loss=0.0183]Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.63it/s, loss=0.013, v_num=tg8t, train_loss=0.00918, val_loss=0.0183]
Validating: 0it [00:00, ?it/s][AEpoch 39: : 2it [00:00, 29.52it/s, loss=0.013, v_num=tg8t, train_loss=0.00918, val_loss=0.00902]                    
                              [AEpoch 39:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.013, v_num=tg8t, train_loss=0.00918, val_loss=0.00902]Epoch 40:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.013, v_num=tg8t, train_loss=0.00918, val_loss=0.00902]Epoch 40:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 37.65it/s, loss=0.012, v_num=tg8t, train_loss=0.00916, val_loss=0.00902]Epoch 40:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.012, v_num=tg8t, train_loss=0.00916, val_loss=0.00902]        Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.012, v_num=tg8t, train_loss=0.00916, val_loss=0.00902]Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 38.04it/s, loss=0.011, v_num=tg8t, train_loss=0.00915, val_loss=0.00902]Epoch 41:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.011, v_num=tg8t, train_loss=0.00915, val_loss=0.00902]        Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.011, v_num=tg8t, train_loss=0.00915, val_loss=0.00902]Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.09it/s, loss=0.011, v_num=tg8t, train_loss=0.00913, val_loss=0.00902]Epoch 42:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.011, v_num=tg8t, train_loss=0.00913, val_loss=0.00902]        Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.011, v_num=tg8t, train_loss=0.00913, val_loss=0.00902]Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 51.76it/s, loss=0.010, v_num=tg8t, train_loss=0.00907, val_loss=0.00902]Epoch 43:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00907, val_loss=0.00902]        Epoch 44:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00907, val_loss=0.00902]Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.60it/s, loss=0.010, v_num=tg8t, train_loss=0.00898, val_loss=0.00902]
Validating: 0it [00:00, ?it/s][AEpoch 44: : 2it [00:00, 27.70it/s, loss=0.010, v_num=tg8t, train_loss=0.00898, val_loss=0.00846]                     
                              [AEpoch 44:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00898, val_loss=0.00846]Epoch 45:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00898, val_loss=0.00846]Epoch 45:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 47.50it/s, loss=0.010, v_num=tg8t, train_loss=0.00887, val_loss=0.00846]Epoch 45:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00887, val_loss=0.00846]        Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00887, val_loss=0.00846]Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 48.58it/s, loss=0.010, v_num=tg8t, train_loss=0.00877, val_loss=0.00846]Epoch 46:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00877, val_loss=0.00846]        Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00877, val_loss=0.00846]Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.01it/s, loss=0.010, v_num=tg8t, train_loss=0.00864, val_loss=0.00846]Epoch 47:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00864, val_loss=0.00846]        Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00864, val_loss=0.00846]Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.91it/s, loss=0.010, v_num=tg8t, train_loss=0.0085, val_loss=0.00846]Epoch 48:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.0085, val_loss=0.00846]        Epoch 49:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.0085, val_loss=0.00846]Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.27it/s, loss=0.010, v_num=tg8t, train_loss=0.00876, val_loss=0.00846]
Validating: 0it [00:00, ?it/s][AEpoch 49: : 2it [00:00, 38.25it/s, loss=0.010, v_num=tg8t, train_loss=0.00876, val_loss=0.0126]                      
                              [AEpoch 49:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00876, val_loss=0.0126]Epoch 50:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00876, val_loss=0.0126]Epoch 50:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 30.99it/s, loss=0.010, v_num=tg8t, train_loss=0.00848, val_loss=0.0126]Epoch 50:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00848, val_loss=0.0126]        Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.010, v_num=tg8t, train_loss=0.00848, val_loss=0.0126]Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.88it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]Epoch 51:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]        Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.21it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]Epoch 52:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]        Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.14it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]Epoch 53:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]        Epoch 54:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00845, val_loss=0.0126]Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 46.40it/s, loss=0.009, v_num=tg8t, train_loss=0.00843, val_loss=0.0126]
Validating: 0it [00:00, ?it/s][AEpoch 54: : 2it [00:00, 52.85it/s, loss=0.009, v_num=tg8t, train_loss=0.00843, val_loss=0.0141]                     
                              [AEpoch 54:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00843, val_loss=0.0141]Epoch 55:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00843, val_loss=0.0141]Epoch 55:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00, 60.39it/s, loss=0.009, v_num=tg8t, train_loss=0.00838, val_loss=0.0141]Epoch 55:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00838, val_loss=0.0141]        Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00838, val_loss=0.0141]Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 42.44it/s, loss=0.009, v_num=tg8t, train_loss=0.00831, val_loss=0.0141]Epoch 56:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00831, val_loss=0.0141]        Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00831, val_loss=0.0141]Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.72it/s, loss=0.009, v_num=tg8t, train_loss=0.00823, val_loss=0.0141]Epoch 57:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00823, val_loss=0.0141]        Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00823, val_loss=0.0141]Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.98it/s, loss=0.009, v_num=tg8t, train_loss=0.00815, val_loss=0.0141]Epoch 58:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00815, val_loss=0.0141]        Epoch 59:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.009, v_num=tg8t, train_loss=0.00815, val_loss=0.0141]Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 36.80it/s, loss=0.009, v_num=tg8t, train_loss=0.00807, val_loss=0.0141]
Validating: 0it [00:00, ?it/s][AEpoch 59: : 2it [00:00, 42.94it/s, loss=0.009, v_num=tg8t, train_loss=0.00807, val_loss=0.0123]                     
                              [ASaving latest checkpoint..
I0611 08:07:01.221431 140711206703488 training_loop.py:1136] Saving latest checkpoint..
Epoch 00060: early stopping triggered.
I0611 08:07:01.222088 140711206703488 early_stopping.py:237] Epoch 00060: early stopping triggered.
Epoch 59: : 2it [00:00, 28.57it/s, loss=0.009, v_num=tg8t, train_loss=0.00807, val_loss=0.0123]
Loading best checkpoint: /balsa/balsa/tensorboard_logs_balsa/63_enqqtg8t/checkpoints/epoch=44.ckpt (current_epoch=59)
---------------------------------------
Planning took 31.9ms
q1a, predicted time: 816.8
q1a,(predicted 816.8),/*+ HashJoin(mc it mi_idx ct t)
 HashJoin(mc it mi_idx ct)
 HashJoin(mc it mi_idx)
 HashJoin(it mi_idx)
 SeqScan(mc)
 IndexScan(it)
 IndexScan(mi_idx)
 SeqScan(ct)
 SeqScan(t)
 Leading((((mc (it mi_idx)) ct) t)) */
---------------------------------------
Planning took 43.1ms
q1b, predicted time: 3471212.2
q1b,(predicted 3471212.2),/*+ NestLoop(mi_idx mc t it ct)
 MergeJoin(mi_idx mc t it)
 MergeJoin(mi_idx mc t)
 MergeJoin(mi_idx mc)
 IndexScan(mi_idx)
 IndexScan(mc)
 IndexScan(t)
 SeqScan(it)
 IndexScan(ct)
 Leading(((((mi_idx mc) t) it) ct)) */
---------------------------------------
Planning took 31.2ms
q1c, predicted time: 764.9
q1c,(predicted 764.9),/*+ HashJoin(ct mc t mi_idx it)
 MergeJoin(mc t mi_idx it)
 HashJoin(t mi_idx it)
 HashJoin(mi_idx it)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(t)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((ct (mc (t (mi_idx it))))) */
---------------------------------------
Planning took 34.7ms
q1d, predicted time: 823.6
q1d,(predicted 823.6),/*+ HashJoin(mc it mi_idx ct t)
 HashJoin(mc it mi_idx ct)
 HashJoin(mc it mi_idx)
 HashJoin(it mi_idx)
 SeqScan(mc)
 IndexScan(it)
 IndexScan(mi_idx)
 SeqScan(ct)
 SeqScan(t)
 Leading((((mc (it mi_idx)) ct) t)) */
Waiting on Ray tasks...value_iter=9
Running 1a: hinted plan
Hash Join cost=816.8402099609375
  Hash Join cost=818.3961181640625
    Hash Join cost=821.7019653320312
      Seq Scan [movie_companies AS mc] cost=0.64
      Hash Join cost=857.3709716796875
        Index Scan [info_type AS it] cost=2.41
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
    Seq Scan [company_type AS ct] cost=1.05
  Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND ((mc.note ~~ "
                          "'%(co-production)%'::text) OR (mc.note ~~ "
                          "'%(presents)%'::text)))"}

q1a,1360.3,/*+ HashJoin(mc it mi_idx ct t)
 HashJoin(mc it mi_idx ct)
 HashJoin(mc it mi_idx)
 HashJoin(it mi_idx)
 SeqScan(mc)
 IndexScan(it)
 IndexScan(mi_idx)
 SeqScan(ct)
 SeqScan(t)
 Leading((((mc (it mi_idx)) ct) t)) */
1a Execution time: 1360.3 (predicted 816.8) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  943.5 (predicted 837.1)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  3714664.0  816.8  /*+ Leading((((mc (it mi_idx)) ct) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout occurred; checking the hint against local PG.
Running 1b: hinted plan
Nested Loop cost=3471212.25
  Merge Join cost=3469077.75
    Merge Join cost=3476300.75
      Merge Join cost=3893495.0
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [movie_companies AS mc] cost=0.61
      Index Scan [title AS t] cost=0.58
    Seq Scan [info_type AS it] cost=2.41
  Index Scan [company_type AS ct] cost=1.05

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '((t.production_year >= 2005) AND (t.production_year <= 2010))'}

q1b,-1.0,/*+ NestLoop(mi_idx mc t it ct)
 MergeJoin(mi_idx mc t it)
 MergeJoin(mi_idx mc t)
 MergeJoin(mi_idx mc)
 IndexScan(mi_idx)
 IndexScan(mc)
 IndexScan(t)
 SeqScan(it)
 IndexScan(ct)
 Leading(((((mi_idx mc) t) it) ct)) */
1b Execution time: -1.0 (predicted 3471212.2) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  62.3 (predicted 3527453.2)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  5178167.0  3471212.2  /*+ Leading(((((mi_idx mc) t) it) ct)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Running 1c: hinted plan
Hash Join cost=764.9267578125
  Index Scan [company_type AS ct] cost=1.05
  Merge Join cost=770.7584838867188
    Index Scan [movie_companies AS mc] cost=0.62
    Hash Join cost=778.099853515625
      Seq Scan [title AS t] cost=0.58
      Hash Join cost=798.8989868164062
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
        Index Scan [info_type AS it] cost=2.41

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'top 250 rank'::text)",
 'movie_companies AS mc': "((mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text) AND (mc.note ~~ "
                          "'%(co-production)%'::text))",
 'title AS t': '(t.production_year > 2010)'}

q1c,399.8,/*+ HashJoin(ct mc t mi_idx it)
 MergeJoin(mc t mi_idx it)
 HashJoin(t mi_idx it)
 HashJoin(mi_idx it)
 IndexScan(ct)
 IndexScan(mc)
 SeqScan(t)
 IndexScan(mi_idx)
 IndexScan(it)
 Leading((ct (mc (t (mi_idx it))))) */
1c Execution time: 399.8 (predicted 764.9) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  56.8 (predicted 794.0)  /*+ Leading(((((mi_idx it) mc) ct) t)) */
SIM-predicted costs, predicted latency, plan: 
  1754182.1  764.9  /*+ Leading((ct (mc (t (mi_idx it))))) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout occurred; checking the hint against local PG.
Running 1d: hinted plan
Hash Join cost=823.572998046875
  Hash Join cost=825.4916381835938
    Hash Join cost=829.9213256835938
      Seq Scan [movie_companies AS mc] cost=0.61
      Hash Join cost=860.4474487304688
        Index Scan [info_type AS it] cost=2.41
        Index Scan [movie_info_idx AS mi_idx] cost=13685.15
    Seq Scan [company_type AS ct] cost=1.05
  Seq Scan [title AS t] cost=0.58

filters
{'company_type AS ct': "((ct.kind)::text = 'production companies'::text)",
 'info_type AS it': "((it.info)::text = 'bottom 10 rank'::text)",
 'movie_companies AS mc': "(mc.note !~~ '%(as Metro-Goldwyn-Mayer "
                          "Pictures)%'::text)",
 'title AS t': '(t.production_year > 2000)'}

q1d,-1.0,/*+ HashJoin(mc it mi_idx ct t)
 HashJoin(mc it mi_idx ct)
 HashJoin(mc it mi_idx)
 HashJoin(it mi_idx)
 SeqScan(mc)
 IndexScan(it)
 IndexScan(mi_idx)
 SeqScan(ct)
 SeqScan(t)
 Leading((((mc (it mi_idx)) ct) t)) */
1d Execution time: -1.0 (predicted 823.6) curr_timeout_ms=1514.85
Expert plan: latency, predicted, hint
  55.8 (predicted 849.7)  /*+ Leading(((((mi_idx it) t) mc) ct)) */
SIM-predicted costs, predicted latency, plan: 
  3737334.5  823.6  /*+ Leading((((mc (it mi_idx)) ct) t)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Timeout detected! Assigning a special label 4096000 (server_ip=172.17.0.2)
Timeout detected! Assigning a special label 4096000 (server_ip=172.17.0.2)
Saved iter=9 checkpoint to: /balsa/balsa/wandb/run-20240611_074349-enqqtg8t/files/checkpoint.pt
Saved Experience to: data/replay-Balsa_JOBRandSplit-39execs-44nodes-8193s-9iters-enqqtg8t.pkl
---------------------------------------
Planning took 27.2ms
[Test set] q2a, predicted time: 830.0
q2a,(predicted 830.0),/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
---------------------------------------
Planning took 26.7ms
[Test set] q2b, predicted time: 830.0
q2b,(predicted 830.0),/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
---------------------------------------
Planning took 28.5ms
[Test set] q2c, predicted time: 830.0
q2c,(predicted 830.0),/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
---------------------------------------
Planning took 31.0ms
[Test set] q2d, predicted time: 830.0
q2d,(predicted 830.0),/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
[Test set] Waiting on Ray tasks...value_iter=9
[Test set] Running 2a: hinted plan
Nested Loop cost=829.9609375
  Hash Join cost=830.8069458007812
    Hash Join cost=830.745849609375
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=862.8311767578125
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49
    Index Scan [keyword AS k] cost=2626.12
  Index Scan [company_name AS cn] cost=0.45

filters
{'company_name AS cn': "((cn.country_code)::text = '[de]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2a,2339.5,/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
2a Execution time: 2339.5 (predicted 830.0) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  8621.4 (predicted 848.1)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3518791.5  830.0  /*+ Leading((((mk (mc t)) k) cn)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2b: hinted plan
Nested Loop cost=829.970458984375
  Hash Join cost=830.8161010742188
    Hash Join cost=830.7561645507812
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=862.8385620117188
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49
    Index Scan [keyword AS k] cost=2626.12
  Index Scan [company_name AS cn] cost=0.45

filters
{'company_name AS cn': "((cn.country_code)::text = '[nl]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2b,2495.7,/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
2b Execution time: 2495.7 (predicted 830.0) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  624.2 (predicted 848.1)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3521255.5  830.0  /*+ Leading((((mk (mc t)) k) cn)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2c: hinted plan
Nested Loop cost=829.9720458984375
  Hash Join cost=830.8184814453125
    Hash Join cost=830.758544921875
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=862.841064453125
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49
    Index Scan [keyword AS k] cost=2626.12
  Index Scan [company_name AS cn] cost=0.45

filters
{'company_name AS cn': "((cn.country_code)::text = '[sm]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2c,3237.9,/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
2c Execution time: 3237.9 (predicted 830.0) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  269.8 (predicted 848.1)  /*+ Leading(((((k mk) mc) cn) t)) */
SIM-predicted costs, predicted latency, plan: 
  3521954.0  830.0  /*+ Leading((((mk (mc t)) k) cn)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
[Test set] Running 2d: hinted plan
Nested Loop cost=829.98828125
  Hash Join cost=830.8307495117188
    Hash Join cost=830.7569580078125
      Seq Scan [movie_keyword AS mk] cost=1229.38
      Hash Join cost=862.7838134765625
        Seq Scan [movie_companies AS mc] cost=0.54
        Seq Scan [title AS t] cost=0.49
    Index Scan [keyword AS k] cost=2626.12
  Index Scan [company_name AS cn] cost=0.45

filters
{'company_name AS cn': "((cn.country_code)::text = '[us]'::text)",
 'keyword AS k': "(k.keyword = 'character-name-in-title'::text)"}

q2d,2160.2,/*+ NestLoop(mk mc t k cn)
 HashJoin(mk mc t k)
 HashJoin(mk mc t)
 HashJoin(mc t)
 SeqScan(mk)
 SeqScan(mc)
 SeqScan(t)
 IndexScan(k)
 IndexScan(cn)
 Leading((((mk (mc t)) k) cn)) */
2d Execution time: 2160.2 (predicted 830.0) curr_timeout_ms=1100000
Expert plan: latency, predicted, hint
  4175.6 (predicted 846.6)  /*+ Leading(((((k mk) t) mc) cn)) */
SIM-predicted costs, predicted latency, plan: 
  3498384.0  830.0  /*+ Leading((((mk (mc t)) k) cn)) */  <-- cheapest [picked]
--------------------------------------------------------------------------------
Dropping buffer cache.
wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 2.551 MB uploadedwandb: / 2.545 MB of 2.551 MB uploadedwandb: - 2.545 MB of 2.551 MB uploadedwandb: \ 2.545 MB of 2.551 MB uploadedwandb: | 2.551 MB of 2.551 MB uploadedwandb: 
wandb: Run history:
wandb:                                   curr_iter_max_ms ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà
wandb:                          curr_iter_skipped_queries ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:                                       curr_timeout ‚ñÅ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:                                    curr_value_iter ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                              epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ
wandb:                                        global_step ‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ
wandb:                         latency/mean_l1_agent_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                        latency/mean_l1_expert_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÑ‚ñÑ
wandb:                   latency/mean_pred-tgt_agent_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                  latency/mean_pred-tgt_expert_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÑ‚ñÑ
wandb:                                        latency/q1a ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                        latency/q1b ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà
wandb:                                        latency/q1c ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÅ
wandb:                                        latency/q1d ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:                                   latency/workload ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÅ
wandb:                              latency/workload_best ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÅ‚ñÅ
wandb:                                 latency_expert/q1a ‚ñÅ
wandb:                                 latency_expert/q1b ‚ñÅ
wandb:                                 latency_expert/q1c ‚ñÅ
wandb:                                 latency_expert/q1d ‚ñÅ
wandb:                            latency_expert/workload ‚ñÅ
wandb:                            latency_expert_test/q2a ‚ñÅ
wandb:                            latency_expert_test/q2b ‚ñÅ
wandb:                            latency_expert_test/q2c ‚ñÅ
wandb:                            latency_expert_test/q2d ‚ñÅ
wandb:                       latency_expert_test/workload ‚ñÅ
wandb:                    latency_test/mean_l1_agent_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                   latency_test/mean_l1_expert_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:              latency_test/mean_pred-tgt_agent_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:             latency_test/mean_pred-tgt_expert_secs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                                   latency_test/q2a ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÜ‚ñÅ
wandb:                                   latency_test/q2b ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÖ‚ñÅ
wandb:                                   latency_test/q2c ‚ñÅ‚ñÜ‚ñá‚ñÅ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÖ‚ñÅ
wandb:                                   latency_test/q2d ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb:                              latency_test/workload ‚ñÅ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ
wandb:                         latency_test/workload_best ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                                                 lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                        num_queries_with_eps_random ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                    num_query_execs ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà
wandb:                                       num_timeouts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÖ‚ñÖ‚ñà
wandb:                                 num_total_timeouts ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:                 predicted_latency_expert_plans/q1a ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                 predicted_latency_expert_plans/q1b ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñá
wandb:                 predicted_latency_expert_plans/q1c ‚ñÅ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÉ
wandb:                 predicted_latency_expert_plans/q1d ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                 predicted_latency_expert_plans/q2a ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                 predicted_latency_expert_plans/q2b ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                 predicted_latency_expert_plans/q2c ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                 predicted_latency_expert_plans/q2d ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                                        timing/plan ‚ñà‚ñÇ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÅ
wandb:                               timing/plan_test_set ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÖ‚ñÅ
wandb:                                       timing/train ‚ñÅ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñà‚ñÖ
wandb:                         timing/wait_for_executions ‚ñà‚ñà‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ
wandb:                timing/wait_for_executions_test_set ‚ñÅ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ
wandb:                             timing_cumulative/plan ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà
wandb:                    timing_cumulative/plan_test_set ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñà
wandb:                            timing_cumulative/train ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:              timing_cumulative/wait_for_executions ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb:     timing_cumulative/wait_for_executions_test_set ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                         timing_cumulative_pct/plan ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                timing_cumulative_pct/plan_test_set ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                        timing_cumulative_pct/train ‚ñÅ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá
wandb:          timing_cumulative_pct/wait_for_executions ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: timing_cumulative_pct/wait_for_executions_test_set ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                                    timing_pct/plan ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñá‚ñÅ‚ñÇ
wandb:                           timing_pct/plan_test_set ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ
wandb:                                   timing_pct/train ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñÇ‚ñÖ
wandb:                     timing_pct/wait_for_executions ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÑ
wandb:            timing_pct/wait_for_executions_test_set ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÅ‚ñà‚ñÖ
wandb:                                    total_grad_norm ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb:                                         total_norm ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:                      train/iter-0-num-normal-trees ‚ñÅ
wandb:                             train/iter-0-num-trees ‚ñÅ
wandb:                                  train/iter-1-loss ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-1-num-normal-trees ‚ñÅ
wandb:                             train/iter-1-num-trees ‚ñÅ
wandb:                              train/iter-1-val_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                  train/iter-2-loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-2-num-normal-trees ‚ñÅ
wandb:                             train/iter-2-num-trees ‚ñÅ
wandb:                              train/iter-2-val_loss ‚ñà‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                  train/iter-3-loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-3-num-normal-trees ‚ñÅ
wandb:                             train/iter-3-num-trees ‚ñÅ
wandb:                              train/iter-3-val_loss ‚ñÅ‚ñà‚ñÖ‚ñÅ
wandb:                                  train/iter-4-loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-4-num-normal-trees ‚ñÅ
wandb:                             train/iter-4-num-trees ‚ñÅ
wandb:                              train/iter-4-val_loss ‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb:                                  train/iter-5-loss ‚ñà‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-5-num-normal-trees ‚ñÅ
wandb:                             train/iter-5-num-trees ‚ñÅ
wandb:                              train/iter-5-val_loss ‚ñÖ‚ñÅ‚ñÑ‚ñá‚ñà
wandb:                                  train/iter-6-loss ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá
wandb:                      train/iter-6-num-normal-trees ‚ñÅ
wandb:                             train/iter-6-num-trees ‚ñÅ
wandb:                              train/iter-6-val_loss ‚ñà‚ñÜ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                  train/iter-7-loss ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-7-num-normal-trees ‚ñÅ
wandb:                             train/iter-7-num-trees ‚ñÅ
wandb:                              train/iter-7-val_loss ‚ñÜ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb:                                  train/iter-8-loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-8-num-normal-trees ‚ñÅ
wandb:                             train/iter-8-num-trees ‚ñÅ
wandb:                              train/iter-8-val_loss ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                  train/iter-9-loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train/iter-9-num-normal-trees ‚ñÅ
wandb:                             train/iter-9-num-trees ‚ñÅ
wandb:                              train/iter-9-val_loss ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                           train/num-new-datapoints ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÅ‚ñÖ‚ñà
wandb:                             train/num-normal-trees ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñÜ‚ñÖ
wandb:                                    train/num-trees ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                         train_loss ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÇ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                           val_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                   curr_iter_max_ms 4096000
wandb:                          curr_iter_skipped_queries 1
wandb:                                       curr_timeout 1.51485
wandb:                                    curr_value_iter 9
wandb:                                              epoch 59
wandb:                                        global_step 59
wandb:                         latency/mean_l1_agent_secs 0.45428
wandb:                        latency/mean_l1_expert_secs 882.25713
wandb:                   latency/mean_pred-tgt_agent_secs -0.08918
wandb:                  latency/mean_pred-tgt_expert_secs 882.20393
wandb:                                        latency/q1a 1.36031
wandb:                                        latency/q1b 4096.0
wandb:                                        latency/q1c 0.39983
wandb:                                        latency/q1d 4096.0
wandb:                                   latency/workload 2.51528
wandb:                              latency/workload_best 2.05357
wandb:                                 latency_expert/q1a 0.94351
wandb:                                 latency_expert/q1b 0.06225
wandb:                                 latency_expert/q1c 0.05679
wandb:                                 latency_expert/q1d 0.05581
wandb:                            latency_expert/workload 1.11837
wandb:                            latency_expert_test/q2a 8.62135
wandb:                            latency_expert_test/q2b 0.62424
wandb:                            latency_expert_test/q2c 0.26977
wandb:                            latency_expert_test/q2d 4.17558
wandb:                       latency_expert_test/workload 13.69095
wandb:                    latency_test/mean_l1_agent_secs 1.72832
wandb:                   latency_test/mean_l1_expert_secs 2.97612
wandb:              latency_test/mean_pred-tgt_agent_secs -1.72832
wandb:             latency_test/mean_pred-tgt_expert_secs -2.575
wandb:                                   latency_test/q2a 2.33946
wandb:                                   latency_test/q2b 2.49569
wandb:                                   latency_test/q2c 3.23789
wandb:                                   latency_test/q2d 2.16016
wandb:                              latency_test/workload 10.23319
wandb:                         latency_test/workload_best 2.09127
wandb:                                                 lr 0.001
wandb:                        num_queries_with_eps_random 0
wandb:                                    num_query_execs 39
wandb:                                       num_timeouts 2
wandb:                                 num_total_timeouts 6
wandb:                 predicted_latency_expert_plans/q1a 0.8371
wandb:                 predicted_latency_expert_plans/q1b 3527.45325
wandb:                 predicted_latency_expert_plans/q1c 0.79398
wandb:                 predicted_latency_expert_plans/q1d 0.84975
wandb:                 predicted_latency_expert_plans/q2a 0.84813
wandb:                 predicted_latency_expert_plans/q2b 0.84813
wandb:                 predicted_latency_expert_plans/q2c 0.84813
wandb:                 predicted_latency_expert_plans/q2d 0.84657
wandb:                                        timing/plan 0.17988
wandb:                               timing/plan_test_set 0.14879
wandb:                                       timing/train 2.6941
wandb:                         timing/wait_for_executions 4.85107
wandb:                timing/wait_for_executions_test_set 10.2758
wandb:                             timing_cumulative/plan 7.26732
wandb:                    timing_cumulative/plan_test_set 4.52294
wandb:                            timing_cumulative/train 21.55238
wandb:              timing_cumulative/wait_for_executions 47.98182
wandb:     timing_cumulative/wait_for_executions_test_set 1281.85642
wandb:                         timing_cumulative_pct/plan 0.00533
wandb:                timing_cumulative_pct/plan_test_set 0.00332
wandb:                        timing_cumulative_pct/train 0.01581
wandb:          timing_cumulative_pct/wait_for_executions 0.0352
wandb: timing_cumulative_pct/wait_for_executions_test_set 0.94034
wandb:                                    timing_pct/plan 0.00991
wandb:                           timing_pct/plan_test_set 0.0082
wandb:                                   timing_pct/train 0.14844
wandb:                     timing_pct/wait_for_executions 0.26728
wandb:            timing_pct/wait_for_executions_test_set 0.56617
wandb:                                    total_grad_norm 0.081
wandb:                                         total_norm 90.89787
wandb:                      train/iter-0-num-normal-trees 14
wandb:                             train/iter-0-num-trees 14
wandb:                                  train/iter-1-loss 0.00042
wandb:                      train/iter-1-num-normal-trees 14
wandb:                             train/iter-1-num-trees 14
wandb:                              train/iter-1-val_loss 0.00026
wandb:                                  train/iter-2-loss 0.00265
wandb:                      train/iter-2-num-normal-trees 14
wandb:                             train/iter-2-num-trees 14
wandb:                              train/iter-2-val_loss 0.00331
wandb:                                  train/iter-3-loss 1.05162
wandb:                      train/iter-3-num-normal-trees 14
wandb:                             train/iter-3-num-trees 14
wandb:                              train/iter-3-val_loss 0.36454
wandb:                                  train/iter-4-loss 0.15439
wandb:                      train/iter-4-num-normal-trees 14
wandb:                             train/iter-4-num-trees 14
wandb:                              train/iter-4-val_loss 0.23583
wandb:                                  train/iter-5-loss 0.62558
wandb:                      train/iter-5-num-normal-trees 14
wandb:                             train/iter-5-num-trees 14
wandb:                              train/iter-5-val_loss 4.05027
wandb:                                  train/iter-6-loss 0.95763
wandb:                      train/iter-6-num-normal-trees 6
wandb:                             train/iter-6-num-trees 14
wandb:                              train/iter-6-val_loss 0.0356
wandb:                                  train/iter-7-loss 0.02836
wandb:                      train/iter-7-num-normal-trees 14
wandb:                             train/iter-7-num-trees 14
wandb:                              train/iter-7-val_loss 0.01843
wandb:                                  train/iter-8-loss 0.00014
wandb:                      train/iter-8-num-normal-trees 12
wandb:                             train/iter-8-num-trees 14
wandb:                              train/iter-8-val_loss 0.00015
wandb:                                  train/iter-9-loss 0.00807
wandb:                      train/iter-9-num-normal-trees 11
wandb:                             train/iter-9-num-trees 14
wandb:                              train/iter-9-val_loss 0.01227
wandb:                           train/num-new-datapoints 16
wandb:                             train/num-normal-trees 11
wandb:                                    train/num-trees 14
wandb:                                         train_loss 0.00807
wandb:                                           val_loss 0.01227
wandb: 
wandb: üöÄ View run morning-hill-51 at: https://wandb.ai/zihao626_/balsa/runs/enqqtg8t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/zihao626_/balsa
wandb: Synced 6 W&B file(s), 10 media file(s), 10 artifact file(s) and 11 other file(s)
wandb: Find logs at: ./wandb/run-20240611_074349-enqqtg8t/logs
